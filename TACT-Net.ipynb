{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TACT-Net: Triple-Tier Attention-integrated Compact Transformer Network for COVID-19 Prediction in CT Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Importing Required Dependecies\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        '''\n",
    "        paper: https://arxiv.org/abs/1807.06521\n",
    "        code: https://gist.github.com/innat/99888fa8065ecbf3ae2b297e5c10db70\n",
    "        '''\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, kernel_size=kernel_size, \n",
    "                                            use_bias=False, \n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.nn.relu)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=kernel_size, \n",
    "                                            use_bias=False, \n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.nn.relu)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(16, kernel_size=kernel_size, \n",
    "                                            use_bias=False, \n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.nn.relu)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(1, kernel_size=kernel_size,  \n",
    "                                            use_bias=False,\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.math.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        avg_out = tf.reduce_mean(inputs, axis=3)\n",
    "        max_out = tf.reduce_max(inputs,  axis=3)\n",
    "        x = tf.stack([avg_out, max_out], axis=3) \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return self.conv4(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttentionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=8):\n",
    "        '''\n",
    "        paper: https://arxiv.org/abs/1807.06521\n",
    "        code: https://gist.github.com/innat/99888fa8065ecbf3ae2b297e5c10db70\n",
    "        '''\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.gapavg = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.gmpmax = tf.keras.layers.GlobalMaxPooling2D()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.conv1 = tf.keras.layers.Conv2D(input_shape[-1]//self.ratio, \n",
    "                                            kernel_size=1, \n",
    "                                            strides=1, padding='same',\n",
    "                                            use_bias=True, activation=tf.nn.relu)\n",
    "    \n",
    "        self.conv2 = tf.keras.layers.Conv2D(input_shape[-1], \n",
    "                                            kernel_size=1, \n",
    "                                            strides=1, padding='same',\n",
    "                                            use_bias=True, activation=tf.nn.relu)\n",
    "        super(ChannelAttentionModule, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # compute gap and gmp pooling \n",
    "        gapavg = self.gapavg(inputs)\n",
    "        gmpmax = self.gmpmax(inputs)\n",
    "        gapavg = tf.keras.layers.Reshape((1, 1, gapavg.shape[1]))(gapavg)   \n",
    "        gmpmax = tf.keras.layers.Reshape((1, 1, gmpmax.shape[1]))(gmpmax)   \n",
    "        # forward passing to the respected layers\n",
    "        gapavg_out = self.conv2(self.conv1(gapavg))\n",
    "        gmpmax_out = self.conv2(self.conv1(gmpmax))\n",
    "        return tf.math.sigmoid(gapavg_out + gmpmax_out)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[3]\n",
    "        return (input_shape[0], output_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_attention(x, nf):\n",
    "    # Apply convolution to capture spatial dependencies\n",
    "    conv = Conv2D(nf, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Apply convolution to obtain attention scores\n",
    "    attention_scores = Conv2D(1, 1, padding='same', activation='sigmoid')(conv)\n",
    "    \n",
    "    # Multiply attention scores with input features\n",
    "    weighted_features = Multiply()([x, attention_scores])\n",
    "    \n",
    "    return weighted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triple Tier Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_module(x, nf):\n",
    "    x_1by1 = Conv2D(nf, (1,1), activation='relu',  padding='same')(x)\n",
    "    x_3by3 = Conv2D(nf, (3,3), activation='relu',  padding='same')(x)\n",
    "    \n",
    "    x_pa = pixel_attention(x_3by3, nf)\n",
    "    \n",
    "    x_ca = ChannelAttentionModule()(x_1by1)\n",
    "    \n",
    "    x_sa = SpatialAttentionModule()(x_1by1)\n",
    "    \n",
    "    x_casa = Multiply()([x_sa, x_ca])\n",
    "    \n",
    "    x_out = Concatenate()([x_casa, x_pa])\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvolutionalTokenizer class that converts images into tokenized feature representations\n",
    "class ConvolutionalTokenizer(keras.Model):\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=1, pooling_kernel_size=3, pooling_stride=2, \n",
    "                 conv_layers=3, num_output_channels=[64, 128, 128], **kwargs):\n",
    "        super(ConvolutionalTokenizer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.conv_model = keras.Sequential()  # Sequential container for the convolutional layers\n",
    "        \n",
    "        # Loop through the desired number of convolutional layers\n",
    "        for i in range(conv_layers):\n",
    "            self.conv_model.add(L.SeparableConv2D(num_output_channels[i], kernel_size=kernel_size, strides=stride, padding=\"same\",\n",
    "                                                 use_bias=False, activation=\"relu\", depth_multiplier=1, \n",
    "                                                 depthwise_initializer=\"he_normal\", pointwise_initializer=\"he_normal\"))\n",
    "                        \n",
    "            self.conv_model.add(L.MaxPool2D(pool_size=pooling_kernel_size, strides=pooling_stride, padding=\"same\"))\n",
    "        \n",
    "    def call(self, images):\n",
    "       \n",
    "        outputs = self.conv_model(images)\n",
    "        \n",
    "        # Flatten the output to produce tokens of shape (batch_size, sequence_length, channels)\n",
    "        flattened = tf.reshape(outputs, (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[3]))\n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionEmbedding class that adds positional encodings to the tokenized input\n",
    "class PositionEmbedding(L.Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Get the sequence length and projection dimension from the input shape\n",
    "        sequence_length = input_shape[1]  \n",
    "        projection_dim = input_shape[-1]  \n",
    "        \n",
    "        # Create an embedding layer to generate positional encodings\n",
    "        self.embedding = L.Embedding(input_dim=sequence_length, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Generate the position indices and apply the embedding layer\n",
    "        sequence_length = tf.shape(inputs)[1]  \n",
    "        positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "        return self.embedding(positions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, mlp_dim, dim, dropout_rate=0.1):\n",
    "    x = L.Dense(mlp_dim, activation='swish')(x)\n",
    "    x = L.Dropout(dropout_rate)(x)\n",
    "    x = L.Dense(dim)(x)  # The output dimension is set to dim\n",
    "    x = L.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def transformer_encoder(x, num_heads, dim, mlp_dim):\n",
    "    skip_1 = x\n",
    "    x = L.LayerNormalization()(x)\n",
    "    x = L.MultiHeadAttention(num_heads=num_heads, key_dim=dim)(x, x)\n",
    "    x = L.Add()([x, skip_1])\n",
    "\n",
    "    skip_2 = x\n",
    "    x = L.LayerNormalization()(x)\n",
    "    x = mlp(x, mlp_dim, dim)  # The output dimension of MLP is dim\n",
    "    \n",
    "    # Ensure skip connection has the same dimension as x\n",
    "    if skip_2.shape[-1] != x.shape[-1]:\n",
    "        skip_2 = L.Dense(x.shape[-1])(skip_2)\n",
    "    \n",
    "    x = L.Add()([x, skip_2])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA-CT Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_ct_block(inputs, num_filters, dim, num_layers=1):\n",
    "    \n",
    "    # TTA Module applied to input features\n",
    "    x_tta = tta_module(inputs, num_filters)\n",
    "    \n",
    "    tokenizer = ConvolutionalTokenizer() \n",
    "    tokenized = tokenizer(inputs)  \n",
    "    \n",
    "    position_embedding = PositionEmbedding()(tokenized)  \n",
    "    x = tokenized + position_embedding  \n",
    "\n",
    "    # Apply transformer layers to the tokenized inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, num_heads=4, dim=dim, mlp_dim=dim*2)\n",
    "\n",
    "    # Reshape the tokenized features back to spatial dimensions (height x width)\n",
    "    B, P, N = x.shape  \n",
    "    H = W = int(P**0.5)  # Calculate the height and width from the sequence length (assuming square)\n",
    "    \n",
    "\n",
    "    x = L.Reshape((H, W, N))(x)\n",
    "\n",
    "    # Upsample to match the input dimensions\n",
    "    x = L.UpSampling2D(size=(inputs.shape[1] // H, inputs.shape[2] // W), interpolation=\"bilinear\")(x)\n",
    "\n",
    "    \n",
    "    x = L.SeparableConv2D(filters=inputs.shape[-1], kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    x = L.BatchNormalization()(x)  \n",
    "    x = L.Activation('swish')(x)  \n",
    "\n",
    "     \n",
    "    x = L.Concatenate(name = \"Vizualization_Map\")([x, x_tta])  \n",
    "\n",
    "    \n",
    "    x = L.SeparableConv2D(filters=num_filters, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = L.BatchNormalization()(x)  \n",
    "    x = L.Activation('swish')(x)  \n",
    "\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TACT-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    inputs = Input(shape=(224,224,3))\n",
    "    \n",
    "    \n",
    "    x = SeparableConv2D(filters=64, kernel_size=3, padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # TTA-CT Block\n",
    "    x = tta_ct_block(x, num_filters=128, dim=128, num_layers=4)\n",
    "    \n",
    "    x = SeparableConv2D(filters=128, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = SeparableConv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = SeparableConv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = SeparableConv2D(filters=512, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = SeparableConv2D(filters=1024, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Flatten the output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Final classification layer (binary output)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
