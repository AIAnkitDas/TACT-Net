{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1199870,"sourceType":"datasetVersion","datasetId":615374}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:17:32.293858Z","iopub.execute_input":"2025-02-10T11:17:32.294054Z","iopub.status.idle":"2025-02-10T11:17:33.806529Z","shell.execute_reply.started":"2025-02-10T11:17:32.294035Z","shell.execute_reply":"2025-02-10T11:17:33.805700Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Directory and category setup\nimage_directory = r'/kaggle/input/sarscov2-ctscan-dataset'\nsize = 224\ndata = []\n\n# Define categories\nCATEGORIES = ['non-COVID', 'COVID']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:17:33.807412Z","iopub.execute_input":"2025-02-10T11:17:33.807731Z","iopub.status.idle":"2025-02-10T11:17:33.811685Z","shell.execute_reply.started":"2025-02-10T11:17:33.807711Z","shell.execute_reply":"2025-02-10T11:17:33.810853Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"for category in CATEGORIES:\n    folder = os.path.join(image_directory, category)\n    label = CATEGORIES.index(category)\n    for image_name in os.listdir(folder):\n        if image_name.endswith('.png'):  # Check for .png files\n            image_path = os.path.join(folder, image_name)\n            try:\n                image = cv2.imread(image_path)\n                image = Image.fromarray(image, 'RGB')\n                image = image.resize((size, size))\n                image_array = np.array(image) / 255.0  # Normalize pixel values\n                data.append([image_array, label])\n            except Exception as e:\n                print(f\"Error processing image {image_path}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:17:33.812435Z","iopub.execute_input":"2025-02-10T11:17:33.812679Z","iopub.status.idle":"2025-02-10T11:18:12.618397Z","shell.execute_reply.started":"2025-02-10T11:17:33.812652Z","shell.execute_reply":"2025-02-10T11:18:12.617722Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Shuffle data\nrandom.shuffle(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:12.619111Z","iopub.execute_input":"2025-02-10T11:18:12.619311Z","iopub.status.idle":"2025-02-10T11:18:12.623850Z","shell.execute_reply.started":"2025-02-10T11:18:12.619294Z","shell.execute_reply":"2025-02-10T11:18:12.623042Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Separate features and labels\nx = []\ny = []\n\nfor features, labels in data:\n    x.append(features)\n    y.append(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:12.624591Z","iopub.execute_input":"2025-02-10T11:18:12.624867Z","iopub.status.idle":"2025-02-10T11:18:12.641867Z","shell.execute_reply.started":"2025-02-10T11:18:12.624848Z","shell.execute_reply":"2025-02-10T11:18:12.641114Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"x = np.array(x)\ny = np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:12.644442Z","iopub.execute_input":"2025-02-10T11:18:12.644644Z","iopub.status.idle":"2025-02-10T11:18:13.531627Z","shell.execute_reply.started":"2025-02-10T11:18:12.644626Z","shell.execute_reply":"2025-02-10T11:18:13.530233Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Save the processed arrays (optional)\nnp.save('x.npy', x)\nnp.save('y.npy', y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:13.533922Z","iopub.execute_input":"2025-02-10T11:18:13.534264Z","iopub.status.idle":"2025-02-10T11:18:15.448844Z","shell.execute_reply.started":"2025-02-10T11:18:13.534229Z","shell.execute_reply":"2025-02-10T11:18:15.447787Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n# Define K-fold cross-validation\nk = 3  # Number of folds\nskf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:15.449822Z","iopub.execute_input":"2025-02-10T11:18:15.450131Z","iopub.status.idle":"2025-02-10T11:18:15.455823Z","shell.execute_reply.started":"2025-02-10T11:18:15.450100Z","shell.execute_reply":"2025-02-10T11:18:15.454827Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:15.457346Z","iopub.execute_input":"2025-02-10T11:18:15.457670Z","iopub.status.idle":"2025-02-10T11:18:28.445617Z","shell.execute_reply.started":"2025-02-10T11:18:15.457640Z","shell.execute_reply":"2025-02-10T11:18:28.444960Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SpatialAttentionModule(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=3):\n        '''\n        paper: https://arxiv.org/abs/1807.06521\n        code: https://gist.github.com/innat/99888fa8065ecbf3ae2b297e5c10db70\n        '''\n        super(SpatialAttentionModule, self).__init__()\n        self.conv1 = tf.keras.layers.Conv2D(64, kernel_size=kernel_size, \n                                            use_bias=False, \n                                            kernel_initializer='he_normal',\n                                            strides=1, padding='same', \n                                            activation=tf.nn.relu)\n        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=kernel_size, \n                                            use_bias=False, \n                                            kernel_initializer='he_normal',\n                                            strides=1, padding='same', \n                                            activation=tf.nn.relu)\n        self.conv3 = tf.keras.layers.Conv2D(16, kernel_size=kernel_size, \n                                            use_bias=False, \n                                            kernel_initializer='he_normal',\n                                            strides=1, padding='same', \n                                            activation=tf.nn.relu)\n        self.conv4 = tf.keras.layers.Conv2D(1, kernel_size=kernel_size,  \n                                            use_bias=False,\n                                            kernel_initializer='he_normal',\n                                            strides=1, padding='same', \n                                            activation=tf.math.sigmoid)\n\n    def call(self, inputs):\n        avg_out = tf.reduce_mean(inputs, axis=3)\n        max_out = tf.reduce_max(inputs,  axis=3)\n        x = tf.stack([avg_out, max_out], axis=3) \n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return self.conv4(x)\n    \n\nclass ChannelAttentionModule(tf.keras.layers.Layer):\n    def __init__(self, ratio=8):\n        '''\n        paper: https://arxiv.org/abs/1807.06521\n        code: https://gist.github.com/innat/99888fa8065ecbf3ae2b297e5c10db70\n        '''\n        super(ChannelAttentionModule, self).__init__()\n        self.ratio = ratio\n        self.gapavg = tf.keras.layers.GlobalAveragePooling2D()\n        self.gmpmax = tf.keras.layers.GlobalMaxPooling2D()\n        \n    def build(self, input_shape):\n        self.conv1 = tf.keras.layers.Conv2D(input_shape[-1]//self.ratio, \n                                            kernel_size=1, \n                                            strides=1, padding='same',\n                                            use_bias=True, activation=tf.nn.relu)\n    \n        self.conv2 = tf.keras.layers.Conv2D(input_shape[-1], \n                                            kernel_size=1, \n                                            strides=1, padding='same',\n                                            use_bias=True, activation=tf.nn.relu)\n        super(ChannelAttentionModule, self).build(input_shape)\n\n    def call(self, inputs):\n        # compute gap and gmp pooling \n        gapavg = self.gapavg(inputs)\n        gmpmax = self.gmpmax(inputs)\n        gapavg = tf.keras.layers.Reshape((1, 1, gapavg.shape[1]))(gapavg)   \n        gmpmax = tf.keras.layers.Reshape((1, 1, gmpmax.shape[1]))(gmpmax)   \n        # forward passing to the respected layers\n        gapavg_out = self.conv2(self.conv1(gapavg))\n        gmpmax_out = self.conv2(self.conv1(gmpmax))\n        return tf.math.sigmoid(gapavg_out + gmpmax_out)\n    \n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[3]\n        return (input_shape[0], output_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:28.446389Z","iopub.execute_input":"2025-02-10T11:18:28.446947Z","iopub.status.idle":"2025-02-10T11:18:28.458629Z","shell.execute_reply.started":"2025-02-10T11:18:28.446924Z","shell.execute_reply":"2025-02-10T11:18:28.457745Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, Multiply\n\ndef pixel_attention(x, nf):\n    # Apply convolution to capture spatial dependencies\n    conv = Conv2D(nf, 3, padding='same', activation='relu')(x)\n    \n    # Apply convolution to obtain attention scores\n    attention_scores = Conv2D(1, 1, padding='same', activation='sigmoid')(conv)\n    \n    # Multiply attention scores with input features\n    weighted_features = Multiply()([x, attention_scores])\n    \n    return weighted_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:28.459335Z","iopub.execute_input":"2025-02-10T11:18:28.459548Z","iopub.status.idle":"2025-02-10T11:18:31.335455Z","shell.execute_reply.started":"2025-02-10T11:18:28.459530Z","shell.execute_reply":"2025-02-10T11:18:31.334607Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def tta_module(x, nf):\n    x_1by1 = Conv2D(nf, (1,1), activation='relu',  padding='same')(x)\n    x_3by3 = Conv2D(nf, (3,3), activation='relu',  padding='same')(x)\n    \n    x_pa = pixel_attention(x_3by3, nf)\n    \n    x_ca = ChannelAttentionModule()(x_1by1)\n    \n    x_sa = SpatialAttentionModule()(x_1by1)\n    \n    x_casa = Multiply()([x_sa, x_ca])\n    \n    x_out = Concatenate()([x_casa, x_pa])\n    return x_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.336252Z","iopub.execute_input":"2025-02-10T11:18:31.336524Z","iopub.status.idle":"2025-02-10T11:18:31.356971Z","shell.execute_reply.started":"2025-02-10T11:18:31.336503Z","shell.execute_reply":"2025-02-10T11:18:31.356201Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#ConvolutionalTokenizer class that converts images into tokenized feature representations\nclass ConvolutionalTokenizer(keras.Model):\n    def __init__(self, kernel_size=3, stride=1, padding=1, pooling_kernel_size=3, pooling_stride=2, \n                 conv_layers=3, num_output_channels=[64, 128, 128], **kwargs):\n        super(ConvolutionalTokenizer, self).__init__(**kwargs)\n        \n        self.conv_model = keras.Sequential()  # Sequential container for the convolutional layers\n        \n        # Loop through the desired number of convolutional layers\n        for i in range(conv_layers):\n            self.conv_model.add(L.SeparableConv2D(num_output_channels[i], kernel_size=kernel_size, strides=stride, padding=\"same\",\n                                                 use_bias=False, activation=\"relu\", depth_multiplier=1, \n                                                 depthwise_initializer=\"he_normal\", pointwise_initializer=\"he_normal\"))\n                        \n            self.conv_model.add(L.MaxPool2D(pool_size=pooling_kernel_size, strides=pooling_stride, padding=\"same\"))\n        \n    def call(self, images):\n       \n        outputs = self.conv_model(images)\n        \n        # Flatten the output to produce tokens of shape (batch_size, sequence_length, channels)\n        flattened = tf.reshape(outputs, (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[3]))\n        return flattened","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.357800Z","iopub.execute_input":"2025-02-10T11:18:31.358076Z","iopub.status.idle":"2025-02-10T11:18:31.373030Z","shell.execute_reply.started":"2025-02-10T11:18:31.358048Z","shell.execute_reply":"2025-02-10T11:18:31.372351Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Define a PositionEmbedding class that adds positional encodings to the tokenized input\nclass PositionEmbedding(L.Layer):\n    def __init__(self):\n        super(PositionEmbedding, self).__init__()\n\n    def build(self, input_shape):\n        # Get the sequence length and projection dimension from the input shape\n        sequence_length = input_shape[1]  \n        projection_dim = input_shape[-1]  \n        \n        # Create an embedding layer to generate positional encodings\n        self.embedding = L.Embedding(input_dim=sequence_length, output_dim=projection_dim)\n\n    def call(self, inputs):\n        # Generate the position indices and apply the embedding layer\n        sequence_length = tf.shape(inputs)[1]  \n        positions = tf.range(start=0, limit=sequence_length, delta=1)\n        return self.embedding(positions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.373657Z","iopub.execute_input":"2025-02-10T11:18:31.373868Z","iopub.status.idle":"2025-02-10T11:18:31.392576Z","shell.execute_reply.started":"2025-02-10T11:18:31.373851Z","shell.execute_reply":"2025-02-10T11:18:31.391778Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def mlp(x, mlp_dim, dim, dropout_rate=0.1):\n    x = L.Dense(mlp_dim, activation='swish')(x)\n    x = L.Dropout(dropout_rate)(x)\n    x = L.Dense(dim)(x)  # The output dimension is set to dim\n    x = L.Dropout(dropout_rate)(x)\n    return x\n\ndef transformer_encoder(x, num_heads, dim, mlp_dim):\n    skip_1 = x\n    x = L.LayerNormalization()(x)\n    x = L.MultiHeadAttention(num_heads=num_heads, key_dim=dim)(x, x)\n    x = L.Add()([x, skip_1])\n\n    skip_2 = x\n    x = L.LayerNormalization()(x)\n    x = mlp(x, mlp_dim, dim)  # The output dimension of MLP is dim\n    \n    # Ensure skip connection has the same dimension as x\n    if skip_2.shape[-1] != x.shape[-1]:\n        skip_2 = L.Dense(x.shape[-1])(skip_2)\n    \n    x = L.Add()([x, skip_2])\n\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.393395Z","iopub.execute_input":"2025-02-10T11:18:31.393634Z","iopub.status.idle":"2025-02-10T11:18:31.412861Z","shell.execute_reply.started":"2025-02-10T11:18:31.393617Z","shell.execute_reply":"2025-02-10T11:18:31.412131Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Compact convolutional transformer block\ndef tta_cct_block(inputs, num_filters, dim, num_layers=1):\n    \n    # TTA Module applied to input features\n    x_tta = tta_module(inputs, num_filters)\n    \n    tokenizer = ConvolutionalTokenizer() \n    tokenized = tokenizer(inputs)  \n    \n    position_embedding = PositionEmbedding()(tokenized)  \n    x = tokenized + position_embedding  \n\n    # Apply transformer layers to the tokenized inputs\n    for _ in range(num_layers):\n        x = transformer_encoder(x, num_heads=4, dim=dim, mlp_dim=dim*2)\n\n    # Reshape the tokenized features back to spatial dimensions (height x width)\n    B, P, N = x.shape  \n    H = W = int(P**0.5)  # Calculate the height and width from the sequence length (assuming square)\n    \n\n    x = L.Reshape((H, W, N))(x)\n\n    # Upsample to match the input dimensions\n    x = L.UpSampling2D(size=(inputs.shape[1] // H, inputs.shape[2] // W), interpolation=\"bilinear\")(x)\n\n    \n    x = L.SeparableConv2D(filters=inputs.shape[-1], kernel_size=1, padding='same', use_bias=False)(x)\n    x = L.BatchNormalization()(x)  \n    x = L.Activation('swish')(x)  \n\n     \n    x = L.Concatenate(name = \"Vizualization_Map\")([x, x_tta])  \n\n    \n    x = L.SeparableConv2D(filters=num_filters, kernel_size=3, padding='same', use_bias=False)(x)\n    x = L.BatchNormalization()(x)  \n    x = L.Activation('swish')(x)  \n\n    return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.413540Z","iopub.execute_input":"2025-02-10T11:18:31.413802Z","iopub.status.idle":"2025-02-10T11:18:31.423620Z","shell.execute_reply.started":"2025-02-10T11:18:31.413783Z","shell.execute_reply":"2025-02-10T11:18:31.422873Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def build_model(input_shape):\n    inputs = Input(shape=(224,224,3))\n    \n    \n    x = SeparableConv2D(filters=64, kernel_size=3, padding='same', use_bias=False)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    \n    # First TTA-CCT Block\n    x = tta_cct_block(x, num_filters=128, dim=128, num_layers=4)\n    \n    x = SeparableConv2D(filters=128, kernel_size=3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    \n    # Second TTA-CCT Block\n    # x = tta_cct_block(x, num_filters=256, dim=64, num_layers=4)\n    \n    x = SeparableConv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    \n    x = SeparableConv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    \n    x = SeparableConv2D(filters=512, kernel_size=3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    \n    x = SeparableConv2D(filters=1024, kernel_size=3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    \n     # Flatten the output\n    x = GlobalAveragePooling2D()(x)\n    \n        # Dense layers\n    x = Dense(1024, activation='relu')(x)\n    x = BatchNormalization()(x)\n    # x = Dropout(0.4)(x)\n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    #x = Dropout(0.5)(x)\n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dense(16, activation='relu')(x)\n    x = BatchNormalization()(x)\n    \n        # Final classification layer (binary output)\n    outputs = Dense(1, activation='sigmoid')(x)\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.424410Z","iopub.execute_input":"2025-02-10T11:18:31.424691Z","iopub.status.idle":"2025-02-10T11:18:31.442790Z","shell.execute_reply.started":"2025-02-10T11:18:31.424671Z","shell.execute_reply":"2025-02-10T11:18:31.442202Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Build the model for this fold\nmodel = build_model(input_shape=(224, 224, 3))\n\n# Print the model summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:31.443458Z","iopub.execute_input":"2025-02-10T11:18:31.443664Z","iopub.status.idle":"2025-02-10T11:18:34.346210Z","shell.execute_reply.started":"2025-02-10T11:18:31.443639Z","shell.execute_reply":"2025-02-10T11:18:34.345292Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │            \u001b[38;5;34m219\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │            \u001b[38;5;34m256\u001b[0m │ separable_conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ convolutional_tokenizer   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m30,976\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mConvolutionalTokenizer\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ position_embedding        │ (\u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m25,088\u001b[0m │ convolutional_tokeniz… │\n│ (\u001b[38;5;33mPositionEmbedding\u001b[0m)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ convolutional_tokeniz… │\n│                           │                        │                │ position_embedding[\u001b[38;5;34m0\u001b[0m]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m263,808\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n│                           │                        │                │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m32,896\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m263,808\u001b[0m │ layer_normalization_2… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m32,896\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m263,808\u001b[0m │ layer_normalization_4… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m32,896\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m263,808\u001b[0m │ layer_normalization_6… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m32,896\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │         \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ up_sampling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mUpSampling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m8,320\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m147,584\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m8,320\u001b[0m │ up_sampling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ spatial_attention_module  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │         \u001b[38;5;34m24,336\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mSpatialAttentionModule\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ channel_attention_module  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mChannelAttentionModule\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │            \u001b[38;5;34m129\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │            \u001b[38;5;34m256\u001b[0m │ separable_conv2d_4[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ spatial_attention_mod… │\n│                           │                        │                │ channel_attention_mod… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ Vizualization_Map         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m320\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │         \u001b[38;5;34m43,840\u001b[0m │ Vizualization_Map[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │            \u001b[38;5;34m512\u001b[0m │ separable_conv2d_5[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │         \u001b[38;5;34m17,536\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │            \u001b[38;5;34m512\u001b[0m │ separable_conv2d_6[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m33,920\u001b[0m │ max_pooling2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ separable_conv2d_7[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_4 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_8        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m67,840\u001b[0m │ max_pooling2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ separable_conv2d_8[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_5 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │        \u001b[38;5;34m133,376\u001b[0m │ max_pooling2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m2,048\u001b[0m │ separable_conv2d_9[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_6 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │        \u001b[38;5;34m528,896\u001b[0m │ max_pooling2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │          \u001b[38;5;34m4,096\u001b[0m │ separable_conv2d_10[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_7 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │      \u001b[38;5;34m1,049,600\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │          \u001b[38;5;34m4,096\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m524,800\u001b[0m │ batch_normalization_8… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │          \u001b[38;5;34m2,048\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m131,328\u001b[0m │ batch_normalization_9… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │          \u001b[38;5;34m1,024\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m32,896\u001b[0m │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │            \u001b[38;5;34m512\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m256\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │          \u001b[38;5;34m1,040\u001b[0m │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m64\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m17\u001b[0m │ batch_normalization_1… │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">219</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ convolutional_tokenizer   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30,976</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvolutionalTokenizer</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ position_embedding        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span> │ convolutional_tokeniz… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ convolutional_tokeniz… │\n│                           │                        │                │ position_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │                        │                │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> │ layer_normalization_2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> │ layer_normalization_4… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> │ layer_normalization_6… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ up_sampling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ up_sampling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ spatial_attention_module  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,336</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialAttentionModule</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ channel_attention_module  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ChannelAttentionModule</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ spatial_attention_mod… │\n│                           │                        │                │ channel_attention_mod… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ Vizualization_Map         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">43,840</span> │ Vizualization_Map[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ separable_conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17,536</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ separable_conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,920</span> │ max_pooling2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ separable_conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_8        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">67,840</span> │ max_pooling2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ separable_conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">133,376</span> │ max_pooling2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ separable_conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ separable_conv2d_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528,896</span> │ max_pooling2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)         │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ separable_conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ batch_normalization_8… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ batch_normalization_9… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ batch_normalization_1… │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,230,861\u001b[0m (16.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,230,861</span> (16.14 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,221,997\u001b[0m (16.11 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,221,997</span> (16.11 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,864\u001b[0m (34.62 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,864</span> (34.62 KB)\n</pre>\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"\nfrom tensorflow.keras import mixed_precision\n# Enable mixed precision training\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)  # Use set_global_policy instead of set_policy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:34.347324Z","iopub.execute_input":"2025-02-10T11:18:34.347677Z","iopub.status.idle":"2025-02-10T11:18:34.352105Z","shell.execute_reply.started":"2025-02-10T11:18:34.347635Z","shell.execute_reply":"2025-02-10T11:18:34.351235Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss\nimport seaborn as sns\nfrom tensorflow.keras.callbacks import LearningRateScheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:18:34.355252Z","iopub.execute_input":"2025-02-10T11:18:34.355475Z","iopub.status.idle":"2025-02-10T11:18:34.669739Z","shell.execute_reply.started":"2025-02-10T11:18:34.355456Z","shell.execute_reply":"2025-02-10T11:18:34.669144Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def lr_schedule(epoch, lr):\n    \"\"\"\n    Custom learning rate scheduler: reduces the learning rate after specific epochs.\n    \"\"\"\n    if epoch < 10:\n        return 1e-4\n    elif epoch < 30:\n        return 5e-5\n    elif epoch < 50:\n        return 1e-5\n    else:\n        return 5e-6\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:20:16.607652Z","iopub.execute_input":"2025-02-10T11:20:16.607999Z","iopub.status.idle":"2025-02-10T11:20:16.612056Z","shell.execute_reply.started":"2025-02-10T11:20:16.607972Z","shell.execute_reply":"2025-02-10T11:20:16.611220Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"fold_no = 1\n# Placeholder for storing metrics\nval_accuracies = []\nval_losses = []\nconfusion_matrices = []\nclassification_reports = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:20:18.989817Z","iopub.execute_input":"2025-02-10T11:20:18.990099Z","iopub.status.idle":"2025-02-10T11:20:18.994044Z","shell.execute_reply.started":"2025-02-10T11:20:18.990079Z","shell.execute_reply":"2025-02-10T11:20:18.993022Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"for train_index, val_index in skf.split(x, y):\n    print(f\"Training fold {fold_no}...\")\n\n    # Split the data into training and validation sets\n    x_train, x_val = x[train_index], x[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    # Build the model for this fold\n    model = build_model(input_shape=(224, 224, 3))\n\n     # AdamW optimizer with weight decay\n    optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5)\n\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    # ModelCheckpoint callback to save the best model\n    checkpoint_filepath = f'best_model_fold_{fold_no}.keras'\n    model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n\n    # Learning rate scheduler callback\n    lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n\n    # Train the model\n    history = model.fit(\n        x_train, y_train,\n        validation_data=(x_val, y_val),\n        epochs=100,\n        batch_size=16,\n        callbacks=[model_checkpoint,lr_scheduler],  # Add the checkpoint callback\n        verbose=1\n    )\n\n    # Load the best model for evaluation\n    model.load_weights(checkpoint_filepath)\n\n    # Save the complete model after training\n    complete_model_path = f'complete_model_fold_{fold_no}.keras'\n    model.save(complete_model_path)\n    print(f\"Saved the complete model for fold {fold_no} at {complete_model_path}\")\n\n    # Evaluate on validation data\n    val_loss, val_accuracy = model.evaluate(x_val, y_val, verbose=0)\n    val_accuracies.append(val_accuracy)\n    val_losses.append(val_loss)\n\n    # Generate predictions\n    y_pred = (model.predict(x_val) > 0.5).astype(\"int32\")\n\n    # Compute confusion matrix and classification report\n    cm = confusion_matrix(y_val, y_pred)\n    confusion_matrices.append(cm)\n    report = classification_report(y_val, y_pred, output_dict=True)\n    classification_reports.append(report)\n\n    print(f\"Validation accuracy for fold {fold_no}: {val_accuracy:.4f}\")\n    print(f\"Validation loss for fold {fold_no}: {val_loss:.4f}\")\n\n    fold_no += 1\n\n# Calculate and print average validation accuracy and loss across all folds\navg_val_accuracy = np.mean(val_accuracies)\navg_val_loss = np.mean(val_losses)\nprint(f\"\\nAverage validation accuracy across all folds: {avg_val_accuracy:.4f}\")\nprint(f\"Average validation loss across all folds: {avg_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:22:53.534387Z","iopub.execute_input":"2025-02-10T11:22:53.534686Z","iopub.status.idle":"2025-02-10T12:32:32.924025Z","shell.execute_reply.started":"2025-02-10T11:22:53.534663Z","shell.execute_reply":"2025-02-10T12:32:32.923320Z"}},"outputs":[{"name":"stdout","text":"Training fold 1...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - accuracy: 0.5306 - loss: 0.8440\nEpoch 1: val_accuracy improved from -inf to 0.49456, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 472ms/step - accuracy: 0.5310 - loss: 0.8433 - val_accuracy: 0.4946 - val_loss: 0.7003 - learning_rate: 1.0000e-04\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 2/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.6807 - loss: 0.6310\nEpoch 2: val_accuracy did not improve from 0.49456\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6809 - loss: 0.6306 - val_accuracy: 0.4946 - val_loss: 1.4163 - learning_rate: 1.0000e-04\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 3/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.7587 - loss: 0.5235\nEpoch 3: val_accuracy did not improve from 0.49456\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7586 - loss: 0.5235 - val_accuracy: 0.4946 - val_loss: 2.5277 - learning_rate: 1.0000e-04\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 4/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7985 - loss: 0.4627\nEpoch 4: val_accuracy did not improve from 0.49456\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.7984 - loss: 0.4627 - val_accuracy: 0.4946 - val_loss: 1.8524 - learning_rate: 1.0000e-04\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 5/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7838 - loss: 0.4550\nEpoch 5: val_accuracy did not improve from 0.49456\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 120ms/step - accuracy: 0.7839 - loss: 0.4549 - val_accuracy: 0.4946 - val_loss: 2.7249 - learning_rate: 1.0000e-04\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 6/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7943 - loss: 0.4266\nEpoch 6: val_accuracy did not improve from 0.49456\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.7943 - loss: 0.4266 - val_accuracy: 0.4946 - val_loss: 1.0557 - learning_rate: 1.0000e-04\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 7/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7996 - loss: 0.4169\nEpoch 7: val_accuracy did not improve from 0.49456\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 120ms/step - accuracy: 0.7997 - loss: 0.4167 - val_accuracy: 0.4933 - val_loss: 2.0004 - learning_rate: 1.0000e-04\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 8/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8304 - loss: 0.3683\nEpoch 8: val_accuracy improved from 0.49456 to 0.50665, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8304 - loss: 0.3683 - val_accuracy: 0.5067 - val_loss: 1.7710 - learning_rate: 1.0000e-04\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 9/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8597 - loss: 0.3422\nEpoch 9: val_accuracy improved from 0.50665 to 0.75574, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 128ms/step - accuracy: 0.8596 - loss: 0.3423 - val_accuracy: 0.7557 - val_loss: 0.5654 - learning_rate: 1.0000e-04\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 10/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8674 - loss: 0.3221\nEpoch 10: val_accuracy did not improve from 0.75574\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 120ms/step - accuracy: 0.8673 - loss: 0.3223 - val_accuracy: 0.6626 - val_loss: 0.8634 - learning_rate: 1.0000e-04\n\nEpoch 11: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 11/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8636 - loss: 0.3203\nEpoch 11: val_accuracy improved from 0.75574 to 0.81137, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 128ms/step - accuracy: 0.8637 - loss: 0.3201 - val_accuracy: 0.8114 - val_loss: 0.4388 - learning_rate: 5.0000e-05\n\nEpoch 12: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 12/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8723 - loss: 0.3036\nEpoch 12: val_accuracy did not improve from 0.81137\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 120ms/step - accuracy: 0.8724 - loss: 0.3035 - val_accuracy: 0.8053 - val_loss: 0.4359 - learning_rate: 5.0000e-05\n\nEpoch 13: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 13/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8827 - loss: 0.2675\nEpoch 13: val_accuracy improved from 0.81137 to 0.84281, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.8827 - loss: 0.2676 - val_accuracy: 0.8428 - val_loss: 0.3644 - learning_rate: 5.0000e-05\n\nEpoch 14: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 14/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8824 - loss: 0.2726\nEpoch 14: val_accuracy did not improve from 0.84281\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8824 - loss: 0.2727 - val_accuracy: 0.8089 - val_loss: 0.4641 - learning_rate: 5.0000e-05\n\nEpoch 15: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 15/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8873 - loss: 0.2624\nEpoch 15: val_accuracy improved from 0.84281 to 0.88150, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 131ms/step - accuracy: 0.8872 - loss: 0.2625 - val_accuracy: 0.8815 - val_loss: 0.2817 - learning_rate: 5.0000e-05\n\nEpoch 16: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 16/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8944 - loss: 0.2633\nEpoch 16: val_accuracy did not improve from 0.88150\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8943 - loss: 0.2634 - val_accuracy: 0.8416 - val_loss: 0.3469 - learning_rate: 5.0000e-05\n\nEpoch 17: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 17/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9024 - loss: 0.2353\nEpoch 17: val_accuracy did not improve from 0.88150\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.9024 - loss: 0.2353 - val_accuracy: 0.8646 - val_loss: 0.3282 - learning_rate: 5.0000e-05\n\nEpoch 18: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 18/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9192 - loss: 0.2244\nEpoch 18: val_accuracy did not improve from 0.88150\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9191 - loss: 0.2245 - val_accuracy: 0.7400 - val_loss: 0.5541 - learning_rate: 5.0000e-05\n\nEpoch 19: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 19/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9214 - loss: 0.2191\nEpoch 19: val_accuracy did not improve from 0.88150\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9213 - loss: 0.2192 - val_accuracy: 0.8646 - val_loss: 0.2935 - learning_rate: 5.0000e-05\n\nEpoch 20: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 20/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9147 - loss: 0.2199\nEpoch 20: val_accuracy did not improve from 0.88150\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9148 - loss: 0.2198 - val_accuracy: 0.8440 - val_loss: 0.3639 - learning_rate: 5.0000e-05\n\nEpoch 21: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 21/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9231 - loss: 0.2139\nEpoch 21: val_accuracy improved from 0.88150 to 0.92261, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 131ms/step - accuracy: 0.9231 - loss: 0.2138 - val_accuracy: 0.9226 - val_loss: 0.2002 - learning_rate: 5.0000e-05\n\nEpoch 22: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 22/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9039 - loss: 0.2347\nEpoch 22: val_accuracy improved from 0.92261 to 0.93349, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9040 - loss: 0.2344 - val_accuracy: 0.9335 - val_loss: 0.1815 - learning_rate: 5.0000e-05\n\nEpoch 23: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 23/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9243 - loss: 0.1803\nEpoch 23: val_accuracy did not improve from 0.93349\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9244 - loss: 0.1803 - val_accuracy: 0.8912 - val_loss: 0.2648 - learning_rate: 5.0000e-05\n\nEpoch 24: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 24/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9387 - loss: 0.1784\nEpoch 24: val_accuracy did not improve from 0.93349\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9387 - loss: 0.1783 - val_accuracy: 0.8259 - val_loss: 0.3886 - learning_rate: 5.0000e-05\n\nEpoch 25: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 25/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9375 - loss: 0.1825\nEpoch 25: val_accuracy did not improve from 0.93349\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9376 - loss: 0.1822 - val_accuracy: 0.9045 - val_loss: 0.2266 - learning_rate: 5.0000e-05\n\nEpoch 26: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 26/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9563 - loss: 0.1333\nEpoch 26: val_accuracy did not improve from 0.93349\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9562 - loss: 0.1335 - val_accuracy: 0.9274 - val_loss: 0.1991 - learning_rate: 5.0000e-05\n\nEpoch 27: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 27/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9521 - loss: 0.1402\nEpoch 27: val_accuracy did not improve from 0.93349\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9521 - loss: 0.1402 - val_accuracy: 0.8525 - val_loss: 0.3580 - learning_rate: 5.0000e-05\n\nEpoch 28: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 28/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9581 - loss: 0.1401\nEpoch 28: val_accuracy did not improve from 0.93349\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9581 - loss: 0.1402 - val_accuracy: 0.9250 - val_loss: 0.1789 - learning_rate: 5.0000e-05\n\nEpoch 29: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 29/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9600 - loss: 0.1232\nEpoch 29: val_accuracy improved from 0.93349 to 0.97944, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9600 - loss: 0.1231 - val_accuracy: 0.9794 - val_loss: 0.1106 - learning_rate: 5.0000e-05\n\nEpoch 30: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 30/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9537 - loss: 0.1265\nEpoch 30: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9537 - loss: 0.1265 - val_accuracy: 0.9601 - val_loss: 0.1245 - learning_rate: 5.0000e-05\n\nEpoch 31: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 31/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9624 - loss: 0.1127\nEpoch 31: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9624 - loss: 0.1127 - val_accuracy: 0.9710 - val_loss: 0.1074 - learning_rate: 1.0000e-05\n\nEpoch 32: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 32/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9715 - loss: 0.0876\nEpoch 32: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9715 - loss: 0.0876 - val_accuracy: 0.9746 - val_loss: 0.0909 - learning_rate: 1.0000e-05\n\nEpoch 33: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 33/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9787 - loss: 0.0854\nEpoch 33: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9787 - loss: 0.0854 - val_accuracy: 0.9528 - val_loss: 0.1204 - learning_rate: 1.0000e-05\n\nEpoch 34: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 34/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9791 - loss: 0.0935\nEpoch 34: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9790 - loss: 0.0936 - val_accuracy: 0.9746 - val_loss: 0.0964 - learning_rate: 1.0000e-05\n\nEpoch 35: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 35/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9734 - loss: 0.0890\nEpoch 35: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9734 - loss: 0.0889 - val_accuracy: 0.9734 - val_loss: 0.0836 - learning_rate: 1.0000e-05\n\nEpoch 36: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 36/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9732 - loss: 0.0961\nEpoch 36: val_accuracy did not improve from 0.97944\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9733 - loss: 0.0960 - val_accuracy: 0.9734 - val_loss: 0.0887 - learning_rate: 1.0000e-05\n\nEpoch 37: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 37/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9722 - loss: 0.1016\nEpoch 37: val_accuracy improved from 0.97944 to 0.98186, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9722 - loss: 0.1015 - val_accuracy: 0.9819 - val_loss: 0.0716 - learning_rate: 1.0000e-05\n\nEpoch 38: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 38/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9872 - loss: 0.0675\nEpoch 38: val_accuracy did not improve from 0.98186\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9872 - loss: 0.0675 - val_accuracy: 0.9770 - val_loss: 0.0788 - learning_rate: 1.0000e-05\n\nEpoch 39: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 39/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9824 - loss: 0.0703\nEpoch 39: val_accuracy did not improve from 0.98186\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9824 - loss: 0.0703 - val_accuracy: 0.9746 - val_loss: 0.0811 - learning_rate: 1.0000e-05\n\nEpoch 40: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 40/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9798 - loss: 0.0741\nEpoch 40: val_accuracy improved from 0.98186 to 0.98307, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9799 - loss: 0.0741 - val_accuracy: 0.9831 - val_loss: 0.0594 - learning_rate: 1.0000e-05\n\nEpoch 41: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 41/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9785 - loss: 0.0805\nEpoch 41: val_accuracy did not improve from 0.98307\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9785 - loss: 0.0805 - val_accuracy: 0.9794 - val_loss: 0.0754 - learning_rate: 1.0000e-05\n\nEpoch 42: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 42/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9775 - loss: 0.0730\nEpoch 42: val_accuracy did not improve from 0.98307\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9775 - loss: 0.0731 - val_accuracy: 0.9831 - val_loss: 0.0683 - learning_rate: 1.0000e-05\n\nEpoch 43: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 43/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9821 - loss: 0.0774\nEpoch 43: val_accuracy did not improve from 0.98307\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9820 - loss: 0.0774 - val_accuracy: 0.9758 - val_loss: 0.0843 - learning_rate: 1.0000e-05\n\nEpoch 44: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 44/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9818 - loss: 0.0716\nEpoch 44: val_accuracy improved from 0.98307 to 0.98428, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9818 - loss: 0.0715 - val_accuracy: 0.9843 - val_loss: 0.0748 - learning_rate: 1.0000e-05\n\nEpoch 45: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 45/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9716 - loss: 0.1030\nEpoch 45: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9717 - loss: 0.1028 - val_accuracy: 0.9843 - val_loss: 0.0650 - learning_rate: 1.0000e-05\n\nEpoch 46: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 46/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9901 - loss: 0.0609\nEpoch 46: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9901 - loss: 0.0610 - val_accuracy: 0.9589 - val_loss: 0.1100 - learning_rate: 1.0000e-05\n\nEpoch 47: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 47/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9899 - loss: 0.0528\nEpoch 47: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9899 - loss: 0.0529 - val_accuracy: 0.9770 - val_loss: 0.0714 - learning_rate: 1.0000e-05\n\nEpoch 48: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 48/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9853 - loss: 0.0628\nEpoch 48: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9853 - loss: 0.0627 - val_accuracy: 0.9807 - val_loss: 0.0736 - learning_rate: 1.0000e-05\n\nEpoch 49: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 49/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9876 - loss: 0.0574\nEpoch 49: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9876 - loss: 0.0575 - val_accuracy: 0.9698 - val_loss: 0.0934 - learning_rate: 1.0000e-05\n\nEpoch 50: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 50/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9832 - loss: 0.0705\nEpoch 50: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9832 - loss: 0.0704 - val_accuracy: 0.9625 - val_loss: 0.1171 - learning_rate: 1.0000e-05\n\nEpoch 51: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 51/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9824 - loss: 0.0770\nEpoch 51: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9824 - loss: 0.0769 - val_accuracy: 0.9746 - val_loss: 0.0827 - learning_rate: 5.0000e-06\n\nEpoch 52: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 52/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9824 - loss: 0.0699\nEpoch 52: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9824 - loss: 0.0698 - val_accuracy: 0.9794 - val_loss: 0.0657 - learning_rate: 5.0000e-06\n\nEpoch 53: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 53/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9838 - loss: 0.0590\nEpoch 53: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9838 - loss: 0.0591 - val_accuracy: 0.9831 - val_loss: 0.0695 - learning_rate: 5.0000e-06\n\nEpoch 54: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 54/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9874 - loss: 0.0615\nEpoch 54: val_accuracy did not improve from 0.98428\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9873 - loss: 0.0615 - val_accuracy: 0.9843 - val_loss: 0.0658 - learning_rate: 5.0000e-06\n\nEpoch 55: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 55/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9932 - loss: 0.0494\nEpoch 55: val_accuracy improved from 0.98428 to 0.98549, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9931 - loss: 0.0496 - val_accuracy: 0.9855 - val_loss: 0.0560 - learning_rate: 5.0000e-06\n\nEpoch 56: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 56/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9907 - loss: 0.0499\nEpoch 56: val_accuracy improved from 0.98549 to 0.98791, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9907 - loss: 0.0500 - val_accuracy: 0.9879 - val_loss: 0.0564 - learning_rate: 5.0000e-06\n\nEpoch 57: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 57/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9949 - loss: 0.0386\nEpoch 57: val_accuracy did not improve from 0.98791\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9949 - loss: 0.0387 - val_accuracy: 0.9879 - val_loss: 0.0567 - learning_rate: 5.0000e-06\n\nEpoch 58: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 58/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9908 - loss: 0.0497\nEpoch 58: val_accuracy improved from 0.98791 to 0.98912, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9908 - loss: 0.0497 - val_accuracy: 0.9891 - val_loss: 0.0573 - learning_rate: 5.0000e-06\n\nEpoch 59: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 59/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9864 - loss: 0.0622\nEpoch 59: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9864 - loss: 0.0621 - val_accuracy: 0.9879 - val_loss: 0.0517 - learning_rate: 5.0000e-06\n\nEpoch 60: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 60/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9862 - loss: 0.0574\nEpoch 60: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9862 - loss: 0.0574 - val_accuracy: 0.9879 - val_loss: 0.0596 - learning_rate: 5.0000e-06\n\nEpoch 61: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 61/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9905 - loss: 0.0510\nEpoch 61: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9904 - loss: 0.0511 - val_accuracy: 0.9843 - val_loss: 0.0577 - learning_rate: 5.0000e-06\n\nEpoch 62: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 62/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9889 - loss: 0.0527\nEpoch 62: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9889 - loss: 0.0527 - val_accuracy: 0.9855 - val_loss: 0.0551 - learning_rate: 5.0000e-06\n\nEpoch 63: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 63/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9822 - loss: 0.0615\nEpoch 63: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9822 - loss: 0.0614 - val_accuracy: 0.9758 - val_loss: 0.0740 - learning_rate: 5.0000e-06\n\nEpoch 64: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 64/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9887 - loss: 0.0475\nEpoch 64: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9886 - loss: 0.0476 - val_accuracy: 0.9891 - val_loss: 0.0558 - learning_rate: 5.0000e-06\n\nEpoch 65: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 65/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9817 - loss: 0.0640\nEpoch 65: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9817 - loss: 0.0640 - val_accuracy: 0.9782 - val_loss: 0.0616 - learning_rate: 5.0000e-06\n\nEpoch 66: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 66/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9965 - loss: 0.0381\nEpoch 66: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9965 - loss: 0.0381 - val_accuracy: 0.9867 - val_loss: 0.0526 - learning_rate: 5.0000e-06\n\nEpoch 67: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 67/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9949 - loss: 0.0407\nEpoch 67: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9949 - loss: 0.0407 - val_accuracy: 0.9794 - val_loss: 0.0633 - learning_rate: 5.0000e-06\n\nEpoch 68: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 68/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9862 - loss: 0.0619\nEpoch 68: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9862 - loss: 0.0618 - val_accuracy: 0.9879 - val_loss: 0.0534 - learning_rate: 5.0000e-06\n\nEpoch 69: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 69/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9982 - loss: 0.0310\nEpoch 69: val_accuracy did not improve from 0.98912\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9981 - loss: 0.0311 - val_accuracy: 0.9855 - val_loss: 0.0534 - learning_rate: 5.0000e-06\n\nEpoch 70: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 70/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9921 - loss: 0.0409\nEpoch 70: val_accuracy improved from 0.98912 to 0.99033, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9921 - loss: 0.0409 - val_accuracy: 0.9903 - val_loss: 0.0471 - learning_rate: 5.0000e-06\n\nEpoch 71: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 71/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9878 - loss: 0.0524\nEpoch 71: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9877 - loss: 0.0524 - val_accuracy: 0.9891 - val_loss: 0.0495 - learning_rate: 5.0000e-06\n\nEpoch 72: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 72/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9904 - loss: 0.0487\nEpoch 72: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9904 - loss: 0.0488 - val_accuracy: 0.9770 - val_loss: 0.0630 - learning_rate: 5.0000e-06\n\nEpoch 73: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 73/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9925 - loss: 0.0439\nEpoch 73: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9925 - loss: 0.0440 - val_accuracy: 0.9903 - val_loss: 0.0521 - learning_rate: 5.0000e-06\n\nEpoch 74: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 74/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9875 - loss: 0.0448\nEpoch 74: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9875 - loss: 0.0448 - val_accuracy: 0.9831 - val_loss: 0.0624 - learning_rate: 5.0000e-06\n\nEpoch 75: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 75/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9916 - loss: 0.0477\nEpoch 75: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9916 - loss: 0.0477 - val_accuracy: 0.9843 - val_loss: 0.0610 - learning_rate: 5.0000e-06\n\nEpoch 76: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 76/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9876 - loss: 0.0495\nEpoch 76: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9877 - loss: 0.0495 - val_accuracy: 0.9722 - val_loss: 0.0784 - learning_rate: 5.0000e-06\n\nEpoch 77: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 77/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9824 - loss: 0.0582\nEpoch 77: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9824 - loss: 0.0581 - val_accuracy: 0.9782 - val_loss: 0.0671 - learning_rate: 5.0000e-06\n\nEpoch 78: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 78/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9902 - loss: 0.0461\nEpoch 78: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9902 - loss: 0.0461 - val_accuracy: 0.9903 - val_loss: 0.0465 - learning_rate: 5.0000e-06\n\nEpoch 79: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 79/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9885 - loss: 0.0424\nEpoch 79: val_accuracy did not improve from 0.99033\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9885 - loss: 0.0425 - val_accuracy: 0.9867 - val_loss: 0.0567 - learning_rate: 5.0000e-06\n\nEpoch 80: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 80/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9898 - loss: 0.0520\nEpoch 80: val_accuracy improved from 0.99033 to 0.99154, saving model to best_model_fold_1.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9898 - loss: 0.0520 - val_accuracy: 0.9915 - val_loss: 0.0426 - learning_rate: 5.0000e-06\n\nEpoch 81: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 81/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9946 - loss: 0.0452\nEpoch 81: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9946 - loss: 0.0453 - val_accuracy: 0.9891 - val_loss: 0.0452 - learning_rate: 5.0000e-06\n\nEpoch 82: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 82/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9939 - loss: 0.0449\nEpoch 82: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9939 - loss: 0.0450 - val_accuracy: 0.9867 - val_loss: 0.0453 - learning_rate: 5.0000e-06\n\nEpoch 83: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 83/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9886 - loss: 0.0528\nEpoch 83: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9886 - loss: 0.0528 - val_accuracy: 0.9891 - val_loss: 0.0448 - learning_rate: 5.0000e-06\n\nEpoch 84: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 84/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9962 - loss: 0.0316\nEpoch 84: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9962 - loss: 0.0317 - val_accuracy: 0.9819 - val_loss: 0.0523 - learning_rate: 5.0000e-06\n\nEpoch 85: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 85/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9936 - loss: 0.0387\nEpoch 85: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9936 - loss: 0.0387 - val_accuracy: 0.9915 - val_loss: 0.0439 - learning_rate: 5.0000e-06\n\nEpoch 86: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 86/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9870 - loss: 0.0494\nEpoch 86: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9870 - loss: 0.0494 - val_accuracy: 0.9770 - val_loss: 0.0716 - learning_rate: 5.0000e-06\n\nEpoch 87: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 87/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9922 - loss: 0.0415\nEpoch 87: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9922 - loss: 0.0415 - val_accuracy: 0.9855 - val_loss: 0.0527 - learning_rate: 5.0000e-06\n\nEpoch 88: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 88/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9910 - loss: 0.0389\nEpoch 88: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9911 - loss: 0.0389 - val_accuracy: 0.9843 - val_loss: 0.0613 - learning_rate: 5.0000e-06\n\nEpoch 89: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 89/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9941 - loss: 0.0440\nEpoch 89: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9941 - loss: 0.0440 - val_accuracy: 0.9879 - val_loss: 0.0507 - learning_rate: 5.0000e-06\n\nEpoch 90: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 90/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9928 - loss: 0.0383\nEpoch 90: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9928 - loss: 0.0383 - val_accuracy: 0.9746 - val_loss: 0.0644 - learning_rate: 5.0000e-06\n\nEpoch 91: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 91/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9957 - loss: 0.0361\nEpoch 91: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9957 - loss: 0.0361 - val_accuracy: 0.9807 - val_loss: 0.0660 - learning_rate: 5.0000e-06\n\nEpoch 92: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 92/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9974 - loss: 0.0304\nEpoch 92: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9974 - loss: 0.0304 - val_accuracy: 0.9879 - val_loss: 0.0518 - learning_rate: 5.0000e-06\n\nEpoch 93: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 93/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9960 - loss: 0.0320\nEpoch 93: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9960 - loss: 0.0321 - val_accuracy: 0.9903 - val_loss: 0.0473 - learning_rate: 5.0000e-06\n\nEpoch 94: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 94/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9913 - loss: 0.0482\nEpoch 94: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9913 - loss: 0.0482 - val_accuracy: 0.9915 - val_loss: 0.0458 - learning_rate: 5.0000e-06\n\nEpoch 95: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 95/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9928 - loss: 0.0475\nEpoch 95: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9928 - loss: 0.0475 - val_accuracy: 0.9855 - val_loss: 0.0491 - learning_rate: 5.0000e-06\n\nEpoch 96: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 96/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9930 - loss: 0.0400\nEpoch 96: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9930 - loss: 0.0401 - val_accuracy: 0.9867 - val_loss: 0.0471 - learning_rate: 5.0000e-06\n\nEpoch 97: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 97/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9925 - loss: 0.0476\nEpoch 97: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9925 - loss: 0.0476 - val_accuracy: 0.9855 - val_loss: 0.0488 - learning_rate: 5.0000e-06\n\nEpoch 98: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 98/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9951 - loss: 0.0371\nEpoch 98: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9951 - loss: 0.0370 - val_accuracy: 0.9843 - val_loss: 0.0472 - learning_rate: 5.0000e-06\n\nEpoch 99: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 99/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9942 - loss: 0.0399\nEpoch 99: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9941 - loss: 0.0399 - val_accuracy: 0.9903 - val_loss: 0.0404 - learning_rate: 5.0000e-06\n\nEpoch 100: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 100/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9941 - loss: 0.0349\nEpoch 100: val_accuracy did not improve from 0.99154\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9941 - loss: 0.0349 - val_accuracy: 0.9807 - val_loss: 0.0549 - learning_rate: 5.0000e-06\nSaved the complete model for fold 1 at complete_model_fold_1.keras\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 194ms/step\nValidation accuracy for fold 1: 0.9915\nValidation loss for fold 1: 0.0427\nTraining fold 2...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.5440 - loss: 0.8462\nEpoch 1: val_accuracy improved from -inf to 0.49577, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 436ms/step - accuracy: 0.5441 - loss: 0.8459 - val_accuracy: 0.4958 - val_loss: 0.7474 - learning_rate: 1.0000e-04\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 2/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5563 - loss: 0.7424\nEpoch 2: val_accuracy did not improve from 0.49577\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.5565 - loss: 0.7422 - val_accuracy: 0.4958 - val_loss: 0.9712 - learning_rate: 1.0000e-04\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 3/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6379 - loss: 0.6454\nEpoch 3: val_accuracy did not improve from 0.49577\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.6381 - loss: 0.6453 - val_accuracy: 0.4958 - val_loss: 0.6937 - learning_rate: 1.0000e-04\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 4/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6893 - loss: 0.5853\nEpoch 4: val_accuracy did not improve from 0.49577\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.6895 - loss: 0.5851 - val_accuracy: 0.4958 - val_loss: 1.7575 - learning_rate: 1.0000e-04\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 5/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7519 - loss: 0.5343\nEpoch 5: val_accuracy did not improve from 0.49577\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.7518 - loss: 0.5342 - val_accuracy: 0.4958 - val_loss: 1.9157 - learning_rate: 1.0000e-04\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 6/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7550 - loss: 0.5262\nEpoch 6: val_accuracy did not improve from 0.49577\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.7551 - loss: 0.5260 - val_accuracy: 0.4958 - val_loss: 1.2612 - learning_rate: 1.0000e-04\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 7/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7613 - loss: 0.4961\nEpoch 7: val_accuracy did not improve from 0.49577\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.7614 - loss: 0.4960 - val_accuracy: 0.4958 - val_loss: 1.7743 - learning_rate: 1.0000e-04\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 8/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7908 - loss: 0.4472\nEpoch 8: val_accuracy improved from 0.49577 to 0.53083, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.7908 - loss: 0.4471 - val_accuracy: 0.5308 - val_loss: 1.1773 - learning_rate: 1.0000e-04\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 9/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7935 - loss: 0.4221\nEpoch 9: val_accuracy did not improve from 0.53083\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.7937 - loss: 0.4220 - val_accuracy: 0.5260 - val_loss: 1.1828 - learning_rate: 1.0000e-04\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 10/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8264 - loss: 0.4028\nEpoch 10: val_accuracy improved from 0.53083 to 0.58041, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.8264 - loss: 0.4027 - val_accuracy: 0.5804 - val_loss: 1.2612 - learning_rate: 1.0000e-04\n\nEpoch 11: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 11/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8476 - loss: 0.3533\nEpoch 11: val_accuracy did not improve from 0.58041\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8477 - loss: 0.3533 - val_accuracy: 0.5804 - val_loss: 1.3606 - learning_rate: 5.0000e-05\n\nEpoch 12: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 12/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8483 - loss: 0.3499\nEpoch 12: val_accuracy improved from 0.58041 to 0.60339, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8484 - loss: 0.3499 - val_accuracy: 0.6034 - val_loss: 1.0448 - learning_rate: 5.0000e-05\n\nEpoch 13: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 13/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8552 - loss: 0.3423\nEpoch 13: val_accuracy improved from 0.60339 to 0.80290, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.8552 - loss: 0.3423 - val_accuracy: 0.8029 - val_loss: 0.5099 - learning_rate: 5.0000e-05\n\nEpoch 14: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 14/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8471 - loss: 0.3427\nEpoch 14: val_accuracy did not improve from 0.80290\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.8472 - loss: 0.3425 - val_accuracy: 0.7836 - val_loss: 0.4846 - learning_rate: 5.0000e-05\n\nEpoch 15: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 15/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8702 - loss: 0.3189\nEpoch 15: val_accuracy did not improve from 0.80290\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8701 - loss: 0.3189 - val_accuracy: 0.7993 - val_loss: 0.4544 - learning_rate: 5.0000e-05\n\nEpoch 16: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 16/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8434 - loss: 0.3571\nEpoch 16: val_accuracy improved from 0.80290 to 0.80532, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.8435 - loss: 0.3568 - val_accuracy: 0.8053 - val_loss: 0.5035 - learning_rate: 5.0000e-05\n\nEpoch 17: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 17/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8775 - loss: 0.2933\nEpoch 17: val_accuracy improved from 0.80532 to 0.89722, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8776 - loss: 0.2932 - val_accuracy: 0.8972 - val_loss: 0.2822 - learning_rate: 5.0000e-05\n\nEpoch 18: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 18/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8735 - loss: 0.3080\nEpoch 18: val_accuracy did not improve from 0.89722\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8737 - loss: 0.3077 - val_accuracy: 0.8283 - val_loss: 0.3842 - learning_rate: 5.0000e-05\n\nEpoch 19: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 19/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9132 - loss: 0.2416\nEpoch 19: val_accuracy did not improve from 0.89722\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9131 - loss: 0.2417 - val_accuracy: 0.7678 - val_loss: 0.5328 - learning_rate: 5.0000e-05\n\nEpoch 20: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 20/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8926 - loss: 0.2766\nEpoch 20: val_accuracy did not improve from 0.89722\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8926 - loss: 0.2766 - val_accuracy: 0.7884 - val_loss: 0.4830 - learning_rate: 5.0000e-05\n\nEpoch 21: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 21/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8895 - loss: 0.2814\nEpoch 21: val_accuracy improved from 0.89722 to 0.91173, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.8896 - loss: 0.2811 - val_accuracy: 0.9117 - val_loss: 0.2344 - learning_rate: 5.0000e-05\n\nEpoch 22: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 22/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9041 - loss: 0.2343\nEpoch 22: val_accuracy did not improve from 0.91173\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9040 - loss: 0.2344 - val_accuracy: 0.8174 - val_loss: 0.3837 - learning_rate: 5.0000e-05\n\nEpoch 23: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 23/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9048 - loss: 0.2399\nEpoch 23: val_accuracy did not improve from 0.91173\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9049 - loss: 0.2397 - val_accuracy: 0.8476 - val_loss: 0.3524 - learning_rate: 5.0000e-05\n\nEpoch 24: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 24/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9000 - loss: 0.2444\nEpoch 24: val_accuracy did not improve from 0.91173\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9000 - loss: 0.2442 - val_accuracy: 0.8307 - val_loss: 0.3888 - learning_rate: 5.0000e-05\n\nEpoch 25: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 25/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9409 - loss: 0.1921\nEpoch 25: val_accuracy improved from 0.91173 to 0.91415, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9408 - loss: 0.1921 - val_accuracy: 0.9141 - val_loss: 0.2623 - learning_rate: 5.0000e-05\n\nEpoch 26: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 26/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9279 - loss: 0.1850\nEpoch 26: val_accuracy did not improve from 0.91415\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9279 - loss: 0.1852 - val_accuracy: 0.5586 - val_loss: 1.2519 - learning_rate: 5.0000e-05\n\nEpoch 27: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 27/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9361 - loss: 0.1809\nEpoch 27: val_accuracy did not improve from 0.91415\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9361 - loss: 0.1809 - val_accuracy: 0.8271 - val_loss: 0.4279 - learning_rate: 5.0000e-05\n\nEpoch 28: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 28/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9223 - loss: 0.1896\nEpoch 28: val_accuracy did not improve from 0.91415\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9224 - loss: 0.1895 - val_accuracy: 0.7715 - val_loss: 0.5756 - learning_rate: 5.0000e-05\n\nEpoch 29: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 29/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9305 - loss: 0.1895\nEpoch 29: val_accuracy improved from 0.91415 to 0.93954, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9304 - loss: 0.1896 - val_accuracy: 0.9395 - val_loss: 0.1824 - learning_rate: 5.0000e-05\n\nEpoch 30: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 30/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9368 - loss: 0.1631\nEpoch 30: val_accuracy did not improve from 0.93954\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9369 - loss: 0.1630 - val_accuracy: 0.7352 - val_loss: 0.8441 - learning_rate: 5.0000e-05\n\nEpoch 31: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 31/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9405 - loss: 0.1724\nEpoch 31: val_accuracy improved from 0.93954 to 0.95768, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 131ms/step - accuracy: 0.9405 - loss: 0.1725 - val_accuracy: 0.9577 - val_loss: 0.1146 - learning_rate: 1.0000e-05\n\nEpoch 32: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 32/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9415 - loss: 0.1567\nEpoch 32: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9416 - loss: 0.1566 - val_accuracy: 0.9492 - val_loss: 0.1353 - learning_rate: 1.0000e-05\n\nEpoch 33: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 33/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9573 - loss: 0.1357\nEpoch 33: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9573 - loss: 0.1357 - val_accuracy: 0.9565 - val_loss: 0.1373 - learning_rate: 1.0000e-05\n\nEpoch 34: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 34/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9385 - loss: 0.1781\nEpoch 34: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9386 - loss: 0.1779 - val_accuracy: 0.9008 - val_loss: 0.2258 - learning_rate: 1.0000e-05\n\nEpoch 35: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 35/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9598 - loss: 0.1228\nEpoch 35: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9597 - loss: 0.1228 - val_accuracy: 0.9504 - val_loss: 0.1417 - learning_rate: 1.0000e-05\n\nEpoch 36: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 36/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9518 - loss: 0.1421\nEpoch 36: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9518 - loss: 0.1420 - val_accuracy: 0.9226 - val_loss: 0.1903 - learning_rate: 1.0000e-05\n\nEpoch 37: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 37/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9509 - loss: 0.1484\nEpoch 37: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9509 - loss: 0.1484 - val_accuracy: 0.9577 - val_loss: 0.1220 - learning_rate: 1.0000e-05\n\nEpoch 38: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 38/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9574 - loss: 0.1418\nEpoch 38: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9574 - loss: 0.1417 - val_accuracy: 0.9299 - val_loss: 0.1787 - learning_rate: 1.0000e-05\n\nEpoch 39: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 39/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9495 - loss: 0.1366\nEpoch 39: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9496 - loss: 0.1366 - val_accuracy: 0.9250 - val_loss: 0.2073 - learning_rate: 1.0000e-05\n\nEpoch 40: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 40/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9681 - loss: 0.1158\nEpoch 40: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9680 - loss: 0.1158 - val_accuracy: 0.9371 - val_loss: 0.1588 - learning_rate: 1.0000e-05\n\nEpoch 41: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 41/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9759 - loss: 0.0975\nEpoch 41: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9758 - loss: 0.0976 - val_accuracy: 0.9371 - val_loss: 0.1686 - learning_rate: 1.0000e-05\n\nEpoch 42: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 42/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9740 - loss: 0.1048\nEpoch 42: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9739 - loss: 0.1049 - val_accuracy: 0.9553 - val_loss: 0.1178 - learning_rate: 1.0000e-05\n\nEpoch 43: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 43/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9710 - loss: 0.1165\nEpoch 43: val_accuracy did not improve from 0.95768\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9710 - loss: 0.1167 - val_accuracy: 0.9565 - val_loss: 0.1113 - learning_rate: 1.0000e-05\n\nEpoch 44: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 44/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9757 - loss: 0.0908\nEpoch 44: val_accuracy improved from 0.95768 to 0.96856, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9757 - loss: 0.0909 - val_accuracy: 0.9686 - val_loss: 0.1000 - learning_rate: 1.0000e-05\n\nEpoch 45: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 45/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9696 - loss: 0.1110\nEpoch 45: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9696 - loss: 0.1110 - val_accuracy: 0.9432 - val_loss: 0.1528 - learning_rate: 1.0000e-05\n\nEpoch 46: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 46/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9643 - loss: 0.1240\nEpoch 46: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9642 - loss: 0.1241 - val_accuracy: 0.9674 - val_loss: 0.0970 - learning_rate: 1.0000e-05\n\nEpoch 47: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 47/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9687 - loss: 0.1165\nEpoch 47: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9687 - loss: 0.1165 - val_accuracy: 0.9432 - val_loss: 0.1620 - learning_rate: 1.0000e-05\n\nEpoch 48: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 48/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9741 - loss: 0.0981\nEpoch 48: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9741 - loss: 0.0982 - val_accuracy: 0.9444 - val_loss: 0.1623 - learning_rate: 1.0000e-05\n\nEpoch 49: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 49/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9753 - loss: 0.1022\nEpoch 49: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9752 - loss: 0.1023 - val_accuracy: 0.9468 - val_loss: 0.1356 - learning_rate: 1.0000e-05\n\nEpoch 50: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 50/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9794 - loss: 0.0995\nEpoch 50: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9793 - loss: 0.0994 - val_accuracy: 0.9468 - val_loss: 0.1439 - learning_rate: 1.0000e-05\n\nEpoch 51: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 51/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9770 - loss: 0.1029\nEpoch 51: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9770 - loss: 0.1029 - val_accuracy: 0.9686 - val_loss: 0.0998 - learning_rate: 5.0000e-06\n\nEpoch 52: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 52/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9669 - loss: 0.1077\nEpoch 52: val_accuracy did not improve from 0.96856\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9668 - loss: 0.1077 - val_accuracy: 0.9649 - val_loss: 0.0975 - learning_rate: 5.0000e-06\n\nEpoch 53: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 53/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9771 - loss: 0.0824\nEpoch 53: val_accuracy improved from 0.96856 to 0.97340, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 131ms/step - accuracy: 0.9771 - loss: 0.0824 - val_accuracy: 0.9734 - val_loss: 0.0856 - learning_rate: 5.0000e-06\n\nEpoch 54: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 54/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9814 - loss: 0.0803\nEpoch 54: val_accuracy did not improve from 0.97340\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9814 - loss: 0.0804 - val_accuracy: 0.9698 - val_loss: 0.0911 - learning_rate: 5.0000e-06\n\nEpoch 55: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 55/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9789 - loss: 0.0824\nEpoch 55: val_accuracy did not improve from 0.97340\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9789 - loss: 0.0824 - val_accuracy: 0.9734 - val_loss: 0.0863 - learning_rate: 5.0000e-06\n\nEpoch 56: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 56/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9713 - loss: 0.1025\nEpoch 56: val_accuracy did not improve from 0.97340\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9713 - loss: 0.1024 - val_accuracy: 0.9710 - val_loss: 0.0912 - learning_rate: 5.0000e-06\n\nEpoch 57: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 57/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9864 - loss: 0.0674\nEpoch 57: val_accuracy improved from 0.97340 to 0.97582, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9864 - loss: 0.0675 - val_accuracy: 0.9758 - val_loss: 0.0768 - learning_rate: 5.0000e-06\n\nEpoch 58: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 58/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9734 - loss: 0.0931\nEpoch 58: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9735 - loss: 0.0930 - val_accuracy: 0.9710 - val_loss: 0.0918 - learning_rate: 5.0000e-06\n\nEpoch 59: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 59/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9641 - loss: 0.1047\nEpoch 59: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9642 - loss: 0.1047 - val_accuracy: 0.9722 - val_loss: 0.0807 - learning_rate: 5.0000e-06\n\nEpoch 60: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 60/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9881 - loss: 0.0678\nEpoch 60: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9880 - loss: 0.0679 - val_accuracy: 0.9589 - val_loss: 0.1255 - learning_rate: 5.0000e-06\n\nEpoch 61: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 61/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9895 - loss: 0.0662\nEpoch 61: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9895 - loss: 0.0662 - val_accuracy: 0.9758 - val_loss: 0.0764 - learning_rate: 5.0000e-06\n\nEpoch 62: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 62/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9802 - loss: 0.0789\nEpoch 62: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9801 - loss: 0.0790 - val_accuracy: 0.9661 - val_loss: 0.1041 - learning_rate: 5.0000e-06\n\nEpoch 63: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 63/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9743 - loss: 0.0842\nEpoch 63: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9743 - loss: 0.0841 - val_accuracy: 0.9722 - val_loss: 0.0799 - learning_rate: 5.0000e-06\n\nEpoch 64: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 64/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9778 - loss: 0.0831\nEpoch 64: val_accuracy did not improve from 0.97582\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9779 - loss: 0.0830 - val_accuracy: 0.9746 - val_loss: 0.0730 - learning_rate: 5.0000e-06\n\nEpoch 65: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 65/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9832 - loss: 0.0776\nEpoch 65: val_accuracy improved from 0.97582 to 0.98549, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9832 - loss: 0.0775 - val_accuracy: 0.9855 - val_loss: 0.0663 - learning_rate: 5.0000e-06\n\nEpoch 66: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 66/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9874 - loss: 0.0620\nEpoch 66: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9874 - loss: 0.0621 - val_accuracy: 0.9794 - val_loss: 0.0698 - learning_rate: 5.0000e-06\n\nEpoch 67: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 67/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9714 - loss: 0.1067\nEpoch 67: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9714 - loss: 0.1066 - val_accuracy: 0.9794 - val_loss: 0.0722 - learning_rate: 5.0000e-06\n\nEpoch 68: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 68/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9881 - loss: 0.0650\nEpoch 68: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9881 - loss: 0.0651 - val_accuracy: 0.9698 - val_loss: 0.0907 - learning_rate: 5.0000e-06\n\nEpoch 69: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 69/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9779 - loss: 0.0805\nEpoch 69: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9779 - loss: 0.0806 - val_accuracy: 0.9843 - val_loss: 0.0695 - learning_rate: 5.0000e-06\n\nEpoch 70: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 70/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9794 - loss: 0.0801\nEpoch 70: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9794 - loss: 0.0801 - val_accuracy: 0.9637 - val_loss: 0.1092 - learning_rate: 5.0000e-06\n\nEpoch 71: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 71/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9778 - loss: 0.0813\nEpoch 71: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9778 - loss: 0.0813 - val_accuracy: 0.9613 - val_loss: 0.1190 - learning_rate: 5.0000e-06\n\nEpoch 72: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 72/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9831 - loss: 0.0736\nEpoch 72: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9831 - loss: 0.0736 - val_accuracy: 0.9831 - val_loss: 0.0665 - learning_rate: 5.0000e-06\n\nEpoch 73: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 73/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9767 - loss: 0.0749\nEpoch 73: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9767 - loss: 0.0750 - val_accuracy: 0.9819 - val_loss: 0.0711 - learning_rate: 5.0000e-06\n\nEpoch 74: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 74/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9776 - loss: 0.0829\nEpoch 74: val_accuracy did not improve from 0.98549\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9776 - loss: 0.0829 - val_accuracy: 0.9819 - val_loss: 0.0753 - learning_rate: 5.0000e-06\n\nEpoch 75: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 75/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9758 - loss: 0.0809\nEpoch 75: val_accuracy improved from 0.98549 to 0.98670, saving model to best_model_fold_2.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 131ms/step - accuracy: 0.9758 - loss: 0.0808 - val_accuracy: 0.9867 - val_loss: 0.0583 - learning_rate: 5.0000e-06\n\nEpoch 76: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 76/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9883 - loss: 0.0573\nEpoch 76: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9883 - loss: 0.0574 - val_accuracy: 0.9855 - val_loss: 0.0594 - learning_rate: 5.0000e-06\n\nEpoch 77: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 77/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9865 - loss: 0.0654\nEpoch 77: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9865 - loss: 0.0655 - val_accuracy: 0.9819 - val_loss: 0.0644 - learning_rate: 5.0000e-06\n\nEpoch 78: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 78/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9815 - loss: 0.0712\nEpoch 78: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9816 - loss: 0.0712 - val_accuracy: 0.9782 - val_loss: 0.0676 - learning_rate: 5.0000e-06\n\nEpoch 79: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 79/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9610 - loss: 0.1122\nEpoch 79: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9611 - loss: 0.1121 - val_accuracy: 0.9819 - val_loss: 0.0692 - learning_rate: 5.0000e-06\n\nEpoch 80: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 80/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9890 - loss: 0.0580\nEpoch 80: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9890 - loss: 0.0580 - val_accuracy: 0.9770 - val_loss: 0.0753 - learning_rate: 5.0000e-06\n\nEpoch 81: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 81/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9876 - loss: 0.0654\nEpoch 81: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9875 - loss: 0.0655 - val_accuracy: 0.9819 - val_loss: 0.0698 - learning_rate: 5.0000e-06\n\nEpoch 82: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 82/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9772 - loss: 0.0784\nEpoch 82: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9773 - loss: 0.0784 - val_accuracy: 0.9819 - val_loss: 0.0595 - learning_rate: 5.0000e-06\n\nEpoch 83: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 83/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9736 - loss: 0.0906\nEpoch 83: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9736 - loss: 0.0905 - val_accuracy: 0.9807 - val_loss: 0.0585 - learning_rate: 5.0000e-06\n\nEpoch 84: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 84/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9845 - loss: 0.0697\nEpoch 84: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9844 - loss: 0.0698 - val_accuracy: 0.9807 - val_loss: 0.0640 - learning_rate: 5.0000e-06\n\nEpoch 85: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 85/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9905 - loss: 0.0541\nEpoch 85: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9904 - loss: 0.0542 - val_accuracy: 0.9674 - val_loss: 0.0943 - learning_rate: 5.0000e-06\n\nEpoch 86: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 86/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9835 - loss: 0.0684\nEpoch 86: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9835 - loss: 0.0684 - val_accuracy: 0.9758 - val_loss: 0.0692 - learning_rate: 5.0000e-06\n\nEpoch 87: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 87/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9855 - loss: 0.0725\nEpoch 87: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9855 - loss: 0.0725 - val_accuracy: 0.9855 - val_loss: 0.0609 - learning_rate: 5.0000e-06\n\nEpoch 88: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 88/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9857 - loss: 0.0588\nEpoch 88: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9856 - loss: 0.0589 - val_accuracy: 0.9734 - val_loss: 0.0668 - learning_rate: 5.0000e-06\n\nEpoch 89: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 89/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9784 - loss: 0.0688\nEpoch 89: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9785 - loss: 0.0687 - val_accuracy: 0.9794 - val_loss: 0.0661 - learning_rate: 5.0000e-06\n\nEpoch 90: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 90/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9902 - loss: 0.0598\nEpoch 90: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9901 - loss: 0.0598 - val_accuracy: 0.9698 - val_loss: 0.0936 - learning_rate: 5.0000e-06\n\nEpoch 91: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 91/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9790 - loss: 0.0784\nEpoch 91: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9790 - loss: 0.0784 - val_accuracy: 0.9843 - val_loss: 0.0639 - learning_rate: 5.0000e-06\n\nEpoch 92: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 92/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9926 - loss: 0.0580\nEpoch 92: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9926 - loss: 0.0580 - val_accuracy: 0.9758 - val_loss: 0.0683 - learning_rate: 5.0000e-06\n\nEpoch 93: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 93/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9859 - loss: 0.0667\nEpoch 93: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9858 - loss: 0.0668 - val_accuracy: 0.9782 - val_loss: 0.0605 - learning_rate: 5.0000e-06\n\nEpoch 94: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 94/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9911 - loss: 0.0586\nEpoch 94: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9910 - loss: 0.0586 - val_accuracy: 0.9782 - val_loss: 0.0624 - learning_rate: 5.0000e-06\n\nEpoch 95: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 95/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9875 - loss: 0.0607\nEpoch 95: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9875 - loss: 0.0606 - val_accuracy: 0.9782 - val_loss: 0.0654 - learning_rate: 5.0000e-06\n\nEpoch 96: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 96/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9877 - loss: 0.0598\nEpoch 96: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9877 - loss: 0.0599 - val_accuracy: 0.9794 - val_loss: 0.0640 - learning_rate: 5.0000e-06\n\nEpoch 97: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 97/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9787 - loss: 0.0723\nEpoch 97: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9787 - loss: 0.0723 - val_accuracy: 0.9819 - val_loss: 0.0616 - learning_rate: 5.0000e-06\n\nEpoch 98: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 98/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9894 - loss: 0.0516\nEpoch 98: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9894 - loss: 0.0516 - val_accuracy: 0.9807 - val_loss: 0.0648 - learning_rate: 5.0000e-06\n\nEpoch 99: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 99/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9901 - loss: 0.0551\nEpoch 99: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9900 - loss: 0.0552 - val_accuracy: 0.9819 - val_loss: 0.0614 - learning_rate: 5.0000e-06\n\nEpoch 100: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 100/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9814 - loss: 0.0759\nEpoch 100: val_accuracy did not improve from 0.98670\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9815 - loss: 0.0757 - val_accuracy: 0.9807 - val_loss: 0.0617 - learning_rate: 5.0000e-06\nSaved the complete model for fold 2 at complete_model_fold_2.keras\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 182ms/step\nValidation accuracy for fold 2: 0.9867\nValidation loss for fold 2: 0.0583\nTraining fold 3...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.5437 - loss: 0.8759\nEpoch 1: val_accuracy improved from -inf to 0.50423, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 442ms/step - accuracy: 0.5441 - loss: 0.8750 - val_accuracy: 0.5042 - val_loss: 0.6994 - learning_rate: 1.0000e-04\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 2/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6480 - loss: 0.6895\nEpoch 2: val_accuracy did not improve from 0.50423\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.6480 - loss: 0.6893 - val_accuracy: 0.4958 - val_loss: 0.7076 - learning_rate: 1.0000e-04\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 3/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6536 - loss: 0.6353\nEpoch 3: val_accuracy did not improve from 0.50423\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.6540 - loss: 0.6349 - val_accuracy: 0.4958 - val_loss: 0.9497 - learning_rate: 1.0000e-04\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 4/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7380 - loss: 0.5427\nEpoch 4: val_accuracy did not improve from 0.50423\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.7380 - loss: 0.5427 - val_accuracy: 0.4958 - val_loss: 2.0474 - learning_rate: 1.0000e-04\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 5/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7759 - loss: 0.4791\nEpoch 5: val_accuracy did not improve from 0.50423\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.7759 - loss: 0.4792 - val_accuracy: 0.4958 - val_loss: 2.3645 - learning_rate: 1.0000e-04\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 6/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7975 - loss: 0.4522\nEpoch 6: val_accuracy did not improve from 0.50423\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.7975 - loss: 0.4522 - val_accuracy: 0.4958 - val_loss: 3.0051 - learning_rate: 1.0000e-04\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 7/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8349 - loss: 0.3933\nEpoch 7: val_accuracy did not improve from 0.50423\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8348 - loss: 0.3934 - val_accuracy: 0.4958 - val_loss: 3.4744 - learning_rate: 1.0000e-04\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 8/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7905 - loss: 0.4349\nEpoch 8: val_accuracy improved from 0.50423 to 0.56348, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.7907 - loss: 0.4346 - val_accuracy: 0.5635 - val_loss: 1.1676 - learning_rate: 1.0000e-04\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 9/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8362 - loss: 0.4112\nEpoch 9: val_accuracy improved from 0.56348 to 0.57316, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8362 - loss: 0.4110 - val_accuracy: 0.5732 - val_loss: 1.4028 - learning_rate: 1.0000e-04\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 10/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8403 - loss: 0.3673\nEpoch 10: val_accuracy did not improve from 0.57316\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8403 - loss: 0.3673 - val_accuracy: 0.5647 - val_loss: 1.2832 - learning_rate: 1.0000e-04\n\nEpoch 11: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 11/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8515 - loss: 0.3534\nEpoch 11: val_accuracy did not improve from 0.57316\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8516 - loss: 0.3534 - val_accuracy: 0.5514 - val_loss: 1.6577 - learning_rate: 5.0000e-05\n\nEpoch 12: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 12/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8689 - loss: 0.3052\nEpoch 12: val_accuracy improved from 0.57316 to 0.61064, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8689 - loss: 0.3053 - val_accuracy: 0.6106 - val_loss: 1.2810 - learning_rate: 5.0000e-05\n\nEpoch 13: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 13/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8688 - loss: 0.3084\nEpoch 13: val_accuracy improved from 0.61064 to 0.70979, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3086 - val_accuracy: 0.7098 - val_loss: 0.9818 - learning_rate: 5.0000e-05\n\nEpoch 14: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 14/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8586 - loss: 0.3173\nEpoch 14: val_accuracy improved from 0.70979 to 0.71463, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8587 - loss: 0.3172 - val_accuracy: 0.7146 - val_loss: 1.0355 - learning_rate: 5.0000e-05\n\nEpoch 15: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 15/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8839 - loss: 0.2778\nEpoch 15: val_accuracy did not improve from 0.71463\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8838 - loss: 0.2779 - val_accuracy: 0.7062 - val_loss: 0.9638 - learning_rate: 5.0000e-05\n\nEpoch 16: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 16/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8928 - loss: 0.2560\nEpoch 16: val_accuracy did not improve from 0.71463\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8928 - loss: 0.2562 - val_accuracy: 0.7037 - val_loss: 0.9699 - learning_rate: 5.0000e-05\n\nEpoch 17: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 17/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8905 - loss: 0.2637\nEpoch 17: val_accuracy did not improve from 0.71463\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8906 - loss: 0.2636 - val_accuracy: 0.6820 - val_loss: 0.8446 - learning_rate: 5.0000e-05\n\nEpoch 18: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 18/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8837 - loss: 0.2666\nEpoch 18: val_accuracy improved from 0.71463 to 0.74123, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.8838 - loss: 0.2664 - val_accuracy: 0.7412 - val_loss: 0.6791 - learning_rate: 5.0000e-05\n\nEpoch 19: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 19/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9011 - loss: 0.2468\nEpoch 19: val_accuracy improved from 0.74123 to 0.78476, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9011 - loss: 0.2468 - val_accuracy: 0.7848 - val_loss: 0.4529 - learning_rate: 5.0000e-05\n\nEpoch 20: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 20/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8981 - loss: 0.2463\nEpoch 20: val_accuracy did not improve from 0.78476\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.8981 - loss: 0.2463 - val_accuracy: 0.7606 - val_loss: 0.4782 - learning_rate: 5.0000e-05\n\nEpoch 21: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 21/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9031 - loss: 0.2260\nEpoch 21: val_accuracy did not improve from 0.78476\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9032 - loss: 0.2260 - val_accuracy: 0.7243 - val_loss: 0.7081 - learning_rate: 5.0000e-05\n\nEpoch 22: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 22/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9175 - loss: 0.2076\nEpoch 22: val_accuracy did not improve from 0.78476\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9175 - loss: 0.2075 - val_accuracy: 0.7533 - val_loss: 0.6699 - learning_rate: 5.0000e-05\n\nEpoch 23: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 23/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9328 - loss: 0.1712\nEpoch 23: val_accuracy improved from 0.78476 to 0.85490, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9327 - loss: 0.1715 - val_accuracy: 0.8549 - val_loss: 0.4451 - learning_rate: 5.0000e-05\n\nEpoch 24: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 24/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9333 - loss: 0.1780\nEpoch 24: val_accuracy improved from 0.85490 to 0.87908, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9332 - loss: 0.1782 - val_accuracy: 0.8791 - val_loss: 0.3140 - learning_rate: 5.0000e-05\n\nEpoch 25: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 25/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9236 - loss: 0.2044\nEpoch 25: val_accuracy improved from 0.87908 to 0.90085, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 132ms/step - accuracy: 0.9236 - loss: 0.2042 - val_accuracy: 0.9008 - val_loss: 0.2588 - learning_rate: 5.0000e-05\n\nEpoch 26: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 26/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9313 - loss: 0.1904\nEpoch 26: val_accuracy improved from 0.90085 to 0.90568, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9313 - loss: 0.1903 - val_accuracy: 0.9057 - val_loss: 0.2668 - learning_rate: 5.0000e-05\n\nEpoch 27: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 27/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9457 - loss: 0.1609\nEpoch 27: val_accuracy did not improve from 0.90568\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9456 - loss: 0.1611 - val_accuracy: 0.8694 - val_loss: 0.3325 - learning_rate: 5.0000e-05\n\nEpoch 28: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 28/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9509 - loss: 0.1435\nEpoch 28: val_accuracy did not improve from 0.90568\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9509 - loss: 0.1435 - val_accuracy: 0.7449 - val_loss: 0.6687 - learning_rate: 5.0000e-05\n\nEpoch 29: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 29/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9466 - loss: 0.1468\nEpoch 29: val_accuracy did not improve from 0.90568\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9466 - loss: 0.1468 - val_accuracy: 0.8271 - val_loss: 0.4686 - learning_rate: 5.0000e-05\n\nEpoch 30: LearningRateScheduler setting learning rate to 5e-05.\nEpoch 30/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9559 - loss: 0.1251\nEpoch 30: val_accuracy did not improve from 0.90568\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9558 - loss: 0.1251 - val_accuracy: 0.8875 - val_loss: 0.2803 - learning_rate: 5.0000e-05\n\nEpoch 31: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 31/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9581 - loss: 0.1217\nEpoch 31: val_accuracy improved from 0.90568 to 0.92745, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9581 - loss: 0.1217 - val_accuracy: 0.9274 - val_loss: 0.1844 - learning_rate: 1.0000e-05\n\nEpoch 32: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 32/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9574 - loss: 0.1296\nEpoch 32: val_accuracy did not improve from 0.92745\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9574 - loss: 0.1296 - val_accuracy: 0.9166 - val_loss: 0.2048 - learning_rate: 1.0000e-05\n\nEpoch 33: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 33/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9748 - loss: 0.0972\nEpoch 33: val_accuracy did not improve from 0.92745\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9748 - loss: 0.0972 - val_accuracy: 0.9214 - val_loss: 0.1920 - learning_rate: 1.0000e-05\n\nEpoch 34: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 34/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9758 - loss: 0.0948\nEpoch 34: val_accuracy did not improve from 0.92745\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9757 - loss: 0.0948 - val_accuracy: 0.9190 - val_loss: 0.2002 - learning_rate: 1.0000e-05\n\nEpoch 35: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 35/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9750 - loss: 0.1039\nEpoch 35: val_accuracy improved from 0.92745 to 0.93712, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9749 - loss: 0.1040 - val_accuracy: 0.9371 - val_loss: 0.1587 - learning_rate: 1.0000e-05\n\nEpoch 36: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 36/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9675 - loss: 0.0950\nEpoch 36: val_accuracy improved from 0.93712 to 0.95163, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9675 - loss: 0.0950 - val_accuracy: 0.9516 - val_loss: 0.1437 - learning_rate: 1.0000e-05\n\nEpoch 37: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 37/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9669 - loss: 0.1146\nEpoch 37: val_accuracy did not improve from 0.95163\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9668 - loss: 0.1146 - val_accuracy: 0.9395 - val_loss: 0.1657 - learning_rate: 1.0000e-05\n\nEpoch 38: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 38/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9693 - loss: 0.0932\nEpoch 38: val_accuracy improved from 0.95163 to 0.95405, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9693 - loss: 0.0932 - val_accuracy: 0.9541 - val_loss: 0.1381 - learning_rate: 1.0000e-05\n\nEpoch 39: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 39/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9782 - loss: 0.0874\nEpoch 39: val_accuracy did not improve from 0.95405\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9781 - loss: 0.0875 - val_accuracy: 0.9444 - val_loss: 0.1547 - learning_rate: 1.0000e-05\n\nEpoch 40: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 40/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9844 - loss: 0.0756\nEpoch 40: val_accuracy did not improve from 0.95405\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9843 - loss: 0.0757 - val_accuracy: 0.9323 - val_loss: 0.1778 - learning_rate: 1.0000e-05\n\nEpoch 41: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 41/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9661 - loss: 0.1046\nEpoch 41: val_accuracy did not improve from 0.95405\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9661 - loss: 0.1046 - val_accuracy: 0.9335 - val_loss: 0.1754 - learning_rate: 1.0000e-05\n\nEpoch 42: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 42/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9779 - loss: 0.0771\nEpoch 42: val_accuracy improved from 0.95405 to 0.95647, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9779 - loss: 0.0771 - val_accuracy: 0.9565 - val_loss: 0.1323 - learning_rate: 1.0000e-05\n\nEpoch 43: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 43/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9798 - loss: 0.0834\nEpoch 43: val_accuracy did not improve from 0.95647\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9798 - loss: 0.0834 - val_accuracy: 0.9468 - val_loss: 0.1526 - learning_rate: 1.0000e-05\n\nEpoch 44: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 44/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9759 - loss: 0.0855\nEpoch 44: val_accuracy did not improve from 0.95647\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9759 - loss: 0.0856 - val_accuracy: 0.9299 - val_loss: 0.1892 - learning_rate: 1.0000e-05\n\nEpoch 45: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 45/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9818 - loss: 0.0786\nEpoch 45: val_accuracy did not improve from 0.95647\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9817 - loss: 0.0786 - val_accuracy: 0.9226 - val_loss: 0.1934 - learning_rate: 1.0000e-05\n\nEpoch 46: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 46/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9735 - loss: 0.0876\nEpoch 46: val_accuracy improved from 0.95647 to 0.96493, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9735 - loss: 0.0876 - val_accuracy: 0.9649 - val_loss: 0.1187 - learning_rate: 1.0000e-05\n\nEpoch 47: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 47/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9767 - loss: 0.0842\nEpoch 47: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9767 - loss: 0.0842 - val_accuracy: 0.9541 - val_loss: 0.1390 - learning_rate: 1.0000e-05\n\nEpoch 48: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 48/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9802 - loss: 0.0877\nEpoch 48: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9802 - loss: 0.0877 - val_accuracy: 0.9565 - val_loss: 0.1313 - learning_rate: 1.0000e-05\n\nEpoch 49: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 49/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9889 - loss: 0.0549\nEpoch 49: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9888 - loss: 0.0551 - val_accuracy: 0.9528 - val_loss: 0.1403 - learning_rate: 1.0000e-05\n\nEpoch 50: LearningRateScheduler setting learning rate to 1e-05.\nEpoch 50/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9797 - loss: 0.0867\nEpoch 50: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9797 - loss: 0.0867 - val_accuracy: 0.9504 - val_loss: 0.1423 - learning_rate: 1.0000e-05\n\nEpoch 51: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 51/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9872 - loss: 0.0598\nEpoch 51: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9872 - loss: 0.0598 - val_accuracy: 0.9589 - val_loss: 0.1202 - learning_rate: 5.0000e-06\n\nEpoch 52: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 52/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9837 - loss: 0.0667\nEpoch 52: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9836 - loss: 0.0667 - val_accuracy: 0.9589 - val_loss: 0.1274 - learning_rate: 5.0000e-06\n\nEpoch 53: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 53/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9737 - loss: 0.0804\nEpoch 53: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9738 - loss: 0.0804 - val_accuracy: 0.9601 - val_loss: 0.1248 - learning_rate: 5.0000e-06\n\nEpoch 54: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 54/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9785 - loss: 0.0809\nEpoch 54: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9784 - loss: 0.0809 - val_accuracy: 0.9565 - val_loss: 0.1289 - learning_rate: 5.0000e-06\n\nEpoch 55: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 55/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9883 - loss: 0.0623\nEpoch 55: val_accuracy did not improve from 0.96493\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9883 - loss: 0.0623 - val_accuracy: 0.9577 - val_loss: 0.1265 - learning_rate: 5.0000e-06\n\nEpoch 56: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 56/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9867 - loss: 0.0590\nEpoch 56: val_accuracy improved from 0.96493 to 0.96614, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9867 - loss: 0.0591 - val_accuracy: 0.9661 - val_loss: 0.1083 - learning_rate: 5.0000e-06\n\nEpoch 57: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 57/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9828 - loss: 0.0729\nEpoch 57: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9828 - loss: 0.0728 - val_accuracy: 0.9577 - val_loss: 0.1325 - learning_rate: 5.0000e-06\n\nEpoch 58: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 58/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9842 - loss: 0.0697\nEpoch 58: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9842 - loss: 0.0697 - val_accuracy: 0.9613 - val_loss: 0.1170 - learning_rate: 5.0000e-06\n\nEpoch 59: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 59/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9790 - loss: 0.0776\nEpoch 59: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9789 - loss: 0.0776 - val_accuracy: 0.9577 - val_loss: 0.1179 - learning_rate: 5.0000e-06\n\nEpoch 60: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 60/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9879 - loss: 0.0576\nEpoch 60: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9880 - loss: 0.0575 - val_accuracy: 0.9613 - val_loss: 0.1076 - learning_rate: 5.0000e-06\n\nEpoch 61: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 61/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9936 - loss: 0.0488\nEpoch 61: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9935 - loss: 0.0489 - val_accuracy: 0.9649 - val_loss: 0.1036 - learning_rate: 5.0000e-06\n\nEpoch 62: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 62/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9781 - loss: 0.0745\nEpoch 62: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9781 - loss: 0.0746 - val_accuracy: 0.9661 - val_loss: 0.1017 - learning_rate: 5.0000e-06\n\nEpoch 63: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 63/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9876 - loss: 0.0537\nEpoch 63: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.9876 - loss: 0.0539 - val_accuracy: 0.9637 - val_loss: 0.1060 - learning_rate: 5.0000e-06\n\nEpoch 64: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 64/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9852 - loss: 0.0554\nEpoch 64: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9852 - loss: 0.0554 - val_accuracy: 0.9577 - val_loss: 0.1157 - learning_rate: 5.0000e-06\n\nEpoch 65: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 65/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9875 - loss: 0.0533\nEpoch 65: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9874 - loss: 0.0534 - val_accuracy: 0.9601 - val_loss: 0.1091 - learning_rate: 5.0000e-06\n\nEpoch 66: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 66/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9909 - loss: 0.0493\nEpoch 66: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9908 - loss: 0.0493 - val_accuracy: 0.9601 - val_loss: 0.1260 - learning_rate: 5.0000e-06\n\nEpoch 67: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 67/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9785 - loss: 0.0647\nEpoch 67: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9785 - loss: 0.0646 - val_accuracy: 0.9528 - val_loss: 0.1290 - learning_rate: 5.0000e-06\n\nEpoch 68: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 68/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9882 - loss: 0.0542\nEpoch 68: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9882 - loss: 0.0543 - val_accuracy: 0.9577 - val_loss: 0.1171 - learning_rate: 5.0000e-06\n\nEpoch 69: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 69/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9914 - loss: 0.0498\nEpoch 69: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9914 - loss: 0.0498 - val_accuracy: 0.9565 - val_loss: 0.1160 - learning_rate: 5.0000e-06\n\nEpoch 70: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 70/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9878 - loss: 0.0559\nEpoch 70: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9878 - loss: 0.0560 - val_accuracy: 0.9637 - val_loss: 0.1035 - learning_rate: 5.0000e-06\n\nEpoch 71: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 71/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9868 - loss: 0.0566\nEpoch 71: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9868 - loss: 0.0566 - val_accuracy: 0.9577 - val_loss: 0.1077 - learning_rate: 5.0000e-06\n\nEpoch 72: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 72/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9849 - loss: 0.0669\nEpoch 72: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9849 - loss: 0.0668 - val_accuracy: 0.9649 - val_loss: 0.1121 - learning_rate: 5.0000e-06\n\nEpoch 73: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 73/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9912 - loss: 0.0485\nEpoch 73: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9912 - loss: 0.0484 - val_accuracy: 0.9637 - val_loss: 0.1133 - learning_rate: 5.0000e-06\n\nEpoch 74: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 74/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9917 - loss: 0.0478\nEpoch 74: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9917 - loss: 0.0479 - val_accuracy: 0.9589 - val_loss: 0.1191 - learning_rate: 5.0000e-06\n\nEpoch 75: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 75/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9823 - loss: 0.0647\nEpoch 75: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9823 - loss: 0.0647 - val_accuracy: 0.9565 - val_loss: 0.1231 - learning_rate: 5.0000e-06\n\nEpoch 76: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 76/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9780 - loss: 0.0828\nEpoch 76: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9781 - loss: 0.0827 - val_accuracy: 0.9613 - val_loss: 0.1033 - learning_rate: 5.0000e-06\n\nEpoch 77: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 77/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9768 - loss: 0.0856\nEpoch 77: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9768 - loss: 0.0855 - val_accuracy: 0.9565 - val_loss: 0.1235 - learning_rate: 5.0000e-06\n\nEpoch 78: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 78/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9832 - loss: 0.0640\nEpoch 78: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9832 - loss: 0.0640 - val_accuracy: 0.9649 - val_loss: 0.1051 - learning_rate: 5.0000e-06\n\nEpoch 79: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 79/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9841 - loss: 0.0579\nEpoch 79: val_accuracy did not improve from 0.96614\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9841 - loss: 0.0579 - val_accuracy: 0.9601 - val_loss: 0.1145 - learning_rate: 5.0000e-06\n\nEpoch 80: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 80/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9908 - loss: 0.0588\nEpoch 80: val_accuracy improved from 0.96614 to 0.96977, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.9908 - loss: 0.0588 - val_accuracy: 0.9698 - val_loss: 0.1006 - learning_rate: 5.0000e-06\n\nEpoch 81: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 81/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9873 - loss: 0.0604\nEpoch 81: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9873 - loss: 0.0604 - val_accuracy: 0.9637 - val_loss: 0.0997 - learning_rate: 5.0000e-06\n\nEpoch 82: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 82/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9888 - loss: 0.0538\nEpoch 82: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9888 - loss: 0.0538 - val_accuracy: 0.9686 - val_loss: 0.1011 - learning_rate: 5.0000e-06\n\nEpoch 83: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 83/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9881 - loss: 0.0559\nEpoch 83: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9881 - loss: 0.0558 - val_accuracy: 0.9649 - val_loss: 0.1102 - learning_rate: 5.0000e-06\n\nEpoch 84: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 84/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9845 - loss: 0.0557\nEpoch 84: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9845 - loss: 0.0557 - val_accuracy: 0.9432 - val_loss: 0.1462 - learning_rate: 5.0000e-06\n\nEpoch 85: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 85/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9935 - loss: 0.0462\nEpoch 85: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9935 - loss: 0.0463 - val_accuracy: 0.9613 - val_loss: 0.1122 - learning_rate: 5.0000e-06\n\nEpoch 86: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 86/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9909 - loss: 0.0437\nEpoch 86: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9909 - loss: 0.0438 - val_accuracy: 0.9577 - val_loss: 0.1166 - learning_rate: 5.0000e-06\n\nEpoch 87: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 87/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9924 - loss: 0.0390\nEpoch 87: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9925 - loss: 0.0390 - val_accuracy: 0.9674 - val_loss: 0.1011 - learning_rate: 5.0000e-06\n\nEpoch 88: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 88/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9932 - loss: 0.0364\nEpoch 88: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9932 - loss: 0.0364 - val_accuracy: 0.9637 - val_loss: 0.1101 - learning_rate: 5.0000e-06\n\nEpoch 89: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 89/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9906 - loss: 0.0448\nEpoch 89: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9906 - loss: 0.0448 - val_accuracy: 0.9661 - val_loss: 0.0993 - learning_rate: 5.0000e-06\n\nEpoch 90: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 90/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9933 - loss: 0.0505\nEpoch 90: val_accuracy did not improve from 0.96977\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9933 - loss: 0.0505 - val_accuracy: 0.9661 - val_loss: 0.1008 - learning_rate: 5.0000e-06\n\nEpoch 91: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 91/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9851 - loss: 0.0528\nEpoch 91: val_accuracy improved from 0.96977 to 0.97219, saving model to best_model_fold_3.keras\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.9851 - loss: 0.0528 - val_accuracy: 0.9722 - val_loss: 0.0920 - learning_rate: 5.0000e-06\n\nEpoch 92: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 92/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9888 - loss: 0.0505\nEpoch 92: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.9888 - loss: 0.0505 - val_accuracy: 0.9661 - val_loss: 0.1011 - learning_rate: 5.0000e-06\n\nEpoch 93: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 93/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9809 - loss: 0.0535\nEpoch 93: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9809 - loss: 0.0535 - val_accuracy: 0.9722 - val_loss: 0.0949 - learning_rate: 5.0000e-06\n\nEpoch 94: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 94/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9910 - loss: 0.0444\nEpoch 94: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9910 - loss: 0.0444 - val_accuracy: 0.9625 - val_loss: 0.1150 - learning_rate: 5.0000e-06\n\nEpoch 95: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 95/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9915 - loss: 0.0429\nEpoch 95: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9915 - loss: 0.0429 - val_accuracy: 0.9661 - val_loss: 0.1096 - learning_rate: 5.0000e-06\n\nEpoch 96: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 96/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9835 - loss: 0.0575\nEpoch 96: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9836 - loss: 0.0574 - val_accuracy: 0.9613 - val_loss: 0.1144 - learning_rate: 5.0000e-06\n\nEpoch 97: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 97/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9858 - loss: 0.0531\nEpoch 97: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9858 - loss: 0.0530 - val_accuracy: 0.9674 - val_loss: 0.0967 - learning_rate: 5.0000e-06\n\nEpoch 98: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 98/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9949 - loss: 0.0465\nEpoch 98: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9949 - loss: 0.0465 - val_accuracy: 0.9710 - val_loss: 0.0984 - learning_rate: 5.0000e-06\n\nEpoch 99: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 99/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9816 - loss: 0.0629\nEpoch 99: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9816 - loss: 0.0629 - val_accuracy: 0.9492 - val_loss: 0.1421 - learning_rate: 5.0000e-06\n\nEpoch 100: LearningRateScheduler setting learning rate to 5e-06.\nEpoch 100/100\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9864 - loss: 0.0516\nEpoch 100: val_accuracy did not improve from 0.97219\n\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 121ms/step - accuracy: 0.9864 - loss: 0.0516 - val_accuracy: 0.9613 - val_loss: 0.1185 - learning_rate: 5.0000e-06\nSaved the complete model for fold 3 at complete_model_fold_3.keras\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 177ms/step\nValidation accuracy for fold 3: 0.9722\nValidation loss for fold 3: 0.0920\n\nAverage validation accuracy across all folds: 0.9835\nAverage validation loss across all folds: 0.0643\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\nimport pandas as pd\n\n# Plot validation accuracy and loss for each fold\ndef plot_metrics(val_accuracies, val_losses, k):\n    folds = range(1, k+1)\n    \n    # Plot Validation Accuracy\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(folds, val_accuracies, marker='o', linestyle='-', color='b', label='Validation Accuracy')\n    plt.xlabel('Fold')\n    plt.ylabel('Accuracy')\n    plt.title('Validation Accuracy for each Fold')\n    plt.grid(True)\n    \n    # Plot Validation Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(folds, val_losses, marker='o', linestyle='-', color='r', label='Validation Loss')\n    plt.xlabel('Fold')\n    plt.ylabel('Loss')\n    plt.title('Validation Loss for each Fold')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot Confusion Matrix for each fold\ndef plot_confusion_matrices(confusion_matrices, k):\n    for i in range(k):\n        plt.figure(figsize=(5, 4))\n        sns.heatmap(confusion_matrices[i], annot=True, fmt=\"d\", cmap='Blues', cbar=False)\n        plt.title(f'Confusion Matrix for Fold {i + 1}')\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.show()\n\n# Print Classification Report for each fold and the average report\ndef print_classification_reports(classification_reports, k):\n    # Ensure that each entry is a dictionary, not a float\n    for i in range(k):\n        print(f\"Classification Report for Fold {i + 1}\")\n        print(pd.DataFrame(classification_reports[i]).transpose())\n        print(\"\\n\")\n    \n    # Calculate average classification report across all folds\n    avg_report = {}\n    # Ensure that the classification report is in dictionary format\n    if isinstance(classification_reports[0], dict):\n        for key in classification_reports[0].keys():\n            avg_report[key] = {metric: np.mean([classification_reports[i][key][metric] for i in range(k)])\n                               for metric in classification_reports[0][key].keys()}\n    else:\n        print(\"Error: classification_reports does not contain dictionaries.\")\n\n    print(\"Average Classification Report across all Folds\")\n    print(pd.DataFrame(avg_report).transpose())\n    print(\"\\n\")\n\n# Call the plotting functions after cross-validation\nplot_metrics(val_accuracies, val_losses, k)\n\nplot_confusion_matrices(confusion_matrices, k)\n\nprint_classification_reports(classification_reports, k)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:32:41.853786Z","iopub.execute_input":"2025-02-10T12:32:41.854072Z","iopub.status.idle":"2025-02-10T12:32:42.641043Z","shell.execute_reply.started":"2025-02-10T12:32:41.854049Z","shell.execute_reply":"2025-02-10T12:32:42.640066Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC9yElEQVR4nOzdeZxO5f/H8dc9Y1bGvowRhiH7EiIKKVtU9r1oskWTZUoZ2SVLyBpRJNnXtPyGMbJFlK2EUkKWGSHGOmY5vz/Od+7cZjDDzJy5zfv5eNyPzpz7Oud+32dunfnc5zrXZTMMw0BEREREREREUp2L1QFEREREREREHlYqukVERERERETSiIpuERERERERkTSioltEREREREQkjajoFhEREREREUkjKrpFRERERERE0oiKbhEREREREZE0oqJbREREREREJI2o6BYRERERERFJIyq6xWkdO3YMm83GZ599Zl83fPhwbDZbsra32WwMHz48VTM9/fTTPP3006m6T0k/kZGRtG7dmjx58mCz2Zg8ebLVkVLdK6+8QrZs2ayOAST/38umTZuw2Wxs2rQpzTOJiLV0bndOP/74I7Vq1SJr1qzYbDb27dtndaRU5+/vz/PPP291DCD5n/OU/NuRtKWiW9LFiy++iLe3N5cvX75jm06dOuHu7s758+fTMVnKHTx4kOHDh3Ps2DGroyTp22+/xWaz4efnR3x8vNVxnEr//v1Zt24dISEhLFiwgMaNG1sdKcN5+umnsdlsST4OHz5sdTwRSUc6t6ethC8cV6xYYXWUu4qJiaFNmzZcuHCBDz/8kAULFlC0aFGrY2U4dzp3+vr6Wh1N0kEWqwNI5tCpUye++uorVq9eTefOnRM9f+3aNb788ksaN25Mnjx57vt1Bg8ezMCBAx8k6j0dPHiQESNG8PTTT+Pv7+/w3Pr169P0tZNj4cKF+Pv7c+zYMTZu3Ej9+vWtjuQ0Nm7cSLNmzXjrrbesjpKhPfLII4wZMybRej8/PwvSiIhVdG4XgD///JPjx48zZ84cunXrZnWcDK1BgwaJ/q14eXlZlEbSk4puSRcvvvgiPj4+LFq0KMkT85dffsnVq1fp1KnTA71OlixZyJLFuo+1u7u7Za8NcPXqVb788kvGjBnDvHnzWLhwYYYtuq9evUrWrFmtjuHg7Nmz5MyZM9X2d+PGDdzd3XFxebg6FeXIkYOXXnrJ6hgiYjGd2wXMcyeQqufPjPg3Qmp49NFHdf7MpB6uvwQlw/Ly8qJly5aEh4fb/+d8q0WLFuHj48OLL77IhQsXeOutt6hQoQLZsmUje/bsPPfcc+zfv/+er5PUvSvR0dH079+ffPny2V/j5MmTibY9fvw4vXv3plSpUnh5eZEnTx7atGnj0NXss88+o02bNgDUq1fP3jUo4V7TpO77Onv2LF27dqVAgQJ4enpSqVIl5s+f79Am4R62CRMmMHv2bAICAvDw8ODxxx/nxx9/vOf7TrB69WquX79OmzZtaN++PatWreLGjRuJ2t24cYPhw4fz6KOP4unpScGCBWnZsiV//vmnvU18fDxTpkyhQoUKeHp6ki9fPho3bsxPP/3kkPnW++4S3H6vUcLv5eDBg3Ts2JFcuXLx1FNPAfDzzz/zyiuvULx4cTw9PfH19eXVV19NsiviqVOn6Nq1K35+fnh4eFCsWDF69erFzZs3OXr0KDabjQ8//DDRdtu3b8dms7F48eIkj9tnn32GzWbDMAxmzJhh/70mOHr0KG3atCF37tx4e3vzxBNP8M033zjsI6Eb4JIlSxg8eDCFChXC29ubqKioJF8z4RhPnjyZcuXK4enpSYECBejZsyf//vuvQ7svv/ySpk2b2t93QEAAo0aNIi4uLtE+d+7cSZMmTciVKxdZs2alYsWKTJkyJclj2bx5c7Jly0a+fPl46623ktzf/YiNjWXUqFH2z7G/vz+DBg0iOjr6ntuePHmS5s2bkzVrVvLnz0///v2TtZ2IpD+d29Pn3H4vyTlHAUybNo1y5crh7e1Nrly5qFatGosWLbI/f/nyZfr164e/vz8eHh7kz5+fBg0asGfPnju+9iuvvELdunUBaNOmDTabzeFYbdy4kdq1a5M1a1Zy5sxJs2bNOHTokMM+7vY3wp1cvHiRfv36UbhwYTw8PChRogTjxo1LdFvdhAkTqFWrFnny5MHLy4uqVavesbv+F198QfXq1e3Hp06dOkn2cti2bRvVq1fH09OT4sWL8/nnn981a0ok53N1J9u2bePxxx/H09OTgIAAPv7441TLJQ9OV7ol3XTq1In58+ezbNkygoKC7OsvXLjAunXr6NChA15eXvz666+sWbOGNm3aUKxYMSIjI/n444+pW7cuBw8eTHEX1m7duvHFF1/QsWNHatWqxcaNG2natGmidj/++CPbt2+nffv2PPLIIxw7doyZM2fy9NNPc/DgQby9valTpw59+vRh6tSpDBo0iDJlygDY/3u769ev8/TTT/PHH38QFBREsWLFWL58Oa+88goXL16kb9++Du0XLVrE5cuX6dmzJzabjfHjx9OyZUuOHj2Km5vbPd/rwoULqVevHr6+vrRv356BAwfy1Vdf2f+YAIiLi+P5558nPDyc9u3b07dvXy5fvkxYWBgHDhwgICAAgK5du/LZZ5/x3HPP0a1bN2JjY9m6dSs//PAD1apVS/bxv1WbNm0oWbIk77//PoZhABAWFsbRo0cJDAzE19eXX3/9ldmzZ/Prr7/yww8/2P/QOn36NNWrV+fixYv06NGD0qVLc+rUKVasWMG1a9coXrw4Tz75JAsXLqR///6JjouPjw/NmjVLMledOnVYsGABL7/8cqKuX5GRkdSqVYtr167Rp08f8uTJw/z583nxxRdZsWIFLVq0cNjXqFGjcHd356233iI6OvquV0h69uzJZ599RmBgIH369OGvv/5i+vTp7N27l++//97+O//ss8/Ili0bwcHBZMuWjY0bNzJ06FCioqL44IMP7PsLCwvj+eefp2DBgvTt2xdfX18OHTrE119/7fBZi4uLo1GjRtSoUYMJEyawYcMGJk6cSEBAAL169brn7zEuLo5z5845rPP09LQP0NatWzfmz59P69atefPNN9m5cydjxozh0KFDrF69+o77vX79Os8++ywnTpygT58++Pn5sWDBAjZu3HjPTCJiDZ3b0/7cfjfJPUfNmTOHPn360Lp1a/r27cuNGzf4+eef2blzJx07dgTgtddeY8WKFQQFBVG2bFnOnz/Ptm3bOHToEFWqVEny9Xv27EmhQoV4//336dOnD48//jgFChQAYMOGDTz33HMUL16c4cOHc/36daZNm8aTTz7Jnj17EnXjT+pvhKRcu3aNunXrcurUKXr27EmRIkXYvn07ISEhnDlzxmEQ1ClTpvDiiy/SqVMnbt68yZIlS2jTpg1ff/21w+dlxIgRDB8+nFq1ajFy5Ejc3d3ZuXMnGzdupGHDhvZ2f/zxB61bt6Zr16506dKFuXPn8sorr1C1alXKlSt3z9/XjRs3Ep0/fXx88PDwSPHn6la//PILDRs2JF++fAwfPpzY2FiGDRtm/11IBmCIpJPY2FijYMGCRs2aNR3Wz5o1ywCMdevWGYZhGDdu3DDi4uIc2vz111+Gh4eHMXLkSId1gDFv3jz7umHDhhm3fqz37dtnAEbv3r0d9texY0cDMIYNG2Zfd+3atUSZd+zYYQDG559/bl+3fPlyAzC+++67RO3r1q1r1K1b1/7z5MmTDcD44osv7Otu3rxp1KxZ08iWLZsRFRXl8F7y5MljXLhwwd72yy+/NADjq6++SvRat4uMjDSyZMlizJkzx76uVq1aRrNmzRzazZ071wCMSZMmJdpHfHy8YRiGsXHjRgMw+vTpc8c2SR3/BLcf24TfS4cOHRK1Teq4L1682ACMLVu22Nd17tzZcHFxMX788cc7Zvr4448NwDh06JD9uZs3bxp58+Y1unTpkmi7pHK//vrrDuv69etnAMbWrVvt6y5fvmwUK1bM8Pf3t39Wv/vuOwMwihcvnuR7ut3WrVsNwFi4cKHD+tDQ0ETrk9pfz549DW9vb+PGjRuGYZj/vooVK2YULVrU+Pfffx3aJhwfwzCMLl26GIDDvyXDMIzHHnvMqFq16j1z161b1wASPRKOb8K/uW7dujls99ZbbxmAsXHjRod9JfXvZdmyZfZ1V69eNUqUKHHHf3MiYi2d201pcW5POK8sX778jm2Se45q1qyZUa5cubu+Xo4cORKdA5PjTjkrV65s5M+f3zh//rx93f79+w0XFxejc+fO9nV3+xshKaNGjTKyZs1q/P777w7rBw4caLi6uhonTpywr7v993/z5k2jfPnyxjPPPGNfd+TIEcPFxcVo0aJFos/orefPokWLJvrb5OzZs4aHh4fx5ptv3jN3UufOWz/ryf1cJezr1s958+bNDU9PT+P48eP2dQcPHjRcXV0d/u2IddS9XNKNq6sr7du3Z8eOHQ7duhYtWkSBAgV49tlnAfDw8LDfAxsXF8f58+fJli0bpUqVumsXp6R8++23APTp08dhfb9+/RK1vXUgi5iYGM6fP0+JEiXImTNnil/31tf39fWlQ4cO9nVubm706dOHK1eusHnzZof27dq1I1euXPafa9euDZhdx+5lyZIluLi40KpVK/u6Dh068H//938O3ZVXrlxJ3rx5eeONNxLtI+Gq8sqVK7HZbAwbNuyObe7Ha6+9lmjdrcc94RvgJ554AsB+3OPj41mzZg0vvPBCklfZEzK1bdsWT09PFi5caH9u3bp1nDt37r7vofr222+pXr26Q1e3bNmy0aNHD44dO8bBgwcd2nfp0iVZg6IsX76cHDly0KBBA86dO2d/VK1alWzZsvHdd9/Z2966v8uXL3Pu3Dlq167NtWvX7COG7927l7/++ot+/foluq8uqd/Z7b+L2rVrJ+tzBua0KWFhYQ6Pt99+G/jv31xwcLDDNm+++SZAkl0eE3z77bcULFiQ1q1b29d5e3vTo0ePZOUSkfSnc7sprc7tycmSnHNUzpw5OXny5F27tefMmZOdO3dy+vTpB8515swZ9u3bxyuvvELu3Lnt6ytWrEiDBg3sv8NbJfU3QlKWL19O7dq1yZUrl8P5s379+sTFxbFlyxZ721t////++y+XLl2idu3aDr/7NWvWEB8fz9ChQxONwXL7+bNs2bL23x9Avnz5KFWqVLJ/l82aNUt0/mzUqBGQ8s9Vgri4ONatW0fz5s0pUqSIfX2ZMmXs+xbrqeiWdJUwmErCPUQnT55k69attG/fHldXV8AssD788ENKliyJh4cHefPmJV++fPz8889cunQpRa93/PhxXFxc7F2mE5QqVSpR2+vXrzN06FD7/UEJr3vx4sUUv+6tr1+yZMlE/xNP6LJ2/Phxh/W3/s8SsJ+kb7/HNykJ9yKdP3+eP/74gz/++IPHHnuMmzdvsnz5cnu7P//8k1KlSt11UJo///wTPz8/hxNlaihWrFiidRcuXKBv374UKFAALy8v8uXLZ2+XcNz/+ecfoqKiKF++/F33nzNnTl544QWHe9QWLlxIoUKFeOaZZ+4r8/Hjx5P8vNzpd5jUe0zKkSNHuHTpEvnz5ydfvnwOjytXrjjcH/nrr7/SokULcuTIQfbs2cmXL5/9S4SEY5RwP/69jhFgv0f/Vrly5UrW5wwga9as1K9f3+FRtmxZ4L9/cyVKlHDYxtfXl5w5cyY6Xrc6fvw4JUqUSPRHTlLHX0QyDp3bTWlxbk9OluSco9555x2yZctG9erVKVmyJK+//jrff/+9wzbjx4/nwIEDFC5cmOrVqzN8+PD7/mIg4XXvlO3cuXNcvXrVYX1Kzp+hoaGJzp0JA8feev78+uuveeKJJ/D09CR37tzky5ePmTNnOvzu//zzT1xcXOznsbu5/XcJKTt/PvLII4nOnwULFgRS/rlK8M8//3D9+nVKliyZ6DmdPzMO3dMt6apq1aqULl2axYsXM2jQIBYvXoxhGA4jm77//vsMGTKEV199lVGjRpE7d25cXFzo169fms47/cYbbzBv3jz69etHzZo1yZEjBzabjfbt26fbfNcJf5zczrjLvU1gnoASvr1O6n+6CxcuTPWrhXe64n23wbiSugLctm1btm/fzoABA6hcuTLZsmUjPj6exo0b39dx79y5M8uXL2f79u1UqFCBtWvX0rt373QbQTy5U3/Ex8eTP39+h6vyt0ooii9evEjdunXJnj07I0eOJCAgAE9PT/bs2cM777xzX8foTp+z1PQgPSJExLno3H5393tuT01lypTht99+4+uvvyY0NJSVK1fy0UcfMXToUEaMGAGY5+PatWuzevVq1q9fzwcffMC4ceNYtWoVzz33XJpnTMn5s0GDBvYeVrd79NFHAdi6dSsvvvgiderU4aOPPqJgwYK4ubkxb948hy/nUyIj/C7FOanolnTXqVMnhgwZws8//8yiRYsoWbIkjz/+uP35FStWUK9ePT799FOH7S5evEjevHlT9FpFixYlPj7efnU3wW+//Zao7YoVK+jSpQsTJ060r7tx4wYXL150aJeSYqJo0aL8/PPPxMfHOxR9CV2CixYtmux93c3ChQtxc3NjwYIFiU4I27ZtY+rUqZw4cYIiRYoQEBDAzp07iYmJueMALgEBAaxbt44LFy7c8Wp3wjf1tx+fu13JvN2///5LeHg4I0aMYOjQofb1R44ccWiXL18+smfPzoEDB+65z8aNG5MvXz4WLlxIjRo1uHbtGi+//HKyM92uaNGiSX5eHvR3GBAQwIYNG3jyySfv+ofGpk2bOH/+PKtWraJOnTr29X/99Vei/QEcOHDA0mniEv7NHTlyxGEQosjISC5evHjX41W0aFEOHDiAYRgO/86SOv4ikrHo3J765/bkZknuOSpr1qy0a9eOdu3acfPmTVq2bMno0aMJCQnB09MTgIIFC9K7d2969+7N2bNnqVKlCqNHj05x0Z3wunfKljdv3vueEiwgIIArV67c81y3cuVKPD09WbduHR4eHvb18+bNS7S/+Ph4Dh48SOXKle8rU2q4389Vvnz58PLySvS3E+j8mZGoe7mku4RvvocOHcq+ffsSzd/p6uqa6BvD5cuXc+rUqRS/VsJJYurUqQ7rbx3Z8m6vO23atERXbhNOErefsJPSpEkTIiIiWLp0qX1dbGws06ZNI1u2bPZpNh7UwoULqV27Nu3ataN169YOjwEDBgDYp8tq1aoV586dY/r06Yn2k/D+W7VqhWEY9m+/k2qTPXt28ubN63DvFMBHH32U7NwJXxDcftxv//24uLjQvHlzvvrqK/uUZUllAnM+1w4dOrBs2TI+++wzKlSoQMWKFZOd6XZNmjRh165d7Nixw77u6tWrzJ49G39//2R1R0tK27ZtiYuLY9SoUYmei42NtX++kjpGN2/eTHScq1SpQrFixZg8eXKiz2Z6fgPfpEkTIPHvcNKkSQBJji5867anT592mM7l2rVrzJ49O/WDikiq0rk99c/tyZHcc9Tt03C6u7tTtmxZDMMgJiaGuLi4RN3t8+fPj5+f331N21iwYEEqV67M/PnzHY7pgQMHWL9+vf1ccT/atm3Ljh07WLduXaLnLl68SGxsLGD+7m02m8Pv+tixY6xZs8Zhm+bNm+Pi4sLIkSMT9X5I7/Pn/XyuXF1dadSoEWvWrOHEiRP29YcOHUryGIk1dKVb0l2xYsWoVasWX375JUCiE/Pzzz/PyJEjCQwMpFatWvzyyy8sXLiQ4sWLp/i1KleuTIcOHfjoo4+4dOkStWrVIjw8nD/++CNR2+eff54FCxaQI0cOypYty44dO9iwYQN58uRJtE9XV1fGjRvHpUuX8PDw4JlnniF//vyJ9tmjRw8+/vhjXnnlFXbv3o2/vz8rVqzg+++/Z/Lkyfj4+KT4Pd1u586d9uklklKoUCGqVKnCwoULeeedd+jcuTOff/45wcHB7Nq1i9q1a3P16lU2bNhA7969adasGfXq1ePll19m6tSpHDlyxN7Ve+vWrdSrV8/+Wt26dWPs2LF069aNatWqsWXLFn7//fdkZ8+ePTt16tRh/PjxxMTEUKhQIdavX5/oKi6YXRPXr19P3bp16dGjB2XKlOHMmTMsX76cbdu2OQwe1rlzZ6ZOncp3333HuHHjUnZAbzNw4EAWL17Mc889R58+fcidOzfz58/nr7/+YuXKlffdbb1u3br07NmTMWPGsG/fPho2bIibmxtHjhxh+fLlTJkyhdatW1OrVi1y5cpFly5d6NOnDzabjQULFiT6Q8DFxYWZM2fywgsvULlyZQIDAylYsCCHDx/m119/TbcTb6VKlejSpQuzZ8+2d43ftWsX8+fPp3nz5tSrV++O23bv3p3p06fTuXNndu/eTcGCBVmwYAHe3t7pkl1E7p/O7al7br/VypUr7Vc7b9WlS5dkn6MaNmyIr68vTz75JAUKFODQoUNMnz6dpk2b4uPjw8WLF3nkkUdo3bo1lSpVIlu2bGzYsIEff/zRoZdASnzwwQc899xz1KxZk65du9qnDMuRIwfDhw+/7+MxYMAA1q5dy/PPP2+fruvq1av88ssvrFixgmPHjpE3b16aNm3KpEmTaNy4MR07duTs2bPMmDGDEiVK8PPPP9v3V6JECd59911GjRpF7dq1admyJR4eHvz444/4+fkxZsyY+86aEg/yuRoxYgShoaHUrl2b3r1724v1cuXKObxXsVD6DpYuYpoxY4YBGNWrV0/03I0bN4w333zTKFiwoOHl5WU8+eSTxo4dOxJN2ZGcaUUMwzCuX79u9OnTx8iTJ4+RNWtW44UXXjD+/vvvRNMt/Pvvv0ZgYKCRN29eI1u2bEajRo2Mw4cPG0WLFk003dScOXOM4sWL26diSJhi5PaMhmFO5ZWwX3d3d6NChQqJptlKeC8ffPBBouNxe87bvfHGGwZg/Pnnn3dsM3z4cAMw9u/fbxiGOYXGu+++axQrVsxwc3MzfH19jdatWzvsIzY21vjggw+M0qVLG+7u7ka+fPmM5557zti9e7e9zbVr14yuXbsaOXLkMHx8fIy2bdsaZ8+eveOUYf/880+ibCdPnjRatGhh5MyZ08iRI4fRpk0b4/Tp00m+7+PHjxudO3c28uXLZ3h4eBjFixc3Xn/9dSM6OjrRfsuVK2e4uLgYJ0+evONxuR1JTBlmGIbx559/Gq1btzZy5sxpeHp6GtWrVze+/vprhzbJmdolKbNnzzaqVq1qeHl5GT4+PkaFChWMt99+2zh9+rS9zffff2888cQThpeXl+Hn52e8/fbbxrp165Kc3mbbtm1GgwYNDB8fHyNr1qxGxYoVjWnTptmf79Kli5E1a9ZEOZL6t5OUunXr3nPamZiYGGPEiBH2z1fhwoWNkJAQ+/Rmt+7r9n8vx48fN1588UXD29vbyJs3r9G3b1/7NGqaMkwkY9O5fZ5Dmwc5txvGf+eVOz0SpglLzjnq448/NurUqWPkyZPH8PDwMAICAowBAwYYly5dMgzDMKKjo40BAwYYlSpVsp8/KlWqZHz00Ud3zXhrzqTOfxs2bDCefPJJw8vLy8iePbvxwgsvGAcPHnRoc7e/Ee7k8uXLRkhIiFGiRAnD3d3dyJs3r1GrVi1jwoQJxs2bN+3tPv30U6NkyZKGh4eHUbp0aWPevHl3PN/NnTvXeOyxxwwPDw8jV65cRt26dY2wsDD780WLFjWaNm2aaLukPh9JudPfGLdKzucqYV+3f342b95sVK1a1XB3dzeKFy9uzJo1K9nndkl7NsPQnf8i8vB57LHHyJ07N+Hh4VZHEREREZFMTPd0i8hD56effmLfvn107tzZ6igiIiIiksnpSreIPDQOHDjA7t27mThxIufOnePo0aP2EVlFRERERKygK90i8tBYsWIFgYGBxMTEsHjxYhXcIiIiImI5y4vuGTNm4O/vj6enJzVq1GDXrl13bBsTE8PIkSMJCAjA09OTSpUqERoa6tDm8uXL9OvXj6JFi+Ll5UWtWrX48ccfHfbxzjvvUKFCBbJmzYqfnx+dO3fm9OnTDvvx9/fHZrM5PMaOHZu6b15EUtXw4cOJj4/n0KFD6Tpli4iIiIjInVhadC9dupTg4GCGDRvGnj17qFSpEo0aNeLs2bNJth88eDAff/wx06ZN4+DBg7z22mu0aNGCvXv32tt069aNsLAwFixYwC+//ELDhg2pX7++fR7Ia9eusWfPHoYMGcKePXtYtWoVv/32Gy+++GKi1xs5ciRnzpyxP9544420ORAiIiIiIiLyULL0nu4aNWrw+OOPM336dADi4+MpXLgwb7zxBgMHDkzU3s/Pj3fffZfXX3/dvq5Vq1Z4eXnxxRdfcP36dXx8fPjyyy9p2rSpvU3VqlV57rnneO+995LM8eOPP1K9enWOHz9OkSJFAPNKd79+/ejXr18qvmMRERERERHJTLJY9cI3b95k9+7dhISE2Ne5uLhQv359duzYkeQ20dHRie7R9PLyYtu2bQDExsYSFxd31zZJuXTpEjabjZw5czqsHzt2LKNGjaJIkSJ07NiR/v37kyXLnQ9ZdHQ00dHR9p/j4+O5cOECefLkwWaz3XE7ERGRlDAMg8uXL+Pn54eLi+V3imVo8fHxnD59Gh8fH52LRUQkVSX7fGzVBOGnTp0yAGP79u0O6wcMGGBUr149yW06dOhglC1b1vj999+NuLg4Y/369YaXl5fh7u5ub1OzZk2jbt26xqlTp4zY2FhjwYIFhouLi/Hoo48muc/r168bVapUMTp27OiwfuLEicZ3331n7N+/35g5c6aRM2dOo3///nd9TwkT0Ouhhx566KFHejz+/vvv5JxyM7W///7b8t+THnrooYceD/fjXudjy7qXnz59mkKFCrF9+3Zq1qxpX//222+zefNmdu7cmWibf/75h+7du/PVV19hs9kICAigfv36zJ07l+vXrwPw559/8uqrr7JlyxZcXV2pUqUKjz76KLt37+bQoUMO+4uJiaFVq1acPHmSTZs2kT179jvmnTt3Lj179uTKlSt4eHgk2eb2K92XLl2iSJEi/PXXX/j4+KTo+Nye87vvvqNevXq4ubnd936s4KzZnTU3OG925U5/zprdWXND6mW/fPkyxYoV4+LFi+TIkSMVEz58Ll26RM6cOfn777/vep5PjpiYGNavX0/Dhg2d6rPnrLnBebM7a25w3uzOmhucN7tyQ1RUFIULF77n+diy7uV58+bF1dWVyMhIh/WRkZH4+vomuU2+fPlYs2YNN27c4Pz58/j5+TFw4ECKFy9ubxMQEMDmzZu5evUqUVFRFCxYkHbt2jm0AfNgt23bluPHj7Nx48Z7nohr1KhBbGwsx44do1SpUkm28fDwSLIgz5079wOd6GNiYvD29iZPnjxO9YEG583urLnBebMrd/pz1uzOmhtSL3vCtuoufW8Jxyh79uypUnR7e3uTPXt2p/rsOWtucN7szpobnDe7s+YG582u3P+51/nYshvB3N3dqVq1KuHh4fZ18fHxhIeHO1z5ToqnpyeFChUiNjaWlStX0qxZs0RtsmbNSsGCBfn3339Zt26dQ5uEgvvIkSNs2LCBPHny3DPvvn37cHFxIX/+/Cl4lyIiIiIiIpKZWXalGyA4OJguXbpQrVo1qlevzuTJk7l69SqBgYEAdO7cmUKFCjFmzBgAdu7cyalTp6hcuTKnTp2yz8n79ttv2/e5bt06DMOgVKlS/PHHHwwYMIDSpUvb9xkTE0Pr1q3Zs2cPX3/9NXFxcURERADmFWl3d3d27NjBzp07qVevHj4+PuzYsYP+/fvz0ksvkStXrnQ+SiIiIiIiIuKsLC2627Vrxz///MPQoUOJiIigcuXKhIaGUqBAAQBOnDjhMArcjRs3GDx4MEePHiVbtmw0adKEBQsWOIw6funSJUJCQjh58iS5c+emVatWjB492t514NSpU6xduxaAypUrO+T57rvvePrpp/Hw8GDJkiUMHz6c6OhoihUrRv/+/QkODk7bAyIiIiIiIiIPFUuLboCgoCCCgoKSfG7Tpk0OP9etW5eDBw/edX9t27albdu2d3ze39+fe40dV6VKFX744Ye7thERERERERG5F03uKSIiIiIiIpJGVHSLiIiIiIiIpBEV3SIiIiIiIiJpREW3iIiIiIiISBpR0S0iIiIiIiKSRlR0i4iIiIiIiKQRFd0iIiIiIiIiaURFdwYXFwebN9vYsqUQmzfbiIuzOpGIiIiIiIgTiovDtnkzhbZswbZ5M+lVXKnozsBWrQJ/f2jQIAuTJlWjQYMs+Pub60VERERERCSZ/ldcZWnQgGqTJpGlQQPSq7hS0Z1BrVoFrVvDyZOO60+dMter8BYREREREUkGi4srFd0ZUFwc9O0LhpH4uYR1/fqlW28IERERERER55QBiisV3RnQ1q2Jv4S5lWHA33+b7UREREREROQOMkBxpaI7AzpzJnntTp1K2xwiIiIiIiJOLbnFVXLb3YcsabZnuW8FCyav3bvvgosLtG0Lrq5pm0lERERERMTpJLe4Sm67+6Ar3RlQ7drwyCNgs925jc0Gx49Dx45QvjwsXAixsemXUUREREREJMOrXRv8/O78vM0GhQub7dKIiu4MyNUVpkwxl28vvG028zF/PowcCblyweHD8NJLULYsfP65im8RERERERHA7BpcuHDSzyUUW5Mnp2nXYRXdGVTLlrBiBRQq5Lj+kUfM9S+/DEOGwLFjMHo05M4NR45Aly5QujTMmwcxMZZEFxERERERyRhmzoSdO82iOn9+x+cSiquWLdM0goruDKxlS7OoDguLJTj4J8LCYvnrL8fPRPbsMGiQ2W7sWMibF/78E159FUqVgk8/VfEtIiIiIiKZ0L590L+/ufzBB3D6NLFhYfwUHExsWBiJiqs0oqI7g3N1hbp1DerUOUXdusYdez34+MA775ifm/HjIV8+c7lbNyhZEmbPhps30ze7iIiIiIiIJa5cgXbtzCLo+efNubhdXTHq1uVUnToYdeum22jUKrofMtmywYABZsE9cSIUKGAOuNazJ5QoYfauiI62OqWIiIiIiEgaMQzo1Qt+/93sQv7ZZ3cfpTqNqeh+SGXNCsHBcPSoOS5AwYLmnO+9e5vF94wZcOOG1SlFRERERERS2fz58MUX5pXsxYshTx5L46jofsh5e0PfvuZ93tOmmQOznTwJQUEQEABTp8L161anFBERERERSQWHDsHrr5vLI0bAU09ZmwcV3ZmGl5dZaP/xh3mV+5FH4PRpsyAvXhw+/BCuXbM6pYiIOLsZM2bg7++Pp6cnNWrUYNeuXXdtv3z5ckqXLo2npycVKlTg22+/dXg+MjKSV155BT8/P7y9vWncuDFHjhxJy7cgIiLO6vp1aNvWLGzq14eBA61OBKjoznQ8Pc0u5n/8AbNmQZEiEBFhdkUvXty8D/zqVatTioiIM1q6dCnBwcEMGzaMPXv2UKlSJRo1asTZs2eTbL99+3Y6dOhA165d2bt3L82bN6d58+YcOHAAAMMwaN68OUePHuXLL79k7969FC1alPr163NVJysREbld//5w4IA5NdiCBek2UNq9qOjOpDw8zMHVjhwxRzb394fISHjrLShWzBwB/coVq1OKiIgzmTRpEt27dycwMJCyZcsya9YsvL29mTt3bpLtp0yZQuPGjRkwYABlypRh1KhRVKlShenTpwNw5MgRfvjhB2bOnMnjjz9OqVKlmDlzJtevX2fx4sXp+dZERCSjW7YMPv7YHDDtiy/A19fqRHZZrA4g1nJ3h+7d4ZVXzC+DRo82B1975x2z8H7zTbNbuo+P1UlFRCQju3nzJrt37yYkJMS+zsXFhfr167Njx44kt9mxYwfBwcEO6xo1asSaNWsAiP7fdBuenp4O+/Tw8GDbtm1069Yt0T6jo6Pt2wFERUUBEBMTQ0xMzP29uf9J2P5B95PenDU3OG92Z80NzpvdWXOD82bPULmPHiVL9+7YgLi33yb+6afhDrlSM3dy96GiWwBwc4NXX4WXX4ZFi+C998wu6IMGwYQJZvfzoCDIkcPqpCIikhGdO3eOuLg4ChQo4LC+QIECHD58OMltIiIikmwfEREBQOnSpSlSpAghISF8/PHHZM2alQ8//JCTJ09y5syZJPc5ZswYRowYkWj9+vXr8fb2vp+3lkhYWFiq7Ce9OWtucN7szpobnDe7s+YG581udW5bTAy1Q0LIFRXF+TJl+L56dYzbxgdJSmrkvpbMQbFUdIsDNzfo0gU6dYIlS2DUKHN6u8GDzeK7f3/o0wdy5rQ6qYiIPOzc3NxYtWoVXbt2JXfu3Li6ulK/fn2ee+45DMNIcpuQkBCHq+dRUVEULlyYhg0bkj179gfKExMTQ1hYGA0aNMDNze2B9pWenDU3OG92Z80NzpvdWXOD82bPKLldBgzA9Y8/MHLnJvvXX/Nc4cJ3bZ+auRN6U92Lim5JUpYs8NJL0KEDLF1qFt+HD8OwYTBpkjnqeb9+kCuX1UlFRCQjyJs3L66urkRGRjqsj4yMxPcO99X5+vres33VqlXZt28fly5d4ubNm+TLl48aNWpQrVq1JPfp4eGBh4dHovVubm6p9kdhau4rPTlrbnDe7M6aG5w3u7PmBufNbmnur76CKVMAsM2bh1vx4sneNDVyJ3d7DaQmd+XqCh07moMALlkC5crBpUswciQULWpeAT9/3uqUIiJiNXd3d6pWrUp4eLh9XXx8POHh4dSsWTPJbWrWrOnQHszufkm1z5EjB/ny5ePIkSP89NNPNGvWLHXfgIiIOJe//zYHpgLziuCLL1oa525UdEuyuLpCu3bw88/mwIDly8Ply+bAa/7+5r3f585ZnVJERKwUHBzMnDlzmD9/PocOHaJXr15cvXqVwMBAADp37uww0Frfvn0JDQ1l4sSJHD58mOHDh/PTTz8RFBRkb7N8+XI2bdpknzasQYMGNG/enIYNG6b7+xMRkQwiNta8MnjhAlStCuPGWZ3orlR0S4q4uECbNrB/P6xcCZUqmVOLjRljFt/vvAP//GN1ShERsUK7du2YMGECQ4cOpXLlyuzbt4/Q0FD7YGknTpxwGACtVq1aLFq0iNmzZ1OpUiVWrFjBmjVrKF++vL3NmTNnePnllyldujR9+vTh5Zdf1nRhIiKZ3YgRsG2bOcXS0qXmfMgZmO7plvvi4gItW0KLFrB2rfm537vXnGZs+nTo1QsGDIDcua1OKiIi6SkoKMjhSvWtNm3alGhdmzZtaNOmzR3316dPH/r06ZNa8URExNlt2GB2twWYPRsCAqzNkwy60i0PxGaDZs1g926z+K5WDa5dg4kToVgxGDDAhQsXMvY3TyIiIiIi4gQiI83Rng0DuneH9u2tTpQsKrolVdhs8MILsGsXfPMNVK8O16/DlCmuvPZaA4KDXTh92uqUIiIiIiLilOLj4eWXzcK7fHmYPNnqRMmmoltSlc0GTZrADz9AaCg88UQ8N2+6Mn26K8WLQ1AQnDxpdUoREREREXEq48ZBWBh4eZn3cXt7W50o2SwvumfMmIG/vz+enp7UqFGDXbt23bFtTEwMI0eOJCAgAE9PTypVqkRoaKhDm8uXL9OvXz+KFi2Kl5cXtWrV4scff3RoYxgGQ4cOpWDBgnh5eVG/fn2OHDni0ObChQt06tSJ7NmzkzNnTrp27cqVK1dS740/5Gw2aNQINm+OY8SI7Tz5ZDzR0TBjhnnbRe/ecOKE1SlFRERERCTD+/57GDLEXJ4+HcqWtTZPClladC9dupTg4GCGDRvGnj17qFSpEo0aNeLs2bNJth88eDAff/wx06ZN4+DBg7z22mu0aNGCvXv32tt069aNsLAwFixYwC+//ELDhg2pX78+p06dsrcZP348U6dOZdasWezcuZOsWbPSqFEjbty4YW/TqVMnfv31V8LCwvj666/ZsmULPXr0SLuD8ZCy2aBSpX/YuDGO8HCoUwdu3oSZM6FECejZE44dszqliIiIiIhkSBcuQIcOEBcHnTrB/6ahdCaWFt2TJk2ie/fuBAYGUrZsWWbNmoW3tzdz585Nsv2CBQsYNGgQTZo0oXjx4vTq1YsmTZowceJEAK5fv87KlSsZP348derUoUSJEgwfPpwSJUowc+ZMwLzKPXnyZAYPHkyzZs2oWLEin3/+OadPn2bNmjUAHDp0iNDQUD755BNq1KjBU089xbRp01iyZAmndWPyfbHZ4JlnYPNm2LQJ6tWDmBhzwMGSJc1xEI4etTqliIiIiIhkGIZhFtl//20WDTNnmoWFk7Gs6L558ya7d++mfv36/4VxcaF+/frs2LEjyW2io6Px9PR0WOfl5cW2bdsAiI2NJS4u7q5t/vrrLyIiIhxeN0eOHNSoUcP+ujt27CBnzpxUq1bN3qZ+/fq4uLiwc+fOB3jXAlC3LmzcCFu2QP365tz2n3wCjz4Kr74Kf/xhdUIREREREbHc1KnmFEnu7uZ93D4+Vie6L5bN033u3Dni4uIoUKCAw/oCBQpw+PDhJLdp1KgRkyZNok6dOgQEBBAeHs6qVauIi4sDwMfHh5o1azJq1CjKlClDgQIFWLx4MTt27KBEiRIARERE2F/n9tdNeC4iIoL8+fM7PJ8lSxZy585tb5OU6OhooqOj7T9HRUUB5r3oMTEx9zwmd5Kw7YPswyp3y/7EE/Dtt7Bjh4333nMhLMyFefPg888NOnQwGDgwjkcfTe/Epof1mGdkyp3+nDW7s+aG1MvujO9dREQkRX76CQYMMJcnToTHHrM2zwOwrOi+H1OmTKF79+6ULl0am81GQEAAgYGBDt3RFyxYwKuvvkqhQoVwdXWlSpUqdOjQgd27d6d5vjFjxjBixIhE69evX493KoyuFxYW9sD7sMq9sr/+OtSvn4ulS0uxZ08BvvjCxqJFNmrXPkmbNr/zyCPWDGL3MB/zjEq505+zZnfW3PDg2a9du5ZKSURERDKgqChzDu6YGGjRwiwWnJhlRXfevHlxdXUlMjLSYX1kZCS+vr5JbpMvXz7WrFnDjRs3OH/+PH5+fgwcOJDixYvb2wQEBLB582auXr1KVFQUBQsWpF27dvY2CfuOjIykYMGCDq9buXJle5vbB3OLjY3lwoULd8wGEBISQnBwsP3nqKgoChcuTMOGDcmePXsyjkrSYmJiCAsLo0GDBri5ud33fqyQkuxNmkD//vDTT7G8954L337rwubNhdmy5RHatjUICYlLt4EKM8sxz0iUO/05a3ZnzQ2plz2hJ5WIiMhDxzDM0Zb//BOKFoVPP3XK+7hvZVnR7e7uTtWqVQkPD6d58+YAxMfHEx4eTlBQ0F239fT0pFChQsTExLBy5Uratm2bqE3WrFnJmjUr//77L+vWrWP8+PEAFCtWDF9fX8LDw+1FdlRUFDt37qRXr14A1KxZk4sXL7J7926qVq0KwMaNG4mPj6dGjRp3zOXh4YGHh0ei9W5ubqnyh2Fq7ccKKclesyZ88w3s3g2jRsGXX9pYutTGsmUutGljzhZQvnwaB/6fzHLMMxLlTn/Omt1Zc8ODZ3fW9y0iInJPn34KS5aAqyssXgy5clmd6IFZOnp5cHAwc+bMYf78+Rw6dIhevXpx9epVAv83DHznzp0JCQmxt9+5cyerVq3i6NGjbN26lcaNGxMfH8/bb79tb7Nu3TpCQ0P566+/CAsLo169epQuXdq+T5vNRr9+/XjvvfdYu3Ytv/zyC507d8bPz89e/JcpU4bGjRvTvXt3du3axffff09QUBDt27fHz88v/Q5QJle1KqxZA3v3mr1KDAOWLYMKFaB1a/j5Z6sTioiIiIhIqjlwAN54w1x+/33zatxDwNJ7utu1a8c///zD0KFDiYiIoHLlyoSGhtoHOTtx4gQuLv99L3Djxg0GDx7M0aNHyZYtG02aNGHBggXkzJnT3ubSpUuEhIRw8uRJcufOTatWrRg9erTDVYG3336bq1ev0qNHDy5evMhTTz1FaGiow6jnCxcuJCgoiGeffRYXFxdatWrF1KlT0/6gSCKVK8OqVWaRPWoUrFgBK1eajxYtzCvfTjyugoiIiIiIXL0K7drBjRvQqBG89ZbViVKN5QOpBQUF3bE7+aZNmxx+rlu3LgcPHrzr/tq2bZtkd/Nb2Ww2Ro4cyciRI+/YJnfu3CxatOiu+5H0VbEiLF9ufgH23nvmVe/Vq83Hiy/C0KHm1XEREREREXEyffrAwYNQsCB8/jm4WNopO1U9PO9EMo3y5c3bPA4cgA4dzHEV1q6FatXg+efhxx+tTigiIiIiIsm2cCHMnWv+Yb9wIdw2fbOzU9EtTqtsWVi0yPxC7KWXzC/DvvkGqlc3R0L/4QerE4qIiIiIyF0dOQKvvWYuDxkC9epZmycNqOgWp1e6NCxYAIcOQZcu5kCH//d/5rgLjRrB9u1WJxQRERERkUSio837uK9cgbp1zftFH0IquuWh8eij8NlncPgwBAaaxff69fDkk1C/PmzdanVCERERERGxGzDAnKoob16zW7mrq9WJ0oSKbnnolChh3hLy++/QrRtkyQLh4VCnjtlb5bbx+UREREREJL2tXg3TppnL8+dDoULW5klDKrrloVW8OMyZY94m0rMnuLmZBXe9embvlY0bzbm/RUREREQkHR0/Dq++ai6/9ZY5INNDTEW3PPT8/WHWLPjjD+jVC9zdYcsWePZZqF0bwsJUfIuIiIiIpIuYGGjfHi5eNEdAHj3a6kRpTkW3ZBpFisBHH8Gff0JQEHh4wPffQ8OG5n3foaEqvkVERERE0tSQIeY0QzlymPMAu7tbnSjNqeiWTOeRR8zbR44ehb59wdMTduyA554zRzz/9lsV3yIiIiIiqW7dOhg3zlz+9FMoVszaPOlERbdkWn5+MHmyWXwHB4OXF+zcCU2bQq1aruzaVUDFt4iIiIhIajh9Gl5+2Vzu3RtatbI2TzpS0S2ZXsGCMHEi/PWXOY6Dtzfs3u3C++8/QY0aWVizRle+RURERETuW1wcvPQS/PMPVKpk/vGdiajoFvmfAgXggw8Siu84PD1j2bfPRosW8NhjsHIlxMdbnVJERERExMmMHg3ffQdZs8LSpeb9nZmIim6R2+TPD++/H8/s2WG8/XYc2bLB/v3QujVUrgzLl6v4FhERERFJls2bYcQIc3nmTChVyto8FlDRLXIH2bPf5L334jl+HAYPhuzZ4ZdfoG1bqFDBHGwxLs7qlCIiIiIiGdQ//0DHjuYVqy5d/runO5NR0S1yD7lzw6hRcOwYDBtmzm5w8CB06ADly8PChSq+RUREREQcxMfDK6+YA6iVLg3Tp1udyDIqukWSKVcuGD7cLL5HjICcOeHwYXNMiLJlYcECiI21OKSIiIiISEbw4YfmXLweHuZ93NmyWZ3IMiq6RVIoZ04YOhSOH4f33jOvhP/+O3TuDGXKwGefqfgWERERkUxs504YONBcnjIFKla0No/FVHSL3Kfs2eHdd80r32PGQJ488McfEBhojg8xdy7ExFidUkREREQkHV28CO3bm1eh2rSBHj2sTmQ5Fd0iD8jHx/wi79gxGD8e8uWDo0eha1d49FGYMwdu3rQ6pYiIiIhIGjMM6NbN/MO4WDHzD2GbzepUllPRLZJKsmWDAQPMeb4nTDCnHjt2zPxyr2RJmDULoqOtTikiIiIikkZmzYKVK8HNzbyPO0cOqxNlCCq6RVJZ1qzw5ptm8f3hh+DrCydOQK9eUKIEzJgBN25YnVJEREREJBXt3w/9+5vLY8fC449bmycDUdEtkka8vaFfP7Or+dSp4OcHJ09CUBAEBMC0aXD9utUpRUREREQe0JUr0Lat2a3z+ef/K74FUNEtkua8vOCNN+DPP83pCR95xJyusE8fs/iePBmuXbM6pYiIiIjIfXr9dXM6n0KFYN483cd9GxXdIunE09P8/9Eff8DMmVCkCJw5Y34RWLw4TJwIV69anVJEREREJPlsn38On38OLi6weDHkzWt1pAxHRbdIOvPwgNdegyNHYPZs8PeHyEh46y1zkMcPPjB76IiIiIiIZGTZTp7EtU8f84cRI6B2bWsDZVAqukUs4u4O3bubPXE++cQsuP/5B95+21weOxYuX7Y6pYiIiIhIEq5fp9oHH2C7dg2eeQZCQqxOlGGp6BaxmJubOaf3b7+Zt8AEBMC5c+b/t/z9YfRoiIqyOqWIiIiIyH9cBgwgx/HjGPnzwxdfgKur1ZEyLBXdIhmEmxu88gocPmzeFvPoo3DhAgwebBbfo0bBxYsWhxQRERERWb4c19mzAYibNw8KFrQ4UMamolskg8mSBV5+GQ4eNL80LF0a/v0Xhg41i+/hw82fRURERETS3dGj0K0bAL+3aoXRoIHFgTI+Fd0iGZSrK3TqBAcOmANBli0Lly6ZY1T4+8OQIeaVcBERERGRdHHzJrRvD1FRxNesyeGOHa1O5BRUdItkcK6u5v/bfvkFli2D8uXNe7zfe88svt9917wHXEREREQkTYWEwI8/Qq5cxC1YgKH7uJNFRbeIk3BxgTZtYP9+WLECKlY0Rzd//32z+B440Bz9XEREREQk1X3zDUyaZC7PmwdFilibx4mo6BZxMi4u0KoV7N0Lq1fDY4/B1aswblxC8e3CxYseVscUERERkYfFyZPQpYu53KcPNGtmbR4no6JbxEm5uEDz5rB7N6xdC1WrwrVrMGmSKz161GfAABciIqxOKSIiIiJOLTYWOnaE8+ehShUYP97qRE5HRbeIk7PZ4IUXzNtrvv4aqlWL5+bNLEyZ4kqxYtCvH5w+bXVKEREREXFKI0fC1q3g4wNLl4KHelSmlIpukYeEzQZNm8L338cxdOgOatSI58YNmDIFiheHN94wewaJiIiIiCRLeLg5ei/Axx9DiRLW5nFSKrpFHjI2G1SpcpYtW+JYvx6efBKio2H6dAgIgNdfh7//tjqliIiIiGRokZHw0ktgGOa83B06WJ3IaVledM+YMQN/f388PT2pUaMGu3btumPbmJgYRo4cSUBAAJ6enlSqVInQ0FCHNnFxcQwZMoRixYrh5eVFQEAAo0aNwjAMexubzZbk44MPPrC38ff3T/T82LFjU/8AiKQRmw0aNDB7A23YALVrm1MrfvSRWXy/9hocP251ShERERHJcOLjoXNniIiAcuXMrpNy3ywtupcuXUpwcDDDhg1jz549VKpUiUaNGnH27Nkk2w8ePJiPP/6YadOmcfDgQV577TVatGjB3r177W3GjRvHzJkzmT59OocOHWLcuHGMHz+eadOm2ducOXPG4TF37lxsNhutWrVyeL2RI0c6tHvjjTfS5kCIpCGbDZ59FrZsge++g6efhpiY/3oIde8Of/1ldUoRERERyTDGj4f168HLy7yP29vb6kROzdKie9KkSXTv3p3AwEDKli3LrFmz8Pb2Zu7cuUm2X7BgAYMGDaJJkyYUL16cXr160aRJEyZOnGhvs337dpo1a0bTpk3x9/endevWNGzY0OEKuq+vr8Pjyy+/pF69ehQvXtzh9Xx8fBzaZc2aNW0OhEg6efpps/DevNksxGNj4ZNP4NFHoWtX+PNPqxOKiIiIiKW2b4fBg83ladPMK93yQLJY9cI3b95k9+7dhISE2Ne5uLhQv359duzYkeQ20dHReHp6Oqzz8vJi27Zt9p9r1arF7Nmz+f3333n00UfZv38/27ZtY1LCRO63iYyM5JtvvmH+/PmJnhs7diyjRo2iSJEidOzYkf79+5Mly50PWXR0NNHR0fafo6KiALNbfExMzB23u5eEbR9kH1Zx1uzOmhuSl71mTfi//4Pt222MHu1CWJgLc+fC/PkGHTsaDBwYR8mS6ZXY5KzH3Flzg/Nmd9bckHrZnfG9i4iIE7hwAdq3h7g4c5qwV1+1OtFDwbKi+9y5c8TFxVGgQAGH9QUKFODw4cNJbtOoUSMmTZpEnTp1CAgIIDw8nFWrVhEXF2dvM3DgQKKioihdujSurq7ExcUxevRoOnXqlOQ+58+fj4+PDy1btnRY36dPH6pUqULu3LnZvn07ISEhnDlz5o7FO8CYMWMYMWJEovXr16/HOxW6ZISFhT3wPqzirNmdNTckP/vrr8Ozz+Zi2bJS7NlTgAULbCxcaKN27ZO0bfs7hQpdSeOkjpz1mDtrbnDe7M6aGx48+7Vr11IpiYiIyP8Yhllk//23eQ/irFnmfYrywCwruu/HlClT6N69O6VLl8ZmsxEQEEBgYKBDd/Rly5axcOFCFi1aRLly5di3bx/9+vXDz8+PLl26JNrn3Llz6dSpU6Ir6MHBwfblihUr4u7uTs+ePRkzZgwed5ibLiQkxGG7qKgoChcuTMOGDcmePft9v++YmBjCwsJo0KABbm5u970fKzhrdmfNDfeXvUkTCA6GH3+MZfRoF7791oXNmwuzdesjtG1rEBISR5kyGS93RuCsucF5sztrbki97Ak9qURERFLNtGnw5Zfg7m7ex+3jY3Wih4ZlRXfevHlxdXUlMjLSYX1kZCS+vr5JbpMvXz7WrFnDjRs3OH/+PH5+fgwcONDhXuwBAwYwcOBA2rdvD0CFChU4fvw4Y8aMSVR0b926ld9++42lS5feM2+NGjWIjY3l2LFjlCpVKsk2Hh4eSRbkbm5uqfKHYWrtxwrOmt1Zc8P9Za9VC775Bn76CUaNgrVrbSxZYmPpUhfatoUhQ9L+th5nPebOmhucN7uz5oYHz+6s71tERDKoPXtgwABzecIEqFLF2jwPGcsGUnN3d6dq1aqEh4fb18XHxxMeHk7NmjXvuq2npyeFChUiNjaWlStX0qxZM/tz165dw8XF8W25uroSHx+faD+ffvopVatWpVKlSvfMu2/fPlxcXMifP/8924o4u2rVzC869+yB5s3N3kZLl0L58tCmDfz8s9UJRURERCRVREVBu3bm3LLNm0NQkNWJHjqWjl4eHBzMnDlzmD9/PocOHaJXr15cvXqVwMBAADp37uww0NrOnTtZtWoVR48eZevWrTRu3Jj4+Hjefvtte5sXXniB0aNH880333Ds2DFWr17NpEmTaNGihcNrR0VFsXz5crp165Yo144dO5g8eTL79+/n6NGjLFy4kP79+/PSSy+RK1euNDoaIhnPY4/B6tWwbx8kzKi3YgVUqgQtW5rrRURERMRJGQa89hr88QcUKQKffqr7uNOApfd0t2vXjn/++YehQ4cSERFB5cqVCQ0NtQ+uduLECYer1jdu3GDw4MEcPXqUbNmy0aRJExYsWEDOnDntbaZNm8aQIUPo3bs3Z8+exc/Pj549ezJ06FCH116yZAmGYdChQ4dEuTw8PFiyZAnDhw8nOjqaYsWK0b9/f4f7tUUyk0qVzGL7wAGz2/ny5WYxvno1NGsGQ4eqF5KIiIiI05k7FxYvBldX87+5c1ud6KFk+UBqQUFBBN2hC8OmTZscfq5bty4HDx686/58fHyYPHkykydPvmu7Hj160KNHjySfq1KlCj/88MNdtxfJjMqXN7uZDx0K771nLn/5pfl4/nlz/eOPW51SRERERO7p11/hjTfM5ffeMwf3kTRhafdyEXFO5cqZX4YePAidOoGLC3z9NVSvbo6EvnOn1QlFRERE5I6uXYO2beH6dWjYEG65XVdSn4puEblvpUvDF1/AoUPQubNZfP/f/8ETT0DjxrB9u9UJRURERCSRPn3Mqye+vrBggflHnKQZHV0ReWCPPgrz58Nvv0FgoHlb0Lp18OST0KABbNtmdUIRSS8zZszA398fT09PatSowa5du+7afvny5ZQuXRpPT08qVKjAt99+6/D8lStXCAoK4pFHHsHLy4uyZcsya9astHwLIiIPt8WL/xswbeFC0OxMaU5Ft4ikmhIlzPE4fv8dunaFLFlgwwaoXRueeQY2b7Y6oYikpaVLlxIcHMywYcPYs2cPlSpVolGjRpw9ezbJ9tu3b6dDhw507dqVvXv30rx5c5o3b86BAwfsbYKDgwkNDeWLL77g0KFD9OvXj6CgINauXZteb0tE5OFx5AgkjGs1eLD5B5qkORXdIpLqiheHTz757//rbm7w3Xfw9NNQty5s3GjOUJEgLg42b7axZUshNm+2ERdnWXQReQCTJk2ie/fuBAYG2q9Ie3t7M3fu3CTbT5kyhcaNGzNgwADKlCnDqFGjqFKlCtOnT7e32b59O126dOHpp5/G39+fHj16UKlSpXteQRcRkdtER0P79nDlCtSpY46AK+nC8tHLReTh5e8PH38M774LY8eaPZm2bIFnn4WnnoJhw+DSJejXD06ezAJUY9IkeOQRmDLFnAtcRJzDzZs32b17NyEhIfZ1Li4u1K9fnx07diS5zY4dOxJNx9moUSPWrFlj/7lWrVqsXbuWV199FT8/PzZt2sTvv//Ohx9+mOQ+o6OjiY6Otv8cFRUFQExMDDExMff79uz7uPW/zsJZc4PzZnfW3OC82Z01N6Rfdpe33sJ1zx6MPHmInT/fvALyAK/prMc8NXMndx8qukUkzRUpAh99BCEhMG4czJlj3ufdoEHS7U+dgtatzbnBVXiLOIdz584RFxdHgQIFHNYXKFCAw4cPJ7lNREREku0jIiLsP0+bNo0ePXrwyCOPkCVLFlxcXJgzZw516tRJcp9jxoxhxIgRidavX78eb2/vlL6tJIWFhaXKftKbs+YG583urLnBebM7a25I2+y+P/xAjf/1ItrZqxeR+/fD/v2psm9nPeapkfvatWvJaqeiW0TSTeHCMH26WXyPHWsuJ8UwzLE9+vWDZs3MgdlEJHOaNm0aP/zwA2vXrqVo0aJs2bKF119/HT8/P+rXr5+ofUhIiMPV86ioKAoXLkzDhg3Jnj37A2WJiYkhLCyMBg0a4Obm9kD7Sk/OmhucN7uz5gbnze6suSEdsp84QZbAQADi+venaip1K3fWY56auRN6U92Lim4RSXeFCkGrVncuusEsvP/+G7ZuNe8FF5GMLW/evLi6uhIZGemwPjIyEl9f3yS38fX1vWv769evM2jQIFavXk3Tpk0BqFixIvv27WPChAlJFt0eHh54eHgkWu/m5pZqfxSm5r7Sk7PmBufN7qy5wXmzO2tuSKPsMTHmvK7//gvVq+M6diyuqfwaznrMUyN3crfXQGoiYokzZ1K3nYhYy93dnapVqxIeHm5fFx8fT3h4ODVr1kxym5o1azq0B7O7X0L7hPuwXW6bP9bV1ZX4+PhUfgciIg+hoUNh+3bIkQOWLAF3d6sTZUq60i0ilihYMHXbiYj1goOD6dKlC9WqVaN69epMnjyZq1evEvi/bo2dO3emUKFCjBkzBoC+fftSt25dJk6cSNOmTVmyZAk//fQTs2fPBiB79uzUrVuXAQMG4OXlRdGiRdm8eTOff/45kyZNsux9iog4hXXrzPv5wJxWplgxa/NkYiq6RcQStWubo5SfOuU4fditChc224mIc2jXrh3//PMPQ4cOJSIigsqVKxMaGmofLO3EiRMOV61r1arFokWLGDx4MIMGDaJkyZKsWbOG8uXL29ssWbKEkJAQOnXqxIULFyhatCijR4/mtddeS/f3JyLiNM6cgZdfNpdfe80coVYso6JbRCzh6mpOC9a6tTloWlKF9wcfaBA1EWcTFBREUFBQks9t2rQp0bo2bdrQpk2bO+7P19eXefPmpVY8EZGHX1wcvPQS/PMPVKwI6hlkOd3TLSKWadnSnBasUCHH9QkXwvbtS/dIIiIiIs7t/fdh40bw9oalS8HLy+pEmZ6KbhGxVMuWcOwYhIXFEhz8E2FhsSxdaj43fjzs3GlpPBERERHnsWULDB9uLs+cCaVLWxpHTCq6RcRyrq5Qt65BnTqnqFvXoHVr6NQJ4uOhSxe4ft3qhCIiIiIZ3Llz0LGj+QdU587mQzIEFd0ikiFNnQq+vvDbbzBkiNVpRERERDIww4BXXjFHqC1VCmbMsDqR3EJFt4hkSLlzw5w55vKkSfD999bmEREREcmwPvwQvvkGPDxg2TLIls3qRHILFd0ikmE9/7z5pW3Cl7dXr1qdSERERCSD2bUL3nnHXP7wQ3PEcslQVHSLSIb24Yfm6OZ//AGDBlmdRkRERCQDuXQJ2reH2FhzHtbXXrM6kSRBRbeIZGg5c8Knn5rLU6dCEtP8ioiIiGQ+hgHdusFff4G/v3lfns1mdSpJgopuEcnwGjWC7t3N5cBAuHLF2jwiIiIilvv4Y1ixArJkMefjzpnT6kRyByq6RcQpTJwIRYuac3oPGGB1GhEREREL/fwz9OtnLo8dC9WrWxpH7k5Ft4g4BR8fmDvXXJ41C8LCrM0jIiIiYokrV6BtW4iOhiZNoH9/qxPJPajoFhGn8cwz8Prr5nLXrubYISIiIiKZSlAQ/PYb+PnB/PngopIuo9NvSEScytixULw4/P03vPmm1WlERERE0tHnn/9XaC9eDHnzWp1IkkFFt4g4lWzZ4LPPzME5P/0Uvv3W6kQiIiIi6eC336B3b3N5+HCoU8fSOJJ8KrpFxOnUrg19+5rL3bvDv/9am0dEREQkTV2/bt7HffWqeb/doEFWJ5IUUNEtIk5p9Gh49FE4ffq/AlxERETkofTmm+aI5fnywRdfgKur1YkkBVR0i4hT8vY2u5m7uMCCBfDll1YnEhEREUkDK1bAzJnm8oIFULCgtXkkxVR0i4jTqlkT3nrLXO7ZE86ftzaPiIiISKr66y9zyhaAd96BRo2szSP3RUW3iDi1ESOgbFmIjDRn0BARERF5KNy8Ce3bQ1SUeaVh1CirE8l9UtEtIk7N09PsZu7qCkuWmD2wRERERJzeu+/Crl2QM6c5PZibm9WJ5D6p6BYRp/f44zBwoLncqxecPWttHhEREZEH8s03MGGCuTxvHhQtam0eeSAqukXkoTB0KFSsCOfOmVNYGobViURERETuw8mT0KWLufzGG9C8uaVx5MGp6BaRh4K7u9nNPEsWWLnS7GouIiIi4lRiY6FTJ3N02Mcegw8+sDqRpALLi+4ZM2bg7++Pp6cnNWrUYNeuXXdsGxMTw8iRIwkICMDT05NKlSoRGhrq0CYuLo4hQ4ZQrFgxvLy8CAgIYNSoURi3XPZ65ZVXsNlsDo/GjRs77OfChQt06tSJ7NmzkzNnTrp27cqVK1dS982LSKp67DEYPNhcfv11OHPG2jwiIiIiKTJqFGzZAtmywdKl4OFhdSJJBZYW3UuXLiU4OJhhw4axZ88eKlWqRKNGjTh7hxsyBw8ezMcff8y0adM4ePAgr732Gi1atGDv3r32NuPGjWPmzJlMnz6dQ4cOMW7cOMaPH8+0adMc9tW4cWPOnDljfyxevNjh+U6dOvHrr78SFhbG119/zZYtW+jRo0fqHwQRSVWDBpnF97//mtOIqZu5iIiIOAPbd9/9N0L5xx9DyZLWBpJUY2nRPWnSJLp3705gYCBly5Zl1qxZeHt7M3fu3CTbL1iwgEGDBtGkSROKFy9Or169aNKkCRMnTrS32b59O82aNaNp06b4+/vTunVrGjZsmOgKuoeHB76+vvZHrly57M8dOnSI0NBQPvnkE2rUqMFTTz3FtGnTWLJkCadPn06bgyEiqcLNDebPN//71VewYIHViURERETuzv3iRVy7dDGvFnTtCh07Wh1JUpFlRffNmzfZvXs39evX/y+Miwv169dnx44dSW4THR2Np6enwzovLy+2bdtm/7lWrVqEh4fz+++/A7B//362bdvGc88957Ddpk2byJ8/P6VKlaJXr16cP3/e/tyOHTvImTMn1apVs6+rX78+Li4u7Ny58/7ftIikiwoVzPm7Afr0MccjEREREcmQ4uOpMmUKtogIKFsWpk61OpGksixWvfC5c+eIi4ujQIECDusLFCjA4cOHk9ymUaNGTJo0iTp16hAQEEB4eDirVq0iLi7O3mbgwIFERUVRunRpXF1diYuLY/To0XTq1MnepnHjxrRs2ZJixYrx559/MmjQIJ577jl27NiBq6srERER5M+f3+G1s2TJQu7cuYmIiLjje4qOjiY6Otr+c1RUFGDeix4TE5P8g3ObhG0fZB9WcdbszpobnDd7aufu1w9Wr3blxx9d6NYtnrVr47DZUmXXDpz1eIPzZnfW3JB62Z3xvYuISNJcJk6kwN69GJ6e2JYuBW9vqyNJKrOs6L4fU6ZMoXv37pQuXRqbzUZAQACBgYEO3dGXLVvGwoULWbRoEeXKlWPfvn3069cPPz8/uvxv6P327dvb21eoUIGKFSsSEBDApk2bePbZZ+8735gxYxiRcHntFuvXr8c7Ff7xhIWFPfA+rOKs2Z01Nzhv9tTM3blzNvbte5p161wJDt5PgwYnUm3ft3PW4w3Om91Zc8ODZ7927VoqJREREUtt347L0KEAxE2eTJby5S0OJGnBsqI7b968uLq6EhkZ6bA+MjISX1/fJLfJly8fa9as4caNG5w/fx4/Pz8GDhxI8eLF7W0GDBjAwIED7YV1hQoVOH78OGPGjLEX3bcrXrw4efPm5Y8//uDZZ5/F19c30WBusbGxXLhw4Y7ZAEJCQggODrb/HBUVReHChWnYsCHZs2e/+wG5i5iYGMLCwmjQoAFubm73vR8rOGt2Z80Nzps9rXJfvQoDB8Lnn1emX7/yFC2aarsGnPd4g/Nmd9bckHrZE3pSiYiIE7twATp0wBYXx8natSkQGGh1IkkjlhXd7u7uVK1alfDwcJr/b8L3+Ph4wsPDCQoKuuu2np6eFCpUiJiYGFauXEnbtm3tz127dg0XF8db1V1dXYmPj7/j/k6ePMn58+cpWLAgADVr1uTixYvs3r2bqlWrArBx40bi4+OpUaPGHffj4eGBRxLD+ru5uaXKH4aptR8rOGt2Z80Nzps9tXO/9RasXQvbt9t47TU31q8HlzQYzcJZjzc4b3ZnzQ0Pnt1Z37eIiPxPwoBpJ05gBASwv1cvGqbFfXCSIVg6enlwcDBz5sxh/vz5HDp0iF69enH16lUC//ctT+fOnQkJCbG337lzJ6tWreLo0aNs3bqVxo0bEx8fz9tvv21v88ILLzB69Gi++eYbjh07xurVq5k0aRItWrQA4MqVKwwYMIAffviBY8eOER4eTrNmzShRogSNGjUCoEyZMjRu3Jju3buza9cuvv/+e4KCgmjfvj1+fn7peIRE5EG5usJnn4GXF4SHmzNwiIiIiFhqxgxYswbc3IhduJBY3cf9ULP0nu527drxzz//MHToUCIiIqhcuTKhoaH2wdVOnDjhcNX6xo0bDB48mKNHj5ItWzaaNGnCggULyJkzp73NtGnTGDJkCL179+bs2bP4+fnRs2dPhv7vXglXV1d+/vln5s+fz8WLF/Hz86Nhw4aMGjXK4Sr1woULCQoK4tlnn8XFxYVWrVoxVSMJijilkiVh7Fjo2xcGDIBGjeCWu1JERERE0s+ePfDmm+byhAlQpQp8+621mSRNWT6QWlBQ0B27k2/atMnh57p163Lw4MG77s/Hx4fJkyczefLkJJ/38vJi3bp198yVO3duFi1adM92IuIcgoJg1SrYvBkCA+G779Kmm7mIiIjIHV2+DO3awc2b0KwZvPEGxMZanUrSmP7kFJFMwcUF5s6FrFlhyxaYNs3qRCIiIpKpGAa89hr88QcULmz+YaL7uDMFFd0ikmkULw4ffGAuh4TA779bm0dEREQykXnzYNEic8CZxYshd26rE0k6UdEtIpnKa69B/fpw/brZzTwuzupEIiIi8tD79VfzXjeAUaPgySetzSPpSkW3iGQqNht8+in4+MD27fDhh1YnEhERkYfatWvmfdzXr0ODBvDOO1YnknSmoltEMp0iRf4rtgcPhkOHrM0jIiIiD7F+/cwr3b6+sGCBRnLNhPQbF5FM6dVXoXFjiI6GLl00cKiIiIikgSVLYM4cs6vdF1/A/6ZGlsxFRbeIZEo2m3kOzJEDfvzxvwHWRERERFLFH39Ajx7m8rvvwrPPWptHLKOiW0QyrUcegalTzeVhw+CXX6zNIyIiIg+J6GjzPu7Ll6F2bfMPDcm0VHSLSKb28svwwgsQE2N2M4+JsTqRiIiIOL133oE9eyBPHnOasCxZrE4kFlLRLSKZms0Gs2ebU2Xu3Qvvv291IhEREXFqa9fClCnm8mefmV3rJFNT0S0imZ6vL0yfbi6/955ZfIuIiIik2IkT8Mor5nJwMDz/vKVxJGNQ0S0iArRvD61amaOYd+li3oolIiIikmwxMdChA/z7Lzz+OIwZY3UiySBUdIuIYHYz/+gjyJvXHFBt1CirE4mIiIhTGTYMtm+H7NnNqcLc3a1OJBmEim4Rkf/Jnx9mzTKXx441pxITERERuaf1680/HsCck7R4cWvzSIaioltE5BatWpldzePizG7mN25YnUhEREQytIgIczoUw4CePaFtW6sTSQajoltE5DbTp0OBAnDoEAwdanUaERERybDi4uCll+DsWahQAT780OpEkgGp6BYRuU2ePOY0YgATJpi3Z4mIiIgkMnYshIeDtzcsWwZeXlYnkgxIRbeISBJefBE6dzZ7ir3yCly7ZnUiERERyVC2bv2vS9yMGVC6tLV5JMNS0S0icgeTJ4OfHxw5Au++a3UaERERyTDOnTOnB4uPN+/n7tLF6kSSganoFhG5g1y54JNPzOUpU2DLFmvziIiISAZgGBAYCKdOwaOPmnOO2mxWp5IMTEW3iMhdPPccdO363/n1yhWrE4mIiIilJk+Gr78GDw9YuhSyZbM6kWRwKrpFRO5h0iQoXBiOHoWBA61OIyIiIpb58Ud45x1zedIkqFzZ0jjiHFR0i4jcQ/bsMHeuuTxjhjlIqYiIiGQyly5Bu3YQEwOtWkGvXlYnEieholtEJBnq1//v3PrqqxAVZW0eERERSUeGAT16wF9/gb+/OeiL7uOWZEpx0e3v78/IkSM5ceJEWuQREcmwxo83z7MnTsBbb1mdRkRERNLN7NnmPNxZssCSJZAzp9WJxImkuOju168fq1atonjx4jRo0IAlS5YQHR2dFtlERDKUbNlg3jxzec4cWLfO2jwiIiKSDn7+Gfr1M5fHjIEaNSyNI87nvoruffv2sWvXLsqUKcMbb7xBwYIFCQoKYs+ePWmRUUQkw3j6aejTx1zu2hUuXrQyjYiIiKSpq1fN+7hv3DCnNAkOtjqROKH7vqe7SpUqTJ06ldOnTzNs2DA++eQTHn/8cSpXrszcuXMxDCM1c4qIZBjvvw8lSpjTc771lqvVcURERCStBAXB4cPg5wfz54OLhsSSlLvvT01MTAzLli3jxRdf5M0336RatWp88skntGrVikGDBtGpU6fUzCkikmFkzQqffWaOn/L55y78+GMBqyOJiIhIaluwwDzhu7jAokWQL5/VicRJpbjo3rNnj0OX8nLlynHgwAG2bdtGYGAgQ4YMYcOGDaxevTot8oqIZAhPPvlfD7OPPqrMhQvW5hHJKGbMmIG/vz+enp7UqFGDXbt23bX98uXLKV26NJ6enlSoUIFvv/3W4XmbzZbk44MPPkjLtyEimd1vv/03bcnQoVC3rrV5xKmluOh+/PHHOXLkCDNnzuTUqVNMmDCB0qVLO7QpVqwY7du3T7WQIiIZ0ahRUKqUwb//etKvn7qZiyxdupTg4GCGDRvGnj17qFSpEo0aNeLs2bNJtt++fTsdOnSga9eu7N27l+bNm9O8eXMOHDhgb3PmzBmHx9y5c7HZbLRq1Sq93paIZDY3bpj3cV+9ag7mMniw1YnEyaW46D569CihoaG0adMGNze3JNtkzZqVeQlD/IqIPKS8vODTT+NwcTFYssSFVausTiRirUmTJtG9e3cCAwMpW7Yss2bNwtvbm7lz5ybZfsqUKTRu3JgBAwZQpkwZRo0aRZUqVZg+fbq9ja+vr8Pjyy+/pF69ehQvXjy93paIZDZvvgn795vdyRcuBFd9sS4PJsVF99mzZ9m5c2ei9Tt37uSnn35KlVAiIs6ienWDFi2OAPDaa/DPPxYHErHIzZs32b17N/Xr17evc3FxoX79+uzYsSPJbXbs2OHQHqBRo0Z3bB8ZGck333xD165dUy+4iMitVq6Ejz4ylz//3BxATeQBZUnpBq+//jpvv/02NW6bn+7UqVOMGzcuyYJcRORh1r79bxw+XJJff7Xx+uuwbJnViUTS37lz54iLi6NAAceBBQsUKMDhw4eT3CYiIiLJ9hEREUm2nz9/Pj4+PrRs2fKOOaKjo4mOjrb/HBUVBZgDwMbExCTrvdxJwvYPup/05qy5wXmzO2tucN7sqZL7r7/I0rUrNiDuzTeJf/ZZSIfjkKmPuQVSM3dy95HiovvgwYNUqVIl0frHHnuMgwcPpnR3IiJOz80tnrlzY6lVy43ly2HpUvNWMBFJXXPnzqVTp054enresc2YMWMYMWJEovXr16/H29s7VXKEhYWlyn7Sm7PmBufN7qy5wXmz329uW2wsTw0aRO5Ll7hQqhTbatbEuG1gx7SW2Y651VIj97Vr15LVLsVFt4eHB5GRkYnupTpz5gxZsqR4dyIiD4XHHoN334WRI6F3b3OQU19fq1OJpJ+8efPi6upKZGSkw/rIyEh87/CPwdfXN9ntt27dym+//cbSpUvvmiMkJITghKkFMK90Fy5cmIYNG5I9e/bkvp0kxcTEEBYWRoMGDe44rk1G5Ky5wXmzO2tucN7sD5rbZeBAXH//HSNnTny+/prnihZNg5RJy6zH3CqpmTuhN9W9pLhKbtiwISEhIXz55ZfkyJEDgIsXLzJo0CAaNGiQ0t2JiDw03n0X1q6FffvM+7tXrzbn8hbJDNzd3alatSrh4eE0b94cgPj4eMLDwwkKCkpym5o1axIeHk6/fv3s68LCwqhZs2aitp9++ilVq1alUqVKd83h4eGBh4dHovVubm6p9kdhau4rPTlrbnDe7M6aG5w3+33l/r//g0mTALB9+iluJUqkQbJ7y1THPANIjdzJ3T7FA6lNmDCBv//+m6JFi1KvXj3q1atHsWLFiIiIYOLEiSkOmpL5PGNiYhg5ciQBAQF4enpSqVIlQkNDHdrExcUxZMgQihUrhpeXFwEBAYwaNQrDMOz7eOedd6hQoQJZs2bFz8+Pzp07c/r0aYf9+Pv7J5oTdOzYsSl+fyKSebi7w/z54OYGX35pDngqkpkEBwczZ84c5s+fz6FDh+jVqxdXr14lMDAQgM6dOxMSEmJv37dvX0JDQ5k4cSKHDx9m+PDh/PTTT4mK9KioKJYvX063bt3S9f2ISCZw6hR07mwuBwXBXcaMELlfKb7SXahQIX7++WcWLlzI/v378fLyIjAwkA4dOqT4m4KE+TxnzZpFjRo1mDx5Mo0aNeK3334jf/78idoPHjyYL774gjlz5lC6dGnWrVtHixYt2L59O4899hgA48aNY+bMmcyfP59y5crx008/ERgYSI4cOejTpw/Xrl1jz549DBkyhEqVKvHvv//St29fXnzxxUSjr48cOZLu3bvbf/bx8Unp4RKRTKZiRRg2zJzS84034JlnNPCpZB7t2rXjn3/+YejQoURERFC5cmVCQ0Ptg6WdOHECF5f/vu+vVasWixYtYvDgwQwaNIiSJUuyZs0aypcv77DfJUuWYBgGHTp0SNf3IyIPubg46NQJzp2DypXhgw+sTiQPqfu6CTtr1qz06NHjgV/81vk8AWbNmsU333zD3LlzGThwYKL2CxYs4N1336VJkyYA9OrViw0bNjBx4kS++OILALZv306zZs1o2rQpYF6xXrx4sf0Keo4cORLdND99+nSqV6/OiRMnKFKkiH29j4/PHe9DExG5k3fegTVr4KefoHt3+PprdTOXzCMoKOiO3ck3bdqUaF2bNm1o06bNXffZo0ePVPm7Q0TEwahRsHkzZMtmjoJ6l0EaRR7EfY98dvDgQU6cOMHNmzcd1r/44ovJ2j5hPs9bu5ndaz7P6OjoRCOWenl5sW3bNvvPtWrVYvbs2fz+++88+uij7N+/n23btjHpf/dpJOXSpUvYbDZy5szpsH7s2LGMGjWKIkWK0LFjR/r373/XweLSapoSZx2OH5w3u7PmBufN/rDl/uQTqF49C99+a+PTT2Pp0sWwIt5dPWzH3BmkVnZnfO8iIhnKd9+Zo58CzJoFjz5qbR55qKW46D569CgtWrTgl19+wWaz2e+Vtv3vMk5cXFyy9nM/83k2atSISZMmUadOHQICAggPD2fVqlUOrzlw4ECioqIoXbo0rq6uxMXFMXr0aDp16pTkPm/cuME777xDhw4dHEY17dOnD1WqVCF37txs376dkJAQzpw5c9fiPa2nKXHW4fjBebM7a25w3uwPU+727Uvw+efl6NvXADaSL9+N9A+WDA/TMXcWD5o9uVOUiIhIEs6eNbuVGwYEBprLImkoxUV33759KVasGOHh4RQrVoxdu3Zx/vx53nzzTSZMmJAWGe2mTJlC9+7dKV26NDabjYCAAAIDA5k7d669zbJly1i4cCGLFi2iXLly7Nu3j379+uHn50eXLl0c9hcTE0Pbtm0xDIOZM2c6PHfrdCMVK1bE3d2dnj17MmbMmCRHRYW0m6bEWYfjB+fN7qy5wXmzP4y5GzWC336LZ+dON5YubcA338RlqG7mD+Mxz+hSK3typyhJrr///hubzcYjjzwCwK5du1i0aBFly5ZVt24RebjEx0OXLnDmDJQpA9OmWZ1IMoEUF907duxg48aN5M2bFxcXF1xcXHjqqacYM2YMffr0Ye/evcnaz/3M55kvXz7WrFnDjRs3OH/+PH5+fgwcONBhzvABAwYwcOBA2rdvD0CFChU4fvw4Y8aMcSi6Ewru48ePs3HjxnsWxTVq1CA2NpZjx45RqlSpJNuk9TQlzjocPzhvdmfNDc6b/WHK7eZmjmZeuTJs2ODCvHku9OxpTb67eZiOubN40Oyp/b47duxIjx49ePnll4mIiKBBgwaUK1eOhQsXEhERwdChQ1P19URELDNhAoSGmvdvL1sGWbNanUgygRRPGRYXF2cfxTtv3rz2qbaKFi3Kb7/9luz93DqfZ4KE+TyTmp/zVp6enhQqVIjY2FhWrlxJs2bN7M9du3bNYWRUAFdXV+Lj4+0/JxTcR44cYcOGDeTJk+eeefft24eLi0uSo6qLiNxJqVLw/vvm8ptvwl9/WZtHJCkHDhygevXqgNljrHz58mzfvp2FCxfy2WefWRtORCS1/PADvPuuuTx1Ktw2U4JIWknxle7y5cuzf/9+ihUrRo0aNRg/fjzu7u7Mnj3b4YpzcgQHB9OlSxeqVatG9erVmTx5cqL5PAsVKsSYMWMA2LlzJ6dOnaJy5cqcOnWK4cOHEx8fz9tvv23f5wsvvMDo0aMpUqQI5cqVY+/evUyaNIlXX30VMAvu1q1bs2fPHr7++mvi4uKIiIgAIHfu3Li7u7Njxw527txJvXr18PHxYceOHfTv35+XXnqJXLlypfSQiUgm17cvrF4NW7fCq69CeDi4pPgrT5G0ExMTY++ptWHDBvugqKVLl+bMmTNWRhMRSR3//gvt20NsLLRrB926WZ1IMpEUF92DBw/m6tWrgDmP9fPPP0/t2rXJkycPS5cuTdG+Ujqf540bNxg8eDBHjx4lW7ZsNGnShAULFjiMOj5t2jSGDBlC7969OXv2LH5+fvTs2dPeNe7UqVOsXbsWgMqVKzvk+e6773j66afx8PBgyZIlDB8+nOjoaIoVK0b//v0d7tcWEUkuFxeYN8+cw3vTJvjoI7jDjEoilihXrhyzZs2iadOmhIWFMWrUKABOnz6drN5gIiIZmmFA165w/DgULw6zZ2suT0lXKS66GzVqZF8uUaIEhw8f5sKFC+TKlcs+gnlKpGQ+z7p163Lw4MG77s/Hx4fJkyczefLkJJ/39/e3j7h+J1WqVOGHH364axsRkZQICIDx481i+513oHFjKFHC6lQipnHjxtGiRQs++OADunTpQqVKlQBYu3atvdu5iIjT+ugjs8uZm5s5H/cDDHAscj9SVHTHxMTg5eXFvn37KH/LPRC5c+dO9WAiIg+bXr1g5UpzatBXXoHNm8HV1epUIvD0009z7tw5oqKiHG6j6tGjR6pMeSkiYpm9eyGht+r48VCtmrV5JFNK0V2Fbm5uFClSJNlzcYuIyH9cXGDuXMiWDb7/HqZMsTqRiOn69etER0fbC+7jx48zefJkfvvtNw0gKiLO6/Jl8/7tmzfhxRfNQVZELJDioXzeffddBg0axIULF9Iij4jIQ83fHyZONJfffRcOH7Y0jggAzZo14/PPPwfg4sWL1KhRg4kTJ9K8eXNmzpxpcToRkftgGGYXsyNH4JFHzG+9dR+3WCTFRff06dPZsmULfn5+lCpViipVqjg8RETk7rp3h4YN4cYNs5u5Og+J1fbs2UPt2rUBWLFiBQUKFOD48eN8/vnnTJ061eJ0IiL34bPPYOFC8z6uxYtBg0KKhVI8kFrz5s3TIIaISOZhs8Enn5jTg+7cCRMmmIOriVjl2rVr+Pj4ALB+/XpatmyJi4sLTzzxBMePH7c4nYhICh08+N80ISNHwlNPWZtHMr0UF93Dhg1LixwiIplK4cLmPd2BgTB0KDz/PJQrZ3UqyaxKlCjBmjVraNGiBevWraN///4AnD17luwa5VdEnMn16+Z93NeuQf36MHCg1YlEUt69XEREUkeXLmaxffOmuRwTY3UiyayGDh3KW2+9hb+/P9WrV6dmzZqAedX7scceszidiEjyubz5Jhw4AAUKwIIF5iimIhZL8afQxcUFV1fXOz5ERCR5bDb4+GPIlQt274Zx46xOJJlV69atOXHiBD/99BPr1q2zr3/22Wf58MMPLUwmIpJ8ftu24frJJ+YJ9osvwNfX6kgiwH10L1+9erXDzzExMezdu5f58+czYsSIVAsmIpIZ+PnBtGnw0kvmbWcvvACVKlmdSjIjX19ffH19OXnyJACPPPII1atXtziViEgy/fknlWfMMJcHDTK7lotkECkuups1a5ZoXevWrSlXrhxLly6la9euqRJMRCSz6NgRVq6E1avNbua7doG7u9WpJDOJj4/nvffeY+LEiVy5cgUAHx8f3nzzTd59911c1D1TRDKy6GhcO3XC5fp14p98Epfhw61OJOIg1c6iTzzxBOHh4am1OxGRTMNmg5kzzdlM9u+H996zOpFkNu+++y7Tp09n7Nix7N27l7179/L+++8zbdo0hgwZYnU8EZG7GzgQlz17uOnjQ9znn0OWFF9XFElTqVJ0X79+nalTp1KoUKHU2J2ISKZToIBZeAO8/755j7dIepk/fz6ffPIJvXr1omLFilSsWJHevXszZ84cPvvsM6vjiYjc2dq1MHkyAHveeMOcHkQkg0nx10C5cuXCZrPZfzYMg8uXL+Pt7c0XX3yRquFERDKTNm2gbVtYtszsZr57N3h4WJ1KMoMLFy5QunTpROtLly7NhQsXLEgkIpIMf/9tzr0JxPXpQ6TGoZAMKsVF94cffuhQdLu4uJAvXz5q1KhBrly5UjWciEhmM2MGbNoEv/4Kw4fDmDFWJ5LMoFKlSkyfPp2pU6c6rJ8+fToVK1a0KJWIyF3ExkKHDnDhAlSrRvz778OGDVanEklSiovuV155JQ1iiIgIQN68MGsWtGwJ48dDs2bwxBNWp5KH3fjx42natCkbNmywz9G9Y8cO/v77b7799luL04mIJGHYMPj+e/DxgSVLNAKpZGgpvqd73rx5LF++PNH65cuXM3/+/FQJJSKSmbVoAZ06QXw8vPIKXL9udSJ52NWtW5fff/+dFi1acPHiRS5evEjLli359ddfWbBggdXxREQcbdjwX1ewTz6BgABr84jcQ4qL7jFjxpA3b95E6/Pnz8/777+fKqFERDK7qVOhYEH47TcYPNjqNJIZ+Pn5MXr0aFauXMnKlSt57733+Pfff/n000+tjiYi8p+ICHjpJTAM6NHDHAxFJINLcdF94sQJihUrlmh90aJFOXHiRKqEEhHJ7HLnhjlzzOUPP4Rt26zNIyIiYrn4eHj5ZYiMhPLl7aOWi2R0KS668+fPz88//5xo/f79+8mTJ0+qhBIREWja1ByU1TDMbuZXr1qdSERExEJjx5pdy729YelS8PKyOpFIsqS46O7QoQN9+vThu+++Iy4ujri4ODZu3Ejfvn1p3759WmQUEcm0PvwQHnkE/vwTQkKsTiMiImKRbdtgyBBzefp0KFvW2jwiKZDi0ctHjRrFsWPHePbZZ8mSxdw8Pj6ezp07655uEZFUliMHfPopNGoE06aZo5o//bTVqeRh0bJly7s+f/HixfQJIiJyN+fPm9ODxceb93NrNiVxMikuut3d3Vm6dCnvvfce+/btw8vLiwoVKlC0aNG0yCcikuk1bGiOFTN7ttnd/OefzRlSRB5Ujhw57vl8586d0ymNiEgSDMM8+Z08CSVLwkcfgc1mdSqRFElx0Z2gZMmSlCxZMjWziIjIHUyYAOvWwbFj8PbbMHOm1YnkYTBv3jyrI4iI3N2UKfDVV+Y83MuW6VtncUopvqe7VatWjBs3LtH68ePH06ZNm1QJJSIijnx8IKE+mjULwsKszSMiIpLmfvrJ/KYZYNIkqFzZ0jgi9yvFRfeWLVto0qRJovXPPfccW7ZsSZVQIiKSWL16EBRkLnftCpcuWZtHREQkzVy6BO3aQUwMtGgBvXtbnUjkvqW46L5y5Qru7u6J1ru5uREVFZUqoUREJGljx0JAAPz9NwQHW51GREQkDRiGOZjJ0aNQtKg5oqju4xYnluKiu0KFCixdujTR+iVLllBWQ/eLiKSprFnNbuY2G8ydC99+a3UiERGRVDZnjnn/dpYssGQJ5MpldSKRB5LigdSGDBlCy5Yt+fPPP3nmmWcACA8PZ9GiRaxYsSLVA4qIiKPataFfP3MO7+7d4cAB/T0iIiIPiV9+gb59zeX334cnnrA2j0gqSPGV7hdeeIE1a9bwxx9/0Lt3b958801OnTrFxo0bKVGiRFpkFBGR24weDY8+CqdP//e3iYiIiFO7etW8j/vGDXjuOXjzTasTiaSKFBfdAE2bNuX777/n6tWrHD16lLZt2/LWW29RqVKl1M4nIiJJ8PKC+fPBxQUWLIAvv7Q6kYiIyAN64w04dAgKFvzvJCfyELjvT/KWLVvo0qULfn5+TJw4kWeeeYYffvghNbOJiMhdPPEEDBhgLvfoAefOWZtHRETkvn3xhTloiYsLLFoE+fJZnUgk1aSo6I6IiGDs2LGULFmSNm3akD17dqKjo1mzZg1jx47l8ccfT6ucIiKShOHDoWxZOHv2v+nEREREnMrvv8Nrr5nLQ4bA009bGkcktSW76H7hhRcoVaoUP//8M5MnT+b06dNMmzYtLbOJiMg9eHqaPfBcXWHpUli+3OpEIiIiKXDjhnkf99WrZrE9ZIjViURSXbKL7v/7v/+ja9eujBgxgqZNm+Lq6pqWuUREJJmqVYOQEHO5d2/zqreIiIhTGDAA9u2DvHlh4ULzW2SRh0yyi+5t27Zx+fJlqlatSo0aNZg+fTrndAOhiEiGMGQIVKxo3tf92mtgGFYnEhERuYdVq2D6dHP588/Bz8/aPCJpJNlF9xNPPMGcOXM4c+YMPXv2ZMmSJfj5+REfH09YWBiXL19Oy5wiInIX7u5mN/MsWWD1ali82OpEIiIid3HsGHTtai4PGGBOESbykErx6OVZs2bl1VdfZdu2bfzyyy+8+eabjB07lvz58/Piiy+mOMCMGTPw9/fH09OTGjVqsGvXrju2jYmJYeTIkQQEBODp6UmlSpUIDQ11aBMXF8eQIUMoVqwYXl5eBAQEMGrUKIxbLvsYhsHQoUMpWLAgXl5e1K9fnyNHjjjs58KFC3Tq1Ins2bOTM2dOunbtypUrV1L8/kRE0kvlyv/dChcUBGfOWBpHREQkaTEx0KEDXLwINWrA6NFWJxJJUw80+V2pUqUYP348J0+eZPF9XFZZunQpwcHBDBs2jD179lCpUiUaNWrE2TvckDh48GA+/vhjpk2bxsGDB3nttddo0aIFe/futbcZN24cM2fOZPr06Rw6dIhx48Yxfvx4h0Hfxo8fz9SpU5k1axY7d+4ka9asNGrUiBs3btjbdOrUiV9//ZWwsDC+/vprtmzZQo8ePVL8HkVE0lNICFSpAv/+Cz17qpu5iIhkQIMHww8/QI4csGQJuLlZnUgkTaXKjPOurq40b96ctWvXpmi7SZMm0b17dwIDAylbtiyzZs3C29ubuXPnJtl+wYIFDBo0iCZNmlC8eHF69epFkyZNmDhxor3N9u3badasGU2bNsXf35/WrVvTsGFD+xV0wzCYPHkygwcPplmzZlSsWJHPP/+c06dPs2bNGgAOHTpEaGgon3zyCTVq1OCpp55i2rRpLFmyhNOnT9/fQRIRSQdubmY3c3d3+Oor8xY5ERGRDOP//g/GjzeX584Ff39L44ikh1Qpuu/HzZs32b17N/Xr1/8vjIsL9evXZ8eOHUluEx0djaenp8M6Ly8vtm3bZv+5Vq1ahIeH8/vvvwOwf/9+tm3bxnP/u0/kr7/+IiIiwuF1c+TIQY0aNeyvu2PHDnLmzEm1atXsberXr4+Liws7d+58wHcuIpK2ypeHESPM5b594eRJa/OIiIgAcPo0dO5sLr/+OrRsaW0ekXSSxaoXPnfuHHFxcRQoUMBhfYECBTh8+HCS2zRq1IhJkyZRp04dAgICCA8PZ9WqVcTFxdnbDBw4kKioKEqXLo2rqytxcXGMHj2aTp06ARAREWF/ndtfN+G5iIgI8ufP7/B8lixZyJ07t71NUqKjo4mOjrb/HBUVBZj3osfExNz1eNxNwrYPsg+rOGt2Z80NzptduVNX376wapUrP/7oQteu8Xz1VRw2m2ObjJr9Xpw1N6Redmd87yKSycXFQadO5jQblSvDhAlWJxJJN5YV3fdjypQpdO/endKlS2Oz2QgICCAwMNChO/qyZctYuHAhixYtoly5cuzbt49+/frh5+dHly5d0jTfmDFjGJFweekW69evx9vb+4H3HxYW9sD7sIqzZnfW3OC82ZU79XTpko19+55m/XpXgoP306DBiSTbZcTsyeGsueHBs1+7di2VkoiIpJP33oNNmyBrVli6FG7rvSryMLOs6M6bNy+urq5ERkY6rI+MjMTX1zfJbfLly8eaNWu4ceMG58+fx8/Pj4EDB1K8eHF7mwEDBjBw4EDat28PQIUKFTh+/DhjxoyhS5cu9n1HRkZSsGBBh9etXLkyAL6+vokGc4uNjeXChQt3zAYQEhJCcHCw/eeoqCgKFy5Mw4YNyZ49ezKOStJiYmIICwujQYMGuDnZQBPOmt1Zc4PzZlfutHH1KrzzDnz+eWX69StP0aL/PZfRs9+Js+aG1Mue0JNKRMQpbNoEI0eay7NmwaOPWhpHJL1ZVnS7u7tTtWpVwsPDad68OQDx8fGEh4cTFBR01209PT0pVKgQMTExrFy5krZt29qfu3btGi4ujrequ7q6Eh8fD0CxYsXw9fUlPDzcXmRHRUWxc+dOevXqBUDNmjW5ePEiu3fvpmrVqgBs3LiR+Ph4atSoccdcHh4eeHh4JFrv5uaWKn8YptZ+rOCs2Z01NzhvduVOXW++CWvXwvff2+jZ042wMLjtf5EZNvu9OGtuePDszvq+RSQT+ucf6NgR4uPhlVfgpZesTiSS7iwbSA0gODiYOXPmMH/+fA4dOkSvXr24evUqgYGBAHTu3JmQkBB7+507d7Jq1SqOHj3K1q1bady4MfHx8bz99tv2Ni+88AKjR4/mm2++4dixY6xevZpJkybRokULAGw2G/369eO9995j7dq1/PLLL3Tu3Bk/Pz978V+mTBkaN25M9+7d2bVrF99//z1BQUG0b98ePz+/9DtAIiIPyNUV5s0DLy/YuNG8wCAiIpIu4uOhSxc4cwZKl4bp061OJGIJS+/pbteuHf/88w9Dhw4lIiKCypUrExoaah/k7MSJEw5XrW/cuMHgwYM5evQo2bJlo0mTJixYsICcOXPa20ybNo0hQ4bQu3dvzp49i5+fHz179mTo0KH2Nm+//TZXr16lR48eXLx4kaeeeorQ0FCHkdEXLlxIUFAQzz77LC4uLrRq1YqpU6em/UEREUllJUvCuHHQpw8MGACNG8Mtd+WIiIikjUmTzCnCPD1h2TLzfm6RTMjygdSCgoLu2J1806ZNDj/XrVuXgwcP3nV/Pj4+TJ48mcmTJ9+xjc1mY+TIkYxMuLckCblz52bRokV3fS0REWfx+uuwapV5W11gIHz3ndWJRETkofbDD5DQY3XyZKhQwdI4IlaytHu5iIikDxcXmDvXvMiwZQtMm2Z1IhEReWj9+y+0bw+xsdC2LfToYXUiEUup6BYRySSKFftvWtSQEPj9d2vziIjIQ8gwoFs3OH7cvJdp9myw2axOJWIpFd0iIplIz55Qvz5cvw7durkSF2d1IhEReajMnGnez+TmBkuWQI4cVicSsZyKbhGRTMRmg08/BR8f+OEHF9auLWF1JBEReVjs2wf9+5vL48bB449bGkcko1DRLSKSyRQpAh9+aC4vWlSae4xPKSIicm9XrkC7dnDzJjz/PPTrZ3UikQxDRbeISCb06qvQuHE8MTGudOvmSmys1YlERMRpGQb06mUOFvLII/DZZ7qPW+QWKrpFRDIhmw1mzowja9ab/PSTC+PHW51IRESc1vz58MUX5lQZixZBnjxWJxLJUFR0i4hkUoUKQbduvwAwfDj88ou1eURExAkdOgSvv24ujxwJtWtbm0ckA1LRLSKSiT399Emefz6emBjo0gViYqxOJCIiTuP6dXMe7mvX4NlnYeBAqxOJZEgqukVEMjGbDT76KI7cuWHvXnj/fasTibObMWMG/v7+eHp6UqNGDXbt2nXX9suXL6d06dJ4enpSoUIFvv3220RtDh06xIsvvkiOHDnImjUrjz/+OCdOnEirtyAiydW/Pxw4APnzm93LXV2tTiSSIanoFhHJ5Hx9YcYMc/m992DPHmvziPNaunQpwcHBDBs2jD179lCpUiUaNWrE2bNnk2y/fft2OnToQNeuXdm7dy/NmzenefPmHDhwwN7mzz//5KmnnqJ06dJs2rSJn3/+mSFDhuDp6Zleb0tEkrJsGXz8sfnt7RdfmCcTEUmSim4REaFdO2jdGmJj4ZVXIDra6kTijCZNmkT37t0JDAykbNmyzJo1C29vb+bOnZtk+ylTptC4cWMGDBhAmTJlGDVqFFWqVGH69On2Nu+++y5NmjRh/PjxPPbYYwQEBPDiiy+SP3/+9HpbInK7o0ehe3dzeeBAaNDA2jwiGVwWqwOIiIj1zG7msHmzOaDayJEwerTVqcSZ3Lx5k927dxMSEmJf5+LiQv369dmxY0eS2+zYsYPg4GCHdY0aNWLNmjUAxMfH88033/D222/TqFEj9u7dS7FixQgJCaF58+ZJ7jM6OproW741ioqKAiAmJoaYBxy0IGH7B91PenPW3OC82Z01NyQj+82buLZti0tUFPG1ahE3ZEiGGBDkoT7mGZRyJ38fKrpFRASAfPlg5kzzivfYsdC8OTz+uNWpxFmcO3eOuLg4ChQo4LC+QIECHD58OMltIiIikmwfEREBwNmzZ7ly5Qpjx47lvffeY9y4cYSGhtKyZUu+++476tatm2ifY8aMYcSIEYnWr1+/Hm9v7/t9ew7CwsJSZT/pzVlzg/Nmd9bccOfs5ebOpcTu3dzMlo1NgYFcX78+nZPd3cN4zDO6zJz72rVryWqnoltEROxatYIOHWDxYnM08z17QLfOilXi4+MBaNasGf379wegcuXKbN++nVmzZiVZdIeEhDhcPY+KiqJw4cI0bNiQ7NmzP1CemJgYwsLCaNCgAW5ubg+0r/TkrLnBebM7a264e3bb11+TZe1aAFzmz6feCy9YETFJD+sxz8iU+7/eVPeioltERBxMmwbffWdOvTp0KIwfb3UicQZ58+bF1dWVyMhIh/WRkZH43mGAJV9f37u2z5s3L1myZKFs2bIObcqUKcO2bduS3KeHhwceHh6J1ru5uaXaH4Wpua/05Ky5wXmzO2tuSCL7339Dt27mct++ZGnZ0ppg9/BQHXMnkZlzJ3d7DaQmIiIO8uSB2bPN5QkTYPt2a/OIc3B3d6dq1aqEh4fb18XHxxMeHk7NmjWT3KZmzZoO7cHs7pfQ3t3dnccff5zffvvNoc3vv/9O0aJFU/kdiMgdxcZCx45w4QJUrQrjxlmdSMSp6Eq3iIgk8sILZvfy+fPN0cz37YNUuh1WHmLBwcF06dKFatWqUb16dSZPnszVq1cJDAwEoHPnzhQqVIgxY8YA0LdvX+rWrcvEiRNp2rQpS5Ys4aeffmJ2wrc+wIABA2jXrh116tShXr16hIaG8tVXX7Fp0yYr3qJI5jRiBGzbBj4+sHQpJNGbRETuTFe6RUQkSZMnQ6FCcOQIDBpkdRpxBu3atWPChAkMHTqUypUrs2/fPkJDQ+2DpZ04cYIzZ87Y29eqVYtFixYxe/ZsKlWqxIoVK1izZg3ly5e3t2nRogWzZs1i/PjxVKhQgU8++YSVK1fy1FNPpfv7E8mUNmz4bzqL2bMhIMDaPCJOSFe6RUQkSTlzwiefwHPPwZQp0KIFJDFulYiDoKAggoKCknwuqavTbdq0oU2bNnfd56uvvsqrr76aGvFEJCUiI+Gll8AwzHm527e3OpGIU9KVbhERuaPGjf8bNycwEK5csTaPiIikk/h4ePlls/AuV87s/iQi90VFt4iI3NXEiVCkCPz1F7zzjtVpREQkzcTFYdu8mUJbtuDSuzeEhYGXFyxbpoE9RB6AupeLiMhdZc8On34KDRrARx9By5bw7LNWpxIRkVS1apU5FdjJk1S7dX1gINw2bZ+IpIyudIuIyD3Vrw+9epnLr74KUVHW5hERkVS0ahW0bg0nTyZ+buZM83kRuW8qukVEJFnGj4dixeDECXjrLavTiIhIqoiLg759zcHS7qRfP7OdiNwXFd0iIpIs2bLBvHnm8pw5EBpqbR4REUkFW7cmfYU7gWHA33+b7UTkvqjoFhGRZKtb17wgAuao5hcvWhpHREQe1N9/J6/dmTNpm0PkIaaiW0REUuT996FkSTh1yuxxKCIiTur772Ho0OS1LVgwbbOIPMRUdIuISIp4e8Nnn4HNBvPnw1dfWZ1IRERS5N9/oWdPeOopOHbs/9u78/AoqnyN49/OzmpwCIRAZAkjmxAQJAZHAwJJCMMF9SI4LiEsyiZiHNZhERgFFREGo+gAQUEFlUVBQEIkuCEoywVEGEEU2RWFsIYsdf+oobVNAiGku7qS9/M8eeiuPl15u9Lk9C916hzwuUxJ4HBAeDjcfrvH4omUNiq6RUTkqrVpA088Yd5++GE4ccLaPCIiUgSGAYsWQaNG8Oqr5rY+fWDuXLO4djhc21+6P306+Pp6NKpIaaKiW0REimXSJGjYEI4ehSFDrE4jIiKXtX8/JCRAz55w7Jj5C3z9epg9GxIT4d13oWZN1+fUqmVuv/tuazKLlBIqukVEpFiCgszh5T4+8OabWsZVRMQrZWebaz42aWIuOxEQABMmwLZtcMcdv7W7+274/nty0tL4KjmZnLQ0s1BXwS1yzVR0i4hIsbVuDSNHmrf794effrI2j4iI/M7GjdCqFYwYAefPQ9u2sH27OXlaYGD+9r6+GDExHLrjDoyYGA0pFykhKrpFROSajBsHTZuaBffAgeYlgyIiYqHMTBg8GKKjzSL7+ushNRU++ggaNLA6nUiZo6JbRESuSWCgOZu5n5956d/bb1udSESkjDIM81qfRo0gJcW8/+CDsHs39OqVf6I0EfEIFd0iInLNbr4Z/vEP8/bAgebkaiIi4kE//gjdusE998Dhw1C/PqxdC6+/DiEhVqcTKdNUdIuISIn4xz+geXP45Rfz+m4NMxcR8YDcXHNJr0aN4P33wd/f/IW8fTu0b291OhHBS4rulJQU6tSpQ1BQEFFRUWzatKnQttnZ2UycOJGIiAiCgoKIjIxk9erVLm3q1KmDw+HI9zVo0CAAvv/++wIfdzgcvPPOO879FPT4woUL3XMQRERszt/fnM3c3x/eew8WLLA6kYhIKbdlC0RFweOPw9mzcNttsHUr/POfUK6c1elE5L8sL7oXLVpEcnIy48ePZ8uWLURGRhIXF8fx48cLbD9mzBheeeUVZs6cya5du+jfvz933XUXW7dudbb58ssvOXLkiPMrLS0NgO7duwMQHh7u8viRI0eYMGECFStWpFOnTi7fLzU11aVdt27d3HMgRERKgWbNYPx48/aQIXDokLV5RERKpTNnIDkZbrkFNm+G4GB49VX4+GNzaTAR8SqWF93Tpk2jX79+JCUl0bhxY2bNmkX58uWZO3duge3nz5/P6NGjSUhIoF69egwYMICEhASef/55Z5uQkBBCQ0OdXytWrCAiIoKYmBgAfH19XR4PDQ1l6dKl3HvvvVSsWNHl+wUHB7u0CwoKct/BEBEpBUaMMFeoOXkSHn5Yw8xFRErU8uXQuDG88ALk5UHPnvDNN9CvH/hY/tFeRArgZ+U3v3jxIps3b2bUqFHObT4+PnTo0IENGzYU+JysrKx8hW+5cuX49NNPC/0eCxYsIDk5GUchMzZu3ryZbdu2kZKSku+xQYMG0bdvX+rVq0f//v1JSkoqdD9ZWVlkZWU572dmZgLmkPjs7OwCn1MUl557Lfuwil2z2zU32De7cnueO7PPng1RUX6sXOlg9uwcevUqucpbx9yer11ErtHhw+YQosWLzft168JLL0F8vLW5ROSKLC26f/75Z3Jzc6levbrL9urVq7N79+4CnxMXF8e0adO44447iIiIID09nSVLlpCbm1tg+2XLlnHy5El69epVaI45c+bQqFEj2rRp47J94sSJ3HnnnZQvX541a9YwcOBAzpw5w5AhQwrcz+TJk5kwYUK+7WvWrKF8+fKFfv+iujRM3o7smt2uucG+2ZXb89yVvWfP+rz2WhOGDjVwONYREnK+RPdflo/5uXPnSiiJiHi93FyYNQtGjYLTp8HXF554wryWpwQ+X4qI+1ladBfHjBkz6NevHw0bNsThcBAREUFSUlKhw9HnzJlDp06dCAsLK/Dx8+fP8+abbzJ27Nh8j/1+W4sWLTh79izPPfdcoUX3qFGjSE5Odt7PzMwkPDyc2NhYKleufDUv00V2djZpaWl07NgRf3//Yu/HCnbNbtfcYN/syu157s4eFwd79uTxxRf+LFrUgQ8+yC2RJWJ1zH8bSSUipdz27eZ1Ohs3mvejosxrt5s1szaXiFwVS4vuqlWr4uvry7Fjx1y2Hzt2jNDQ0AKfExISwrJly7hw4QInTpwgLCyMkSNHUq9evXxtf/jhB9auXcuSJUsKzfDuu+9y7tw5HnrooSvmjYqKYtKkSWRlZREYGJjv8cDAwAK3+/v7l8gHw5LajxXsmt2uucG+2ZXb89yV/dJs5pGRsHatD6mpPjzySEnuv+wec7u+bhEponPnYMIEeP5580x35coweTI88oh5pltEbMXS2RYCAgJo2bIl6enpzm15eXmkp6cTHR192ecGBQVRs2ZNcnJyWLx4MV27ds3XJjU1lWrVqtG5c+dC9zNnzhz+53/+h5CQkCvm3bZtG1WqVCmwsBYRkfxuvNH8nAjmaMj9+63NIyLi9VavNmcgf/ZZs+C+5x5zorSBA1Vwi9iU5cPLk5OTSUxMpFWrVrRu3Zrp06dz9uxZkpKSAHjooYeoWbMmk//7qW3jxo0cOnSI5s2bc+jQIZ588kny8vIYPny4y37z8vJITU0lMTERP7+CX+bevXv5+OOPWblyZb7Hli9fzrFjx7j11lsJCgoiLS2Np59+mr///e8lfAREREq3IUNgyRL45BPo3RvS0zXBrohIPkePmuttL1xo3g8Ph5QU6NLF2lwics0sL7p79OjBTz/9xLhx4zh69CjNmzdn9erVzsnVDhw4gM/vPp1duHCBMWPG8N1331GxYkUSEhKYP38+wcHBLvtdu3YtBw4coHfv3oV+77lz51KrVi1iY2PzPebv709KSgqPP/44hmFQv3595/JmIiJSdD4+kJpqXoKYkWF+hnz0UatTiYh4ibw8c8mHESPMtRZ9fOCxx2DiRPjDUrYiYk+WF90AgwcPZvDgwQU+lpGR4XI/JiaGXbt2XXGfsbGxGFdYHPbpp5/m6aefLvCx+Ph44rUEg4hIiYiIgOeeg0GDzM+VnTpB/fpWpxIRsdiuXeZ12peWvr35ZnOitJYtrc0lIiVKA/xERMQj+veHO++E8+ehVy/zUkURkTLpwgUYOxaaNzcL7goV4IUXzFnKVXCLlDoqukVExCN8fGDuXHO05GefwYwZVicSEbFAejo0bQr//CdkZ5vXbO/aBUOHQiHzEImIvanoFhERj6ldG6ZNM2+PHg27d1ubR0TEY376CRIToUMH2LsXwsJg8WJ47z244Qar04mIG6noFhERj+rbF+LiICvL/PyZk2N1IhERNzIMmDcPGjWC118HhwMGDzbPbt99t3lfREo1Fd0iIuJRDoc5Ue9118GmTTB1qtWJRETcZM8eczKLpCQ4ccJcxmHDBpg50/wlKCJlgopuERHxuFq1frume/x42LnT2jwiIiUqK4sbFy3Cr2VLc63EcuXg2Wfhq68gKsrqdCLiYSq6RUTEEg89BH/9K1y8aA4zz862OpGISAn4+GP8WrWi0Vtv4bh4EeLj4euvYdgw8Pe3Op2IWEBFt4iIWMLhMJejrVIFtmyBKVOsTiQicg1++cWctCImBseePVwIDiZnwQJYuRLq1rU6nYhYSEW3iIhYpkYNePFF8/bEibBtm6VxRESunmHAG29Aw4YwZw4AuX378tGLL2Lce68mShMRFd0iImKt++6Du+4yZzFPTDSHm4uI2MK+feZyDA88YC4J1rgxfPIJeS+9RHbFilanExEvoaJbREQs5XDArFlQtSps3w7//KfViUREruDiRZg8GW66CdLSIDDQ/OW1dSv85S9WpxMRL6OiW0RELFetGrz0knn76afNCX5FRLzS559Dy5YwejRcuADt25tLMPzjHxAQYHU6EfFCKrpFRMQrdO8OPXpAbq45zDwry+pEIiK/c/IkDBhgnsneudMcnvP66+aZ7vr1rU4nIl5MRbeIiHiNF180z3rv2mWu3y0iYjnDgLffhkaNzGthDAOSkmD3bnjwQU2UJiJXpKJbRES8RtWq5jJiAM89B198YW0eESnjvv8e/vpXcxjO0aNw442wbh3MnQt/+pPV6UTEJlR0i4iIV+na1ZwIOC8PevWC8+etTiQiZU5ODkydCk2amOtsBwSYw2+2b4e2ba1OJyI2o6JbRES8zr/+Za7hvWcPjBljdRoRKVO+/BJuuQWGDYNz5yAmBv7v/+DJJ81ZykVErpKKbhER8TpVqsDs2ebtF16ATz6xNo+IlAGZmTBkCERFwbZt5i+iOXPM4eQNG1qdTkRsTEW3iIh4pYQE6N37tzmLzp61OpGIlFrLlkHjxjBzpvlL54EHzInSevfWRGkics1UdIuIiNeaNg3Cw2HfPhg50uo0IlLq/PgjdOsGd90Fhw5BRASsWQPz55tLKYiIlAAV3SIi4rWuu84c3QnmcmLr1lmbR0RKidxcmDHDPLv93nvg5wejR8OOHdCxo9XpRKSUUdEtIiJerWNHeOQR83bv3nD6tLV5RMTmtm6FW2+FoUPhzBlo08bc9tRTUK6c1elEpBRS0S0iIl7vueegTh1zydyRI9V1iUgxnDkDTzwBrVrBV1+ZQ2lmzTJnarzpJqvTiUgppk8uIiLi9SpVgrlzzdv//rcvW7eGWBtIROzlgw/MNbenTYO8PLj3XvjmG3MYjY8+DouIe+m3jIiI2EK7dvDoo+btF19swalT1uYRERs4csQssP/6VzhwAGrXNgvwRYugRg2r04lIGaGiW0REbGPyZIiIMDhxohzDhvlaHUdEvFVeHrz8srm+9jvvgK8vDBsGX39trkcoIuJBKrpFRMQ2KlSA2bNzcTgM5s3z4YMPrE4kIl5nxw647TYYOBAyM+GWW8xruJ991vwlIiLiYSq6RUTEVm67zaBLl30A9OsHv/5qcSAR8Q7nzsGoUXDzzfDFF+ZkEDNnwoYN0Ly51elEpAxT0S0iIrZz//3fcOONBkeOwJAhVqcREcutWQNNm8KUKZCTA3fdBbt2weDB5tByERELqegWERHbCQzMY86cXHx8YMECWLbM6kQiYoljx+D++yEuDr77DmrVMn8hLFli3hYR8QIqukVExJaiogyGDTNvP/II/PyztXlExIPy8mD2bGjUCN5801z267HHzLPbXbtanU5ExIWKbhERsa0JE8yld48fN0eRikgZ8M030Lbtb5M6tGgBGzfC9OnmddwiIl5GRbeIiNhWYCC89pp5yeaiRebKQCJSSl24AOPGQWQkfPIJlC8Pzz8PmzZBq1ZWpxMRKZSKbhERsbWWLWH0aPP2gAHmJZ4iUro41q2DZs1g0iTIzobOnc2h5MnJ4OdndTwRkctS0S0iIrY3Zox58uvECbPwNgyrE4lIifj5Z1rMmIFfXBx8+y3UqGEOaVm+HGrXtjqdiEiRqOgWERHbCwgwh5n7+cHSpfDWW1YnKrtSUlKoU6cOQUFBREVFsWnTpsu2f+edd2jYsCFBQUE0bdqUlStXujzeq1cvHA6Hy1d8fLw7X4J4A8OA117Dr2lTbli3DsPhgIEDzeu5//d/weGwOqGISJF5RdF9NR10dnY2EydOJCIigqCgICIjI1m9erVLmzp16uTroB0OB4MGDXK2adu2bb7H+/fv77KfAwcO0LlzZ8qXL0+1atUYNmwYOTk5JfviRUSkRERGmpd7gjmp2uHD1uYpixYtWkRycjLjx49ny5YtREZGEhcXx/Hjxwts//nnn3PffffRp08ftm7dSrdu3ejWrRs7d+50aRcfH8+RI0ecX2/pryql27ffQocO0KsXjhMnOFW7Nrnr10NKClx3ndXpRESumuVF99V20GPGjOGVV15h5syZ7Nq1i/79+3PXXXexdetWZ5svv/zSpXNOS0sDoHv37i776tevn0u7Z5991vlYbm4unTt35uLFi3z++ee89tprzJs3j3GXPtGJiIjXGTnSvMb711/NZcQ0zNyzpk2bRr9+/UhKSqJx48bMmjWL8uXLM3fu3ALbz5gxg/j4eIYNG0ajRo2YNGkSN998My+++KJLu8DAQEJDQ51fVapU8cTLEU+7eBH++U9o2hQ++gjKlSP3qadY//zzGLfeanU6EZFis7zovtoOev78+YwePZqEhATq1avHgAEDSEhI4Pnnn3e2CQkJcemcV6xYQUREBDExMS77Kl++vEu7ypUrOx9bs2YNu3btYsGCBTRv3pxOnToxadIkUlJSuHjxonsOhoiIXBN/f3OYeUAArFhh3hbPuHjxIps3b6ZDhw7ObT4+PnTo0IENGzYU+JwNGza4tAeIi4vL1z4jI4Nq1arRoEEDBgwYwIkTJ0r+BYi1Pv0UmjeHsWMhKwtiY2HnTvKGDcPQRGkiYnOW/ha71EGPGjXKue1KHXRWVhZBQUEu28qVK8enn35a6PdYsGABycnJOP5w/c8bb7zBggULCA0NpUuXLowdO5by5csD5geBpk2bUr16dWf7uLg4BgwYwNdff02LFi2K9ZpFRMS9mjSBiRPNs96PPWaOUq1Vy+pUpd/PP/9Mbm6uS78JUL16dXbv3l3gc44ePVpg+6NHjzrvx8fHc/fdd1O3bl327dvH6NGj6dSpExs2bMDX1zffPrOyssjKynLez8zMBMzL07Kzs4v9+i7t4/f/2oVX5/71V3xHjcLnvydbjGrVyJ06FaNHD3A4vDv7Zdg1N9g3u11zg32zK3fR92Fp0V2cDjouLo5p06Zxxx13EBERQXp6OkuWLCE3N7fA9suWLePkyZP06tXLZfvf/vY3ateuTVhYGNu3b2fEiBHs2bOHJUuWAIV/ELj0WEHc1dHb9Q0N9s1u19xg3+zK7Xl2zV6U3EOGwJIlvmza5EOfPnksX57rFfMuldQxt9vP7Fr07NnTebtp06Y0a9aMiIgIMjIyaN++fb72kydPZsKECfm2r1mzxvmH9Wt16bI1u/Gq3IZBzU8+4aY5c/A/dQqA7zt2ZNdDD5FdqRKsWuXS3KuyXwW75gb7ZrdrbrBv9rKc+9y5c0VqZ7vxOjNmzKBfv340bNgQh8NBREQESUlJhQ5HnzNnDp06dSIsLMxl+8MPP+y83bRpU2rUqEH79u3Zt28fERERxcrm7o7erm9osG92u+YG+2ZXbs+za/Yr5U5MrMi2bW1Zs8aXxx/fTmzsDx5KdmXXesyL2sl7UtWqVfH19eXYHxZKP3bsGKGhoQU+JzQ09KraA9SrV4+qVauyd+/eAovuUaNGkZyc7LyfmZlJeHg4sbGxLpeRFUd2djZpaWl07NgRf3//a9qXJ3ld7u++w3fIEHzWrAHAaNiQ3JdeouZf/kLNPzT1uuxFZNfcYN/sds0N9s2u3L+dZL0SS4vu4nTQISEhLFu2jAsXLnDixAnCwsIYOXIk9erVy9f2hx9+YO3atc6z15cTFRUFwN69e4mIiCA0NDTfLOqXchaWzV0dvV3f0GDf7HbNDfbNrtyeZ9fsV5P77FkYPhxefz2Sxx9vYvmyviV1zIvayXtSQEAALVu2JD09nW7dugGQl5dHeno6gwcPLvA50dHRpKenM3ToUOe2tLQ0oqOjC/0+Bw8e5MSJE9SoUaPAxwMDAwkMDMy33d/fv8Te5yW5L0+yPHd2NkybBhMmwPnzEBgIY8bgGD4cv4CAyz7V8uzFZNfcYN/sds0N9s1elnMX9fmWFt3F6aAvCQoKombNmmRnZ7N48WLuvffefG1SU1OpVq0anTt3vmKWbdu2ATg78ejoaJ566imOHz9OtWrVAPODQOXKlWncuHGB+3B3R2/XNzTYN7tdc4N9syu359k1e1FyJyfD++/Dp586eOQRf9LSwMfyKUSv/Zh7688rOTmZxMREWrVqRevWrZk+fTpnz54lKSkJgIceeoiaNWsyefJkAB577DFiYmJ4/vnn6dy5MwsXLuSrr77i1VdfBeDMmTNMmDCBe+65h9DQUPbt28fw4cOpX78+cXFxlr1OKYYvvoCHH4YdO8z7d94JL78MN95obS4REQ+wfHj51XbQGzdu5NChQzRv3pxDhw7x5JNPkpeXx/Dhw132m5eXR2pqKomJifj9YdbLffv28eabb5KQkMCf/vQntm/fzuOPP84dd9xBs2bNAIiNjaVx48Y8+OCDPPvssxw9epQxY8YwaNCgAgtrERHxPr6+kJoKzZqZKxDNmgUDB1qdqvTq0aMHP/30E+PGjePo0aM0b96c1atXO+dEOXDgAD6/+6tHmzZtePPNNxkzZgyjR4/mz3/+M8uWLeOmm24CwNfXl+3bt/Paa69x8uRJwsLCiI2NZdKkSeqL7eLUKRg1yvzPZxjwpz+ZZ7sffBCvmGhBRMQDLC+6r7aDvnDhAmPGjOG7776jYsWKJCQkMH/+fIKDg132u3btWg4cOEDv3r3zfc+AgADWrl3rLPDDw8O55557GDNmjLONr68vK1asYMCAAURHR1OhQgUSExOZOHGiew6EiIi4Rf368Mwz5uRqw4ZBXBwUc+oOKYLBgwcXOlotIyMj37bu3bvTvXv3AtuXK1eODz/8sCTjiacYBixebP7HO3LE3JaYCFOnQtWq1mYTEfEwy4tuuLoOOiYmhl27dl1xn7GxsRiGUeBj4eHhrF+//or7qF27NitXrrxiOxER8W6DBsGSJZCRAUlJ5r/eMMxcpFT64QfzP90HH5j3//xneOUVaNfO2lwiIhbRRw4RESn1fHxg7lyoWBE++QT+9S+rE4mUQjk55tDxxo3NgtvfH8aOhe3bVXCLSJmmoltERMqEunXNka1gXmL6n/9Ym0ekVNm8GaKi4Ikn4Nw5uP12+L//g4kTISjI6nQiIpZS0S0iImXGww9Dx45w4QL06gW5uVYnErG506dh6FBo3Rq2bIHgYPj3v81rOBo1sjiciIh3UNEtIiJlhsMBc+ZA5cqwYYM5ElZEium998yh5DNmQF4e/O1vsHs39O2rSRNERH5HvxFFRKRMCQ+HF14wb48dC0WYm1NEfu/QIbj7bujWDQ4ehHr14MMP4Y034L+rz4iIyG9UdIuISJmTlAQJCZCVZa5ilJNjdSIRG8jNhZkzzWHjS5eCnx+MHAk7dkBsrNXpRES8lopuEREpcxwO87LT4GD46itzHW8RuYxt26BNG3Pd7dOnITravIZ78mQoX97qdCIiXk1Ft4iIlElhYeZJO4AJE8xVjUTkD86ehWHDoFUr2LTJnBDhpZfg00+haVOr04mI2IKKbhERKbPuvx+6doXsbHOY+cWLVicS8SIrV0KTJuZae7m50L27OVHagAGaKE1E5CroN6aIiJRZDge88gr86U/m6Nmnn7Y6kYgXOHIEevSAzp3hhx+gdm1YsQLefhtq1LA6nYiI7ajoFhGRMq16dUhJMW8/9ZR5mapImZSXB7NmmROlvf22eTb7iSfg66/NAlxERIpFRbeIiJR5PXqYI2dzcsxh5llZVicS8bCdO+H2282h46dOmddwf/WVObS8QgWr04mI2JqKbhEREcyz3SEhZu0xcaLVaUQ85Px5GD0aWrSAzz+HihVhxgz44gtzm4iIXDMV3SIiIpgF9yuvmLenTDEnahYp1dauNWcgnzzZHObRtSvs2mUuC+bra3U6EZFSQ0W3iIjIf911F/ztb+alrYmJcOGC1YlE3OD4cXjwQejYEfbtg5o1YelSWLYMwsOtTiciUuqo6BYREfmdmTMhNNRcGWnsWKvTiJQgw4C5c82J0hYsMKfvHzLEPLvdrZvV6URESi0V3SIiIr9z/fXw6qvm7eefNy9zFbG7igcP4tuhA/TpA7/8As2bw8aN5vXblStbHU9EpFRT0S0iIvIHXbqYw8sNA3r1gnPnrE4kUkwXLuAzcSJthw7F55NPoHx5c0byL7+EW26xOp2ISJmgoltERKQA06ebl7p++605ubOI7WRkQGQkvv/8J745OeR16mSuuf3EE+DnZ3U6EZEyQ0W3iIhIAYKDYfZs8/aMGbB+vaVxRIruxAno3RvatYP//AcjNJQvhw0jd9kyqFPH6nQiImWOim4REZFCxMdDv37m7aQkOHPG2jwil2UYMH8+NGwIqanmRGkDBpCzfTuHb7vNvC8iIh6noltEROQypk6FG26A/fth+HCr04gU4ttvzSXAHnoIfv4ZbroJPvsMXnrJHLYhIiKWUdEtIiJyGZUrm6ssAbz8Mqxda20eERcXL8JTT0HTppCeDkFB8PTTsGULREdbnU5ERFDRLSIickXt28PAgebtPn0gM9PaPCIAfPoptGgBY8ZAVpZ5pnvnThg1Cvz9rU4nIiL/paJbRESkCJ55BurVgwMHzMmfRSzz66/wyCNw++2waxeEhMCCBfDhhxARYXU6ERH5AxXdIiIiRVCx4m9zU82eDatWWZ1IyhzDgIULoVEjePVVc1ufPrB7N9x/vyZKExHxUiq6RUREiuiOO+Cxx8zbffuaJxxFPGL/fkhIgPvug2PHzBnK1683/wJ0/fVWpxMRkctQ0S0iInIVnnoK/vxnOHwYHn/c6jRS6mVnw7PPQpMmsHo1BATAhAmwbZv5VyAREfF6KrpFRESuQvny8Npr4ONj/vv++1YnklJr40Zo1QpGjIDz56FtW9i+HcaNg8BAq9OJiEgRqegWERG5StHRv02m9sgjcOKEtXmklMnMhMGDzTfa9u3m8PHUVPjoI2jQwOp0IiJylVR0i4iIFMPEieZ8VkePwqOPWp1GSgXDgCVLzDdWSop5/6GHzInSevXSRGkiIjaloltERKQYgoLM4eW+vvDWW7B4sdWJxNYOHICuXeGee8wJA+rXh7VrzTdZSIjV6URE5Bqo6BYRESmmW26BkSPN2wMGwE8/WZtHbCgnB6ZPh8aNYfly8PeHMWNgxw5o397qdCIiUgJUdIuIiFyDsWOhaVOz4B4wwBwRLFIkmzdDVJQ5Df7Zs3Dbbeas5JMmmUMpRESkVFDRLSIicg0CA80RwH5+5hDzRYusTiRe78wZs9Bu3Rq2bIHgYHj1Vfj4Y/OMt4iIlCoqukVERK5RixbmiGCAQYPMydVECrR8uVlYT58OeXnQsyd88w3062euQyciIqWOfruLiIiUgNGjzeL7l1/MZcQ0zFxcHDpkTpL2P/8DP/4IdevCqlXmLHyhoVanExERN/KKojslJYU6deoQFBREVFQUmzZtKrRtdnY2EydOJCIigqCgICIjI1m9erVLmzp16uBwOPJ9DRo0CIBffvmFRx99lAYNGlCuXDluuOEGhgwZwqlTp1z2U9A+Fi5cWPIHQEREbM/f3xxm7u8P778PCxZYnUi8Qm6uufxXo0bmcmC+vjB8OOzcCfHxVqcTEREPsLzoXrRoEcnJyYwfP54tW7YQGRlJXFwcx48fL7D9mDFjeOWVV5g5cya7du2if//+3HXXXWzdutXZ5ssvv+TIkSPOr7S0NAC6d+8OwOHDhzl8+DBTp05l586dzJs3j9WrV9OnT5983y81NdVlX926dSv5gyAiIqVC06bw5JPm7UcfNU9uShm2fbs5OdrgwXD6tDlp2pYt8MwzUL681elERMRDLC+6p02bRr9+/UhKSqJx48bMmjWL8uXLM3fu3ALbz58/n9GjR5OQkEC9evUYMGAACQkJPP/88842ISEhhIaGOr9WrFhBREQEMTExANx0000sXryYLl26EBERwZ133slTTz3F8uXLycnJcfl+wcHBLvsK0myiIiJyGcOHm0uJnTplXqarYeZl0LlzMGIE3HwzbNwIlSubZ7s/+wyaNbM6nYiIeJifld/84sWLbN68mVGjRjm3+fj40KFDBzZs2FDgc7KysvIVvuXKlePTTz8t9HssWLCA5ORkHA5HoVlOnTpF5cqV8fNzPSSDBg2ib9++1KtXj/79+5OUlFTofrKyssjKynLez8zMBMwh8dnZ2YV+7yu59Nxr2YdV7JrdrrnBvtmV2/Psmt0OuWfPhtat/Vi1ysG//51DUpJZeZdUdm9+7WXe6tXm2nHff2/ev+ce+Ne/ICzM0lgiImIdS4vun3/+mdzcXKpXr+6yvXr16uzevbvA58TFxTFt2jTuuOMOIiIiSE9PZ8mSJeTm5hbYftmyZZw8eZJevXpdNsekSZN4+OGHXbZPnDiRO++8k/Lly7NmzRoGDhzImTNnGDJkSIH7mTx5MhMmTMi3fc2aNZQvgWFkl4bJ25Fds9s1N9g3u3J7nl2ze3vu++6LYN68mxg61MDHZx0hIeedj11r9nPnzl1rPClpR4+ay4BdmvslPNw8u92li7W5RETEcpYW3cUxY8YM+vXrR8OGDXE4HERERJCUlFTocPQ5c+bQqVMnwgr5C3NmZiadO3emcePGPHnpQrz/Gjt2rPN2ixYtOHv2LM8991yhRfeoUaNITk522Xd4eDixsbFUrlz5Kl/pb7Kzs0lLS6Njx474+/sXez9WsGt2u+YG+2ZXbs+za3a75I6Lgz178tiwwZ+FCzuwfHku69fnkpa2k44db6JtW198fYu370sjqcSDcnNxrF9PzY8/xlGhArRrZ06KlpdnDm0YMQJOnjSX/XrsMZg4ESpWtDq1iIh4AUuL7qpVq+Lr68uxY8dcth87dozQQpbPCAkJYdmyZVy4cIETJ04QFhbGyJEjqVevXr62P/zwA2vXrmXJkiUF7uv06dPEx8dTqVIlli5desUPb1FRUUyaNImsrCwCAwPzPR4YGFjgdn9//xL5YFhS+7GCXbPbNTfYN7tye55ds3t7bn9/mDcPmjeH9HQfwsJ8OHnSH2jFtGlQqxbMmAF3312cfXvv6y6VliyBxx7D7+BBWgHOH+ATT8C775rXagO0bAmvvmpeyy0iIvJflk6kFhAQQMuWLUlPT3duy8vLIz09nejo6Ms+NygoiJo1a5KTk8PixYvp2rVrvjapqalUq1aNzp0753ssMzOT2NhYAgICeP/994s0Qdq2bduoUqVKgYW1iIjIH914I/Tsad4+edL1sUOH4H//16znxIstWWL+oA4edN1+8KA5nPyzz6BCBXjhBfjiCxXcIiKSj+XDy5OTk0lMTKRVq1a0bt2a6dOnc/bsWZKSkgB46KGHqFmzJpMnTwZg48aNHDp0iObNm3Po0CGefPJJ8vLyGD58uMt+8/LySE1NJTExMd/kaJcK7nPnzrFgwQIyMzOdQ/VCQkLw9fVl+fLlHDt2jFtvvZWgoCDS0tJ4+umn+fvf/+6BoyIiIqVBbi4Udvm2YYDDAUOHQteuFHuoubhRbq45VPxyU9AHBZlrbtep47FYIiJiL5YX3T169OCnn35i3LhxHD16lObNm7N69Wrn5GoHDhzAx+e3E/IXLlxgzJgxfPfdd1SsWJGEhATmz59PcHCwy37Xrl3LgQMH6N27d77vuWXLFjZu3AhA/fr1XR7bv38/derUwd/fn5SUFB5//HEMw6B+/frO5c1ERESK4pNP8p8g/T3DgB9/NNu1beuxWFJUV/oBAly4YM5UrqJbREQKYXnRDTB48GAGDx5c4GMZGRku92NiYti1a9cV9xkbG4tRyF+m27ZtW+hjl8THxxMfH3/F7yMiIlKYI0dKtp14mH6AIiJSAiy9pltERKQ0q1GjZNuJh+kHKCIiJUBFt4iIiJvcfrs5ybXDUfDjDoe5nPPtt3s2lxSRfoAiIlICVHSLiIi4ia+vuSwY5K/bLt2fPl2TqHkt/QBFRKQEqOgWERFxo7vvNpdyrlnTdXutWub24qzTLR6kH6CIiFwjr5hITUREpDS7+25zWbB163JYtWobnTo1p107P50gtYv//gBz1q1j26pVNO/UCb927XSGW0REikRFt4iIiAf4+kJMjMHZs4eIiYlUvWY3vr4YMTEcOnuWyJgYFdwiIlJkGl4uIiIiIiIi4iYqukVERERERETcREW3iIiIiIiIiJuo6BYRERERERFxExXdIiIiIiIiIm6ioltERERERETETVR0i4iIiIiIiLiJim4RERERERERN1HRLSIiIiIiIuImKrpFRERERERE3ERFt4iIiIiIiIib+FkdoDQzDAOAzMzMa9pPdnY2586dIzMzE39//5KI5jF2zW7X3GDf7MrteXbNbtfcUHLZL/Url/oZKVxJ9cVg3/eeXXODfbPbNTfYN7tdc4N9syt30ftjFd1udPr0aQDCw8MtTiIiIqXR6dOnue6666yO4dXUF4uIiLtdqT92GPozudvk5eVx+PBhKlWqhMPhKPZ+MjMzCQ8P58cff6Ry5colmND97JrdrrnBvtmV2/Psmt2uuaHkshuGwenTpwkLC8PHR1eKXU5J9cVg3/eeXXODfbPbNTfYN7tdc4N9syt30ftjnel2Ix8fH2rVqlVi+6tcubKt3tC/Z9fsds0N9s2u3J5n1+x2zQ0lk11nuIumpPtisO97z665wb7Z7Zob7JvdrrnBvtnLeu6i9Mf687iIiIiIiIiIm6joFhEREREREXETFd02EBgYyPjx4wkMDLQ6ylWza3a75gb7Zlduz7NrdrvmBntnF/v+/OyaG+yb3a65wb7Z7Zob7JtduYtOE6mJiIiIiIiIuInOdIuIiIiIiIi4iYpuERERERERETdR0S0iIiIiIiLiJiq6Pezjjz+mS5cuhIWF4XA4WLZs2RWfk5GRwc0330xgYCD169dn3rx5+dqkpKRQp04dgoKCiIqKYtOmTZZnX7JkCR07diQkJITKlSsTHR3Nhx9+6NLmySefxOFwuHw1bNjQ0twZGRn5MjkcDo4ePerSzhuPea9evQrM3qRJE2cbTxzzyZMnc8stt1CpUiWqVatGt27d2LNnzxWf984779CwYUOCgoJo2rQpK1eudHncMAzGjRtHjRo1KFeuHB06dODbb7+1NPe///1vbr/9dqpUqUKVKlXo0KFDvvdCQT+X+Pj4Estd3Ozz5s3LlysoKMiljTce87Zt2xb4Pu/cubOzjbuP+csvv0yzZs2ca3xGR0ezatWqyz7H6ve3uFJ/rP7YXbnVF1uT3Rv6Y7v2xcXNrv646FR0e9jZs2eJjIwkJSWlSO33799P586dadeuHdu2bWPo0KH07dvXpbNctGgRycnJjB8/ni1bthAZGUlcXBzHjx+3NPvHH39Mx44dWblyJZs3b6Zdu3Z06dKFrVu3urRr0qQJR44ccX59+umnlua+ZM+ePS65qlWr5nzMW4/5jBkzXDL/+OOPXH/99XTv3t2lnbuP+fr16xk0aBBffPEFaWlpZGdnExsby9mzZwt9zueff859991Hnz592Lp1K926daNbt27s3LnT2ebZZ5/lX//6F7NmzWLjxo1UqFCBuLg4Lly4YFnujIwM7rvvPtatW8eGDRsIDw8nNjaWQ4cOubSLj493OeZvvfVWiWS+luwAlStXdsn1ww8/uDzujcd8yZIlLpl37tyJr69vvve5O495rVq1mDJlCps3b+arr77izjvvpGvXrnz99dcFtveG97e4Un+s/thdudUXW5PdG/pju/bFxc2u/vgqGGIZwFi6dOll2wwfPtxo0qSJy7YePXoYcXFxzvutW7c2Bg0a5Lyfm5trhIWFGZMnTy7RvL9XlOwFady4sTFhwgTn/fHjxxuRkZElF+wKipJ73bp1BmD8+uuvhbaxyzFfunSp4XA4jO+//965zdPH3DAM4/jx4wZgrF+/vtA29957r9G5c2eXbVFRUcYjjzxiGIZh5OXlGaGhocZzzz3nfPzkyZNGYGCg8dZbb1mW+49ycnKMSpUqGa+99ppzW2JiotG1a1c3JCxcUbKnpqYa1113XaGP2+WYv/DCC0alSpWMM2fOOLdZccyrVKlizJ49u8DHvPH9Lb9Rf6z+uKjUF1vzu8qu/bFd+2LDUH9c0sdbZ7q93IYNG+jQoYPLtri4ODZs2ADAxYsX2bx5s0sbHx8fOnTo4GzjLfLy8jh9+jTXX3+9y/Zvv/2WsLAw6tWrx/3338+BAwcsSuiqefPm1KhRg44dO/LZZ585t9vpmM+ZM4cOHTpQu3Ztl+2ePuanTp0CyPez/70rvdf379/P0aNHXdpcd911REVFue24FyX3H507d47s7Ox8z8nIyKBatWo0aNCAAQMGcOLEiRLN+kdFzX7mzBlq165NeHh4vr8M2+WYz5kzh549e1KhQgWX7Z465rm5uSxcuJCzZ88SHR1dYBtvfH/L1VF/bB2798fqi6+dXftju/bFoP64pI+3im4vd/ToUapXr+6yrXr16mRmZnL+/Hl+/vlncnNzC2zzx2uerDZ16lTOnDnDvffe69wWFRXFvHnzWL16NS+//DL79+/n9ttv5/Tp05blrFGjBrNmzWLx4sUsXryY8PBw2rZty5YtWwBsc8wPHz7MqlWr6Nu3r8t2Tx/zvLw8hg4dym233cZNN91UaLvC3uuXjumlfz113Iua+49GjBhBWFiYyy/r+Ph4Xn/9ddLT03nmmWdYv349nTp1Ijc3t8RzQ9GzN2jQgLlz5/Lee++xYMEC8vLyaNOmDQcPHgTsccw3bdrEzp07873PPXHMd+zYQcWKFQkMDKR///4sXbqUxo0bF9jW297fcvXUH3teaeiP1RdfO7v2x3bti68m+++pP748v2I/U+QqvPnmm0yYMIH33nvP5VqsTp06OW83a9aMqKgoateuzdtvv02fPn2siEqDBg1o0KCB836bNm3Yt28fL7zwAvPnz7ckU3G89tprBAcH061bN5ftnj7mgwYNYufOnSV+rZq7FSf3lClTWLhwIRkZGS6ToPTs2dN5u2nTpjRr1oyIiAgyMjJo3759ieaGomePjo52+UtwmzZtaNSoEa+88gqTJk0q8VxXUpxjPmfOHJo2bUrr1q1dtnvimDdo0IBt27Zx6tQp3n33XRITE1m/fn2hHb2IN1B/7Fnqi6+dXftju/bFoP7YHXSm28uFhoZy7Ngxl23Hjh2jcuXKlCtXjqpVq+Lr61tgm9DQUE9GLdTChQvp27cvb7/9dr7hHH8UHBzMjTfeyN69ez2Urmhat27tzGSHY24YBnPnzuXBBx8kICDgsm3decwHDx7MihUrWLduHbVq1bps28Le65eO6aV/PXHcryb3JVOnTmXKlCmsWbOGZs2aXbZtvXr1qFq1quXH/I/8/f1p0aKFM5e3H/OzZ8+ycOHCIn1AdccxDwgIoH79+rRs2ZLJkycTGRnJjBkzCmzrTe9vKR71x97BTv2x+uJrZ9f+2K59Mag/dtfxVtHt5aKjo0lPT3fZlpaW5vyLWEBAAC1btnRpk5eXR3p6eqHXMnjSW2+9RVJSEm+99ZbL8gGFOXPmDPv27aNGjRoeSFd027Ztc2by9mMO5gyUe/fuLdIvP3ccc8MwGDx4MEuXLuWjjz6ibt26V3zOld7rdevWJTQ01KVNZmYmGzduLLHjXpzcYM5yOWnSJFavXk2rVq2u2P7gwYOcOHHC8mP+R7m5uezYscOZy5uPOZhLfmRlZfHAAw9csa07jvkf5eXlkZWVVeBj3vD+lmuj/tg72Kk/Vl/s2exgfX9s1774WrOrPy6CYk/BJsVy+vRpY+vWrcbWrVsNwJg2bZqxdetW44cffjAMwzBGjhxpPPjgg8723333nVG+fHlj2LBhxjfffGOkpKQYvr6+xurVq51tFi5caAQGBhrz5s0zdu3aZTz88MNGcHCwcfToUUuzv/HGG4afn5+RkpJiHDlyxPl18uRJZ5snnnjCyMjIMPbv32989tlnRocOHYyqVasax48ftyz3Cy+8YCxbtsz49ttvjR07dhiPPfaY4ePjY6xdu9bZxluP+SUPPPCAERUVVeA+PXHMBwwYYFx33XVGRkaGy8/+3LlzzjYPPvigMXLkSOf9zz77zPDz8zOmTp1qfPPNN8b48eMNf39/Y8eOHc42U6ZMMYKDg4333nvP2L59u9G1a1ejbt26xvnz5y3LPWXKFCMgIMB49913XZ5z+vRpwzDMn+Hf//53Y8OGDcb+/fuNtWvXGjfffLPx5z//2bhw4UKJ5C5u9gkTJhgffvihsW/fPmPz5s1Gz549jaCgIOPrr792eX3edswv+ctf/mL06NEj33ZPHPORI0ca69evN/bv329s377dGDlypOFwOIw1a9YUmNkb3t/iSv2x+mN35b5EfbFns3tDf2zXvri42S9Rf3xlKro97NLyF3/8SkxMNAzDnFY/JiYm33OaN29uBAQEGPXq1TNSU1Pz7XfmzJnGDTfcYAQEBBitW7c2vvjiC8uzx8TEXLa9YZjLrdSoUcMICAgwatasafTo0cPYu3evpbmfeeYZIyIiwggKCjKuv/56o23btsZHH32Ub7/eeMwNw1zWoFy5csarr75a4D49ccwLygy4vHdjYmJc3guGYRhvv/22ceONNxoBAQFGkyZNjA8++MDl8by8PGPs2LFG9erVjcDAQKN9+/bGnj17LM1du3btAp8zfvx4wzAM49y5c0ZsbKwREhJi+Pv7G7Vr1zb69etX4h/Ci5N96NChzvdw9erVjYSEBGPLli0u+/XGY24YhrF7924DcHaqv+eJY967d2+jdu3aRkBAgBESEmK0b9/eJYs3vr/Flfpj9cfuym0Y6outyO4N/bFd++LiZjcM9cdF5TAMw0BERERERERESpyu6RYRERERERFxExXdIiIiIiIiIm6ioltERERERETETVR0i4iIiIiIiLiJim4RERERERERN1HRLSIiIiIiIuImKrpFRERERERE3ERFt4iIiIiIiIibqOgWEa/Ttm1bhg4detk2derUYfr06R7JIyIiUhapPxYpGSq6RcQtevXqhcPhyPe1d+9eq6OJiIiUGeqPRaznZ3UAESm94uPjSU1NddkWEhJiURoREZGySf2xiLV0pltE3CYwMJDQ0FCXL19fX9avX0/r1q0JDAykRo0ajBw5kpycnEL3c/z4cbp06UK5cuWoW7cub7zxhgdfhYiIiL2pPxaxls50i4hHHTp0iISEBHr16sXrr7/O7t276devH0FBQTz55JMFPqdXr14cPnyYdevW4e/vz5AhQzh+/Lhng4uIiJQi6o9FPEdFt4i4zYoVK6hYsaLzfqdOnbjxxhsJDw/nxRdfxOFw0LBhQw4fPsyIESMYN24cPj6uA3D+85//sGrVKjZt2sQtt9wCwJw5c2jUqJFHX4uIiIhdqT8WsZaKbhFxm3bt2vHyyy8771eoUIFBgwYRHR2Nw+Fwbr/ttts4c+YMBw8e5IYbbnDZxzfffIOfnx8tW7Z0bmvYsCHBwcFuzy8iIlIaqD8WsZaKbhFxmwoVKlC/fn2rY4iIiJRp6o9FrKWJ1ETEoxo1asSGDRswDMO57bPPPqNSpUrUqlUrX/uGDRuSk5PD5s2bndv27NnDyZMnPRFXRESkVFJ/LOI5KrpFxKMGDhzIjz/+yKOPPsru3bt57733GD9+PMnJyfmuHwNo0KAB8fHxPPLII2zcuJHNmzfTt29fypUrZ0F6ERGR0kH9sYjnqOgWEY+qWbMmK1euZNOmTURGRtK/f3/69OnDmDFjCn1OamoqYWFhxMTEcPfdd/Pwww9TrVo1D6YWEREpXdQfi3iOw/j9mBIRERERERERKTE60y0iIiIiIiLiJiq6RURERERERNxERbeIiIiIiIiIm6joFhEREREREXETFd0iIiIiIiIibqKiW0RERERERMRNVHSLiIiIiIiIuImKbhERERERERE3UdEtIiIiIiIi4iYqukVERERERETcREW3iIiIiIiIiJuo6BYRERERERFxk/8HK8OO+rt/TRQAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x400 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAcIAAAGJCAYAAAAZhzPqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAArhklEQVR4nO3de3zP9f//8ft7s703Zg4ZM4fNKYeIUEgZOZ8lJJ+yOSQ6YA6hPuVUFMoxoZySpKilVMgxRXwwyvkY5TBzms0ObK/fH37eX28zNt7bWz1v18tll0vv5+v5er4er/em+56v1+v5ns2yLEsAABjKw90FAADgTgQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIe55+/fvV6NGjZQnTx7ZbDZFRka6dPwjR47IZrNpzpw5Lh33n6xu3bqqW7euy8aLi4tT9+7dFRgYKJvNpr59+7ps7KwWEhKi8PDw2/abM2eObDabjhw5kuU1wbUIQmTIwYMH9cILL6hkyZLy8fGRv7+/ateurYkTJyohISFLjx0WFqbff/9db7/9tubNm6fq1atn6fGyU3h4uGw2m/z9/W/6Pu7fv182m002m03jxo3L9PjHjx/XsGHDFBUV5YJq79yoUaM0Z84c9erVS/PmzdNzzz2XpccLCQlxvG83fiUmJmbpsW9n3bp1atWqlYoVKyYfHx8FBgaqSZMm+uWXX9xal8lyuLsA3PuWLl2q9u3by263q3PnzqpYsaKSk5O1fv16DRw4UDt37tSMGTOy5NgJCQnasGGDXn/9db388stZcozg4GAlJCTIy8srS8a/nRw5cujSpUv69ttv1aFDB6dt8+fPl4+Pzx3/z/v48eMaPny4QkJCVKVKlQzvt3z58js6XnpWrVqlmjVraujQoS4d91aqVKmi/v37p2n39vbOthpuZt++ffLw8FDPnj0VGBioc+fO6dNPP1WdOnW0dOlSNWnSxK31mYggxC0dPnxYHTt2VHBwsFatWqXChQs7tr300ks6cOCAli5dmmXHP336tCQpb968WXYMm80mHx+fLBv/dux2u2rXrq0FCxakCcLPPvtMzZs31+LFi7OllkuXLilnzpwuD4vo6GhVqFDBZeNduXJFqampt6yzSJEievbZZ112TFfp3r27unfv7tT24osvqmTJkpowYQJB6AZcGsUtjRkzRnFxcZo5c6ZTCF5TunRp9enTx/H6ypUrGjlypEqVKiW73a6QkBC99tprSkpKctovJCRELVq00Pr16/XII4/Ix8dHJUuW1CeffOLoM2zYMAUHB0uSBg4cKJvNppCQEElXLyle++/rDRs2TDabzaltxYoVeuyxx5Q3b175+fmpbNmyeu211xzb07tHuGrVKj3++OPKlSuX8ubNq9atW2v37t03Pd6BAwcUHh6uvHnzKk+ePOrSpYsuXbqU/ht7g06dOumHH37Q+fPnHW2bN2/W/v371alTpzT9z549qwEDBqhSpUry8/OTv7+/mjZtqu3btzv6rFmzRg8//LAkqUuXLo5Lg9fOs27duqpYsaK2bNmiOnXqKGfOnI735cZ7hGFhYfLx8Ulz/o0bN1a+fPl0/Pjxm57XmjVrZLPZdPjwYS1dutRRw7X7aNHR0erWrZsKFSokHx8fVa5cWXPnznUa49r3Z9y4cZowYYLjZ2vXrl0Zem/TEx8fr/79+6tYsWKy2+0qW7asxo0bp4z8QZ6dO3fqiSeekK+vr4oWLaq33npLqampd1xLzpw5FRAQ4PT9R/ZhRohb+vbbb1WyZEk9+uijGerfvXt3zZ07V+3atVP//v3122+/afTo0dq9e7e+/vprp74HDhxQu3bt1K1bN4WFhWnWrFkKDw9XtWrV9MADD6ht27bKmzevIiIi9Mwzz6hZs2by8/PLVP07d+5UixYt9OCDD2rEiBGy2+06cODAbe/H/PTTT2ratKlKliypYcOGKSEhQZMnT1bt2rW1devWNCHcoUMHlShRQqNHj9bWrVv18ccfq2DBgnr33XczVGfbtm3Vs2dPffXVV+rataukq7PBcuXKqWrVqmn6Hzp0SJGRkWrfvr1KlCihU6dOafr06QoNDdWuXbsUFBSk8uXLa8SIEXrzzTfVo0cPPf7445Lk9L08c+aMmjZtqo4dO+rZZ59VoUKFblrfxIkTtWrVKoWFhWnDhg3y9PTU9OnTtXz5cs2bN09BQUE33a98+fKaN2+eIiIiVLRoUcelyoCAACUkJKhu3bo6cOCAXn75ZZUoUUJffvmlwsPDdf78eadfsCRp9uzZSkxMVI8ePWS325U/f/5bvqeXL19WTEyMU1vOnDmVM2dOWZalVq1aafXq1erWrZuqVKmiZcuWaeDAgfr77781fvz4dMc9efKk6tWrpytXrmjw4MHKlSuXZsyYIV9f31vWc6PY2FglJycrJiZGn3zyif744w+nX9CQjSwgHRcuXLAkWa1bt85Q/6ioKEuS1b17d6f2AQMGWJKsVatWOdqCg4MtSda6descbdHR0Zbdbrf69+/vaDt8+LAlyRo7dqzTmGFhYVZwcHCaGoYOHWpd/2M9fvx4S5J1+vTpdOu+dozZs2c72qpUqWIVLFjQOnPmjKNt+/btloeHh9W5c+c0x+vatavTmE8++aR13333pXvM688jV65clmVZVrt27az69etblmVZKSkpVmBgoDV8+PCbvgeJiYlWSkpKmvOw2+3WiBEjHG2bN29Oc27XhIaGWpKsadOm3XRbaGioU9uyZcssSdZbb71lHTp0yPLz87PatGlz23O0rKvf7+bNmzu1TZgwwZJkffrpp4625ORkq1atWpafn58VGxvrOC9Jlr+/vxUdHZ3h40lK8zV06FDLsiwrMjLScS7Xa9eunWWz2awDBw44jRUWFuZ43bdvX0uS9dtvvznaoqOjrTx58liSrMOHD2eoxsaNGzvq8vb2tl544QUrISEhQ/vCtbg0inTFxsZKknLnzp2h/t9//70kqV+/fk7t12YBN95LrFChgmOWIl2dJZQtW1aHDh2645pvdO3e4jfffJPhS1cnTpxQVFSUwsPDnWYdDz74oBo2bOg4z+v17NnT6fXjjz+uM2fOON7DjOjUqZPWrFmjkydPatWqVTp58uRNL4tKV+8renhc/eebkpKiM2fOOC77bt26NcPHtNvt6tKlS4b6NmrUSC+88IJGjBihtm3bysfHR9OnT8/wsW70/fffKzAwUM8884yjzcvLS71791ZcXJzWrl3r1P+pp55SQEBAhsevUaOGVqxY4fTVuXNnx7E9PT3Vu3dvp3369+8vy7L0ww8/3LLumjVr6pFHHnG0BQQE6D//+U+Ga5Okd955R8uXL9fMmTNVs2ZNJScn68qVK5kaA65BECJd/v7+kqSLFy9mqP+ff/4pDw8PlS5d2qk9MDBQefPm1Z9//unUXrx48TRj5MuXT+fOnbvDitN6+umnVbt2bXXv3l2FChVSx44d9cUXX9wyFK/VWbZs2TTbypcvr5iYGMXHxzu133gu+fLlk6RMnUuzZs2UO3duLVy4UPPnz9fDDz+c5r28JjU1VePHj1eZMmVkt9tVoEABBQQEaMeOHbpw4UKGj1mkSJFMPRgzbtw45c+fX1FRUZo0aZIKFiyY4X1v9Oeff6pMmTKOQL+mfPnyju3XK1GiRKbGL1CggBo0aOD0VbJkScfYQUFBaX7JS+/YN6v7Rjf7ebmVKlWqqGHDhuratatWrFihTZs2ZWi9IlyPIES6/P39FRQUpD/++CNT+934sEp6PD09b9puZeBhhfSOkZKS4vTa19dX69at008//aTnnntOO3bs0NNPP62GDRum6Xs37uZcrrHb7Wrbtq3mzp2rr7/+Ot3ZoHR1XV6/fv1Up04dffrpp1q2bJlWrFihBx54IFMPbWT2vta2bdsUHR0tSfr9998zte/dymyt/yTe3t5q1aqVvvrqqyxfl4u0CELcUosWLXTw4EFt2LDhtn2Dg4OVmpqq/fv3O7WfOnVK58+fdzwB6gr58uW76RN2N/tN3sPDQ/Xr19f777+vXbt26e2339aqVau0evXqm459rc69e/em2bZnzx4VKFBAuXLlursTSEenTp20bds2Xbx4UR07dky336JFi1SvXj3NnDlTHTt2VKNGjdSgQYM070lGfynJiPj4eHXp0kUVKlRQjx49NGbMGG3evPmOxwsODtb+/fvTBPeePXsc27NKcHCwjh8/nuZqR0aOfa3uG93s5yUzEhISZFlWhq/AwHUIQtzSq6++qly5cql79+46depUmu0HDx7UxIkTJV29tCdJEyZMcOrz/vvvS5KaN2/usrpKlSqlCxcuaMeOHY62EydOpHky9ezZs2n2vbaw/MYlHdcULlxYVapU0dy5c52C5Y8//tDy5csd55kV6tWrp5EjR2rKlCkKDAxMt5+np2ea2eaXX36pv//+26ntWmC74rH8QYMG6ejRo5o7d67ef/99hYSEKCwsLN338XaaNWumkydPauHChY62K1euaPLkyfLz81NoaOhd13yrY6ekpGjKlClO7ePHj5fNZlPTpk1vue/GjRu1adMmR9vp06c1f/78DB372oz6eufPn9fixYtVrFixu7rcjDvD8gncUqlSpfTZZ5/p6aefVvny5Z0+WebXX391PO4uSZUrV1ZYWJhmzJih8+fPKzQ0VJs2bdLcuXPVpk0b1atXz2V1dezYUYMGDdKTTz6p3r1769KlS/rwww91//33Oz0sMmLECK1bt07NmzdXcHCwoqOjNXXqVBUtWlSPPfZYuuOPHTtWTZs2Va1atdStWzfH8ok8efJo2LBhLjuPG3l4eOi///3vbfu1aNFCI0aMUJcuXfToo4/q999/1/z58x33wK4pVaqU8ubNq2nTpil37tzKlSuXatSoken7batWrdLUqVM1dOhQx3KO2bNnq27dunrjjTc0ZsyYTI0nST169ND06dMVHh6uLVu2KCQkRIsWLdIvv/yiCRMmZPghrTvRsmVL1atXT6+//rqOHDmiypUra/ny5frmm2/Ut29flSpVKt19X331Vc2bN09NmjRRnz59HMsngoODnX4xS0/Tpk1VtGhR1ahRQwULFtTRo0c1e/ZsHT9+3OmXAmQjtz6zin+Mffv2Wc8//7wVEhJieXt7W7lz57Zq165tTZ482UpMTHT0u3z5sjV8+HCrRIkSlpeXl1WsWDFryJAhTn0s6+aP01tW2sf201s+YVmWtXz5cqtixYqWt7e3VbZsWevTTz9Ns3xi5cqVVuvWra2goCDL29vbCgoKsp555hlr3759aY5x4xKDn376yapdu7bl6+tr+fv7Wy1btrR27drl1Ofa8W5cnjF79uwMPUp//fKJ9KS3fKJ///5W4cKFLV9fX6t27drWhg0bbrrs4ZtvvrEqVKhg5ciRw+k8Q0NDrQceeOCmx7x+nNjYWCs4ONiqWrWqdfnyZad+ERERloeHh7Vhw4ZbnkN63+9Tp05ZXbp0sQoUKGB5e3tblSpVSvN9uNXPQGaPd72LFy9aERERVlBQkOXl5WWVKVPGGjt2rJWamppmrOuXT1iWZe3YscMKDQ21fHx8rCJFilgjR460Zs6cmaHv+ZQpU6zHHnvMKlCggJUjRw4rICDAatmypdNSImQvm2Vl4m4+AAD/MtwjBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAY7V/5yTK+tQa7uwTA7WLWjnZ3CYDb5fK+/eftMiMEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABgth7sLwD/D80/W0PNtayq4cD5J0u5DpzRq1kot37hPklSiSH6980pz1XowWHbvHFqxcZ/6vbdE0efiHGPs+WqQY/9r3pj6g8bNW5t9JwK42Jb/bdYnc2Zq966dijl9Wu9NmKJ69Rs4tg99fbC+XRLptE+t2o/pg2kfZ3OlSA9BiAz5+3Ss3pj6ow4ci5HNZtOzzarqyzGdVTNskv48cU7fTeim3w+cUNNXPpIkDX2+kRaPC1Od7lNlWZZjnOEzlmv2N5scry9eSsr2cwFcKTEhQfffX06tn3xKA/q+ctM+j9Z+XMPeGuV47e3lnV3lIQMIQmTI9+t3O70eNn25nm9bU49ULK6ggDwKLpxPNcMmOYKt+8gvdGL5UNWtXkqrNx9w7Bd3KUmnzsYJ+Leo/Xgd1X68zi37eHt7q0CBgGyqCJnFPUJkmoeHTe0bPKhcPt767fejsnvnkGVZSrp8xdEnMfmKUlMtPfpgiNO+/Z+rq79+fEMb5vZWxH/qyNOTH0H8+/3vf5tUP/RRPdmyiUaNHKbz58+5uyRcx60zwpiYGM2aNUsbNmzQyZMnJUmBgYF69NFHFR4eroAAfoO6lzxQqpDWzHhRPt45FJeQrKcHz9OeI9GKOR+v+MTLevulpnrzw2Wy2aS3XmyqHDk8FVggt2P/qV/8om17j+tc7CXVfDBYI3o2UeB9uTVo0lI3nhWQtR597HE90aCRgooU0V/HjmnKpPF6pVcPzfn0c3l6erq7PEiyWdffwMlGmzdvVuPGjZUzZ041aNBAhQoVkiSdOnVKK1eu1KVLl7Rs2TJVr179luMkJSUpKcn5PlPBhiNk8+Cqr6t55fBUscC8ypPLR08+UVHhLR9WoxdnaM+RaNV/pIwmDWyjkKB8Sk219MWK7SpXoqD+t+sv9RkbedPxOreorimDnlSBJ95U8uWU7D0ZA8SsHe3uEoxTtVK5NA/L3OivY8fUqllDffjRbNWoWSsbqzNTLm/bbfu4LS1eeeUVtW/fXtOmTZPN5lyoZVnq2bOnXnnlFW3YsOGW44wePVrDhw93avMsUltexR5zec2mu3wlRYf+OiNJ2rb3b1UrX1QvPV1br7z7tVZu2q8H2o/VfXly6kpKqi7EJerwd6/ryPEd6Y63eedReeXwVHDhfNp/NCa7TgNwq6LFiilvvnw6dvRPgvAe4bYbNNu3b1dERESaEJQkm82miIgIRUVF3XacIUOG6MKFC05fOYrUzIKKcSMPm4fsXs6/S525cEkX4hIVWq2UCubLpe9+3pXu/pXLBCklJVWnz8VndanAPePUyZO6cP68AgIKursU/H9umxEGBgZq06ZNKleu3E23b9q0yXG59FbsdrvsdrtTG5dFXW9Er8ZatmGfjp08r9y5vPV0oyqqU7WEWvadJUl6rnk17T0SrdPn41WjYnGNi2ipyZ//4pjp1ahYXA8/UExrtxzSxUtJqlmxuN7t00ILlm3T+YsJ7jw14K5cuhSvY0ePOl7//fdf2rtnt/zz5FGePHk0/cMPVL9BIxUoUEDHjh3TxPfHqljx4qpVm6tW9wq3JcaAAQPUo0cPbdmyRfXr109zj/Cjjz7SuHHj3FUebhCQz08z3+ygwPty60Jcov44eEIt+87Sqv+/NOL+4gEa0auJ8vv76s8T5zRmzmpN+ny9Y/+k5Ctq36CyXu/WQHbvHDpy/KwmL1yvSQt+dtcpAS6xa+cf6tE1zPH6/bHvSJJatmqjIW8M0/59e/XdkkhdjL2ogIIBqlmrtl58uY+8vVlLeK9w28MykrRw4UKNHz9eW7ZsUUrK1YclPD09Va1aNfXr108dOnS4o3F9aw12ZZnAPxIPywAZe1jGrUF4zeXLlxUTc/USWoECBeTl5XVX4xGEAEEISPf4U6PX8/LyUuHChd1dBgDAQHysBwDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGgEIQDAaAQhAMBoBCEAwGg5MtJpyZIlGR6wVatWd1wMAADZLUNB2KZNmwwNZrPZlJKScjf1AACQrTIUhKmpqVldBwAAbsE9QgCA0TI0I7xRfHy81q5dq6NHjyo5OdlpW+/evV1SGAAA2SHTQbht2zY1a9ZMly5dUnx8vPLnz6+YmBjlzJlTBQsWJAgBAP8omb40GhERoZYtW+rcuXPy9fXVxo0b9eeff6patWoaN25cVtQIAECWyXQQRkVFqX///vLw8JCnp6eSkpJUrFgxjRkzRq+99lpW1AgAQJbJdBB6eXnJw+PqbgULFtTRo0clSXny5NGxY8dcWx0AAFks0/cIH3roIW3evFllypRRaGio3nzzTcXExGjevHmqWLFiVtQIAECWyfSMcNSoUSpcuLAk6e2331a+fPnUq1cvnT59WjNmzHB5gQAAZCWbZVmWu4twNd9ag91dAuB2MWtHu7sEwO1yedtu24cF9QAAo2X6HmGJEiVks6WfsIcOHbqrggAAyE6ZDsK+ffs6vb58+bK2bdumH3/8UQMHDnRVXQAAZItMB2GfPn1u2v7BBx/of//7310XBABAdnLZPcKmTZtq8eLFrhoOAIBs4bIgXLRokfLnz++q4QAAyBZ3tKD++odlLMvSyZMndfr0aU2dOtWlxQEAkNUyvY5w2LBhTkHo4eGhgIAA1a1bV+XKlXN5gXci8Yq7KwDcL9/DL7u7BMDtErZNuW2ff+WCeoIQIAgBKWNBmOl7hJ6enoqOjk7TfubMGXl6emZ2OAAA3CrTQZjeBDIpKUne3t53XRAAANkpww/LTJo0SZJks9n08ccfy8/Pz7EtJSVF69atu2fuEQIAkFEZDsLx48dLujojnDZtmtNlUG9vb4WEhGjatGmurxAAgCyU4SA8fPiwJKlevXr66quvlC9fviwrCgCA7JLpdYSrV6/OijoAAHCLTD8s89RTT+ndd99N0z5mzBi1b9/eJUUBAJBdMh2E69atU7NmzdK0N23aVOvWrXNJUQAAZJdMB2FcXNxNl0l4eXkpNjbWJUUBAJBdMh2ElSpV0sKFC9O0f/7556pQoYJLigIAILtk+mGZN954Q23bttXBgwf1xBNPSJJWrlypzz77TIsWLXJ5gQAAZKVMB2HLli0VGRmpUaNGadGiRfL19VXlypW1atUq/gwTAOAf564/dDs2NlYLFizQzJkztWXLFqWkpLiqtjvGh24DfOg2IGXRh25fs27dOoWFhSkoKEjvvfeennjiCW3cuPFOhwMAwC0ydWn05MmTmjNnjmbOnKnY2Fh16NBBSUlJioyM5EEZAMA/UoZnhC1btlTZsmW1Y8cOTZgwQcePH9fkyZOzsjYAALJchmeEP/zwg3r37q1evXqpTJkyWVkTAADZJsMzwvXr1+vixYuqVq2aatSooSlTpigmJiYrawMAIMtlOAhr1qypjz76SCdOnNALL7ygzz//XEFBQUpNTdWKFSt08eLFrKwTAIAscVfLJ/bu3auZM2dq3rx5On/+vBo2bKglS5a4sr47wvIJgOUTgJTFyyckqWzZshozZoz++usvLViw4G6GAgDALe56Qf29iBkhwIwQkLJhRggAwD8dQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCAAwGkEIADAaQQgAMBpBCJdJSUnRlEkT1LTRE3qk6oNq3qSBpn/4gSzLcndpgMsN6NJQCdumaOyApxxtdu8cGj+4g/5a/a5O//KeFozrroL5czvtV/eR+7V6Tj9Frx+nwytG6a3ereXpyf+K3Yl3Hy4ze+ZH+nLhAg15/U19/e336hsxQHNmfazP5s9zd2mAS1WrUFzdnqqtHfv+cmofM+ApNa9TUf95daYadZ+gwgF59Pl73R3bK91fRJGTe2n5r7tU85l39NzgWWoeWklv9W6d3aeA6xCEcJmoqG2q+0R91QmtqyJFiqph4yaq9ehj+uP3He4uDXCZXL7emj0qXC+OXKDzsQmOdn8/H4W3qaVB73+ltZv3advuY+ox9FPVqlJKj1QKkSS1a1RVf+w/rtEzftShYzFav+WAXp8YqRc6PC6/nHY3nREIQrhMlSoPadPGjTpy5LAkae+ePdq2bYsee7yOmysDXGfCkKf1489/aPVve53aHypfXN5eObRq4/+17ztySkdPnFWNB0tIunrpNDHpstN+CUmX5evjrYfKF8/64nFTOdxdAP49unbvobi4OLVp0VSenp5KSUnRK30i1LxFK3eXBrhE+8bVVKVcMT327Jg02wLv81dS8mVdiEtwao8+E6tC9/lLklb8ulsvd6qnDk2qadHyrQq8z1+v9WgqSSoc4J/1J4CbuqdnhMeOHVPXrl1v2ScpKUmxsbFOX0lJSdlUIa637Mcf9P3SbzV6zHv6/MuvNHLUO5o7e5aWRH7t7tKAu1a0UF6NHfiUurw+R0nJV+5ojJUb9+i1CZGa9FpHXfhtgnZ886aWrd8pSUpN5aEyd7mng/Ds2bOaO3fuLfuMHj1aefLkcfoa++7obKoQ1xv/3hh17dZDTZs1V5n7y6plqzZ6tnOYZn483d2lAXftofLFVeg+f234bJAubp6oi5snqk71MnrxmVBd3DxRp87Gyu7tpTx+vk77FbzPX6fOxDpeT/p0lQLrDNT9zd5U0XqD9e2aq/fQD/8Vk63ng//j1kujS5YsueX2Q4cO3XaMIUOGqF+/fk5tlic3nd0hMSFRHh42pzZPT09+08W/wupNe1Wt3dtObTOGP6u9h0/pvTkr9Nepc0q+fEX1apRV5MooSVKZ4IIqXji/fttxOM14J05fkCR1aFJdx06c1bY9x7L8HHBzbg3CNm3ayGaz3XKdmc1mS3ebJNntdtntzsGXeGdXLXCXQuvW00czpimwcJBKlS6tPbt3a97c2Wr95FO33xm4x8VdStKugyec2uITknX2QryjfU7kBr3bv63OXojXxfhEvT+ovTZuP6RNvx9x7BPRub6W/7pbqampal2/igZ0aahnX53FL4xu5NYgLFy4sKZOnarWrW++hiYqKkrVqlXL5qpwpwa//l99MGmiRo0crrNnzyigYEG1a/+0Xuj1krtLA7LFq+MWKzXV0oJx3WX3zqGfft2tPqMXOvVpVLuCXu3eWHavHPp9399qHzFDy3/Z5aaKIUk2y40f+9GqVStVqVJFI0aMuOn27du366GHHlJqamqmxmVGCEj5Hn7Z3SUAbpewbcpt+7h1Rjhw4EDFx8enu7106dJavXp1NlYEADCNW2eEWYUZIcCMEJAyNiO8p5dPAACQ1QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0QhCAIDRCEIAgNEIQgCA0WyWZVnuLgL/LklJSRo9erSGDBkiu93u7nIAt+DfwT8HQQiXi42NVZ48eXThwgX5+/u7uxzALfh38M/BpVEAgNEIQgCA0QhCAIDRCEK4nN1u19ChQ3lAAEbj38E/Bw/LAACMxowQAGA0ghAAYDSCEABgNIIQAGA0ghAu98EHHygkJEQ+Pj6qUaOGNm3a5O6SgGyzbt06tWzZUkFBQbLZbIqMjHR3SbgNghAutXDhQvXr109Dhw7V1q1bVblyZTVu3FjR0dHuLg3IFvHx8apcubI++OADd5eCDGL5BFyqRo0aevjhhzVlyhRJUmpqqooVK6ZXXnlFgwcPdnN1QPay2Wz6+uuv1aZNG3eXgltgRgiXSU5O1pYtW9SgQQNHm4eHhxo0aKANGza4sTIASB9BCJeJiYlRSkqKChUq5NReqFAhnTx50k1VAcCtEYQAAKMRhHCZAgUKyNPTU6dOnXJqP3XqlAIDA91UFQDcGkEIl/H29la1atW0cuVKR1tqaqpWrlypWrVqubEyAEhfDncXgH+Xfv36KSwsTNWrV9cjjzyiCRMmKD4+Xl26dHF3aUC2iIuL04EDBxyvDx8+rKioKOXPn1/Fixd3Y2VID8sn4HJTpkzR2LFjdfLkSVWpUkWTJk1SjRo13F0WkC3WrFmjevXqpWkPCwvTnDlzsr8g3BZBCAAwGvcIAQBGIwgBAEYjCAEARiMIAQBGIwgBAEYjCAEARiMIAQBGIwgBAEYjCIF/gfDwcKc//lq3bl317ds32+tYs2aNbDabzp8/n+3HBu4UQQhkofDwcNlsNtlsNnl7e6t06dIaMWKErly5kqXH/eqrrzRy5MgM9SW8YDo+dBvIYk2aNNHs2bOVlJSk77//Xi+99JK8vLw0ZMgQp37Jycny9vZ2yTHz58/vknEAEzAjBLKY3W5XYGCggoOD1atXLzVo0EBLlixxXM58++23FRQUpLJly0qSjh07pg4dOihv3rzKnz+/WrdurSNHjjjGS0lJUb9+/ZQ3b17dd999evXVV3XjRwbfeGk0KSlJgwYNUrFixWS321W6dGnNnDlTR44ccXxAdL58+WSz2RQeHi7p6p/QGj16tEqUKCFfX19VrlxZixYtcjrO999/r/vvv1++vr6qV6+eU53APwVBCGQzX19fJScnS5JWrlypvXv3asWKFfruu+90+fJlNW7cWLlz59bPP/+sX375RX5+fmrSpIljn/fee09z5szRrFmztH79ep09e1Zff/31LY/ZuXNnLViwQJMmTdLu3bs1ffp0+fn5qVixYlq8eLEkae/evTpx4oQmTpwoSRo9erQ++eQTTZs2TTt37lRERISeffZZrV27VtLVwG7btq1atmypqKgode/eXYMHD86qtw3IOhaALBMWFma1bt3asizLSk1NtVasWGHZ7XZrwIABVlhYmFWoUCErKSnJ0X/evHlW2bJlrdTUVEdbUlKS5evray1btsyyLMsqXLiwNWbMGMf2y5cvW0WLFnUcx7IsKzQ01OrTp49lWZa1d+9eS5K1YsWKm9a4evVqS5J17tw5R1tiYqKVM2dO69dff3Xq261bN+uZZ56xLMuyhgwZYlWoUMFp+6BBg9KMBdzruEcIZLHvvvtOfn5+unz5slJTU9WpUycNGzZML730kipVquR0X3D79u06cOCAcufO7TRGYmKiDh48qAsXLujEiRNOf98xR44cql69eprLo9dERUXJ09NToaGhGa75wIEDunTpkho2bOjUnpycrIceekiStHv37jR/Z7JWrVoZPgZwryAIgSxWr149ffjhh/L29lZQUJBy5Pi/f3a5cuVy6hsXF6dq1app/vz5acYJCAi4o+P7+vpmep+4uDhJ0tKlS1WkSBGnbXa7/Y7qAO5VBCGQxXLlyqXSpUtnqG/VqlW1cOFCFSxYUP7+/jftU7hwYf3222+qU6eOJOnKlSvasmWLqlatetP+lSpVUmpqqtauXasGDRqk2X5tRpqSkuJoq1Chgux2u44ePZruTLJ8+fJasmSJU9vGjRtvf5LAPYaHZYB7yH/+8x8VKFBArVu31s8//6zDhw9rzZo16t27t/766y9JUp8+ffTOO+8oMjJSe/bs0YsvvnjLNYAhISEKCwtT165dFRkZ6Rjziy++kCQFBwfLZrPpu+++0+nTpxUXF6fcuXNrwIABioiI0Ny5c3Xw4EFt3bpVkydP1ty5cyVJPXv21P79+zVw4EDt3btXn332mebMmZPVbxHgcgQhcA/JmTOn1q1bp+LFi6tt27YqX768unXrpsTERMcMsX///nruuecUFhamWrVqKXfu3HryySdvOe6HH36odu3a6cUXX1S5cuX0/PPPKz4+XpJUpEgRDR8+XIMHD1ahQoX08ssvS5JGjhypN954Q6NHj1b58uXVpEkTLV26VCVKlJAkFS9eXIsXL1ZkZKQqV66sadOmadSoUVn47gBZw2ald4cdAAADMCMEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGI0gBAAYjSAEABiNIAQAGO3/AdwLqOF0/D0FAAAAAElFTkSuQmCC\n"},"metadata":{}},{"name":"stdout","text":"Classification Report for Fold 1\n              precision    recall  f1-score     support\n0              0.990244  0.992665  0.991453  409.000000\n1              0.992806  0.990431  0.991617  418.000000\naccuracy       0.991536  0.991536  0.991536    0.991536\nmacro avg      0.991525  0.991548  0.991535  827.000000\nweighted avg   0.991539  0.991536  0.991536  827.000000\n\n\nClassification Report for Fold 2\n              precision    recall  f1-score     support\n0              0.992593  0.980488  0.986503  410.000000\n1              0.981043  0.992806  0.986889  417.000000\naccuracy       0.986699  0.986699  0.986699    0.986699\nmacro avg      0.986818  0.986647  0.986696  827.000000\nweighted avg   0.986769  0.986699  0.986698  827.000000\n\n\nClassification Report for Fold 3\n              precision    recall  f1-score     support\n0              0.980149  0.963415  0.971710  410.000000\n1              0.964623  0.980815  0.972652  417.000000\naccuracy       0.972189  0.972189  0.972189    0.972189\nmacro avg      0.972386  0.972115  0.972181  827.000000\nweighted avg   0.972320  0.972189  0.972185  827.000000\n\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-8cc80f789553>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mplot_confusion_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mprint_classification_reports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_reports\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-8cc80f789553>\u001b[0m in \u001b[0;36mprint_classification_reports\u001b[0;34m(classification_reports, k)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassification_reports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             avg_report[key] = {metric: np.mean([classification_reports[i][key][metric] for i in range(k)])\n\u001b[0;32m---> 55\u001b[0;31m                                for metric in classification_reports[0][key].keys()}\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: classification_reports does not contain dictionaries.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'keys'"],"ename":"AttributeError","evalue":"'float' object has no attribute 'keys'","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize average classification report\navg_classification_report = {}\n\n# Get all classes from the first report\nclasses = classification_reports[0].keys()\n\n# Initialize the structure for aggregation\nfor cls in classes:\n    if isinstance(classification_reports[0][cls], dict):\n        avg_classification_report[cls] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0}\n    else:\n        avg_classification_report[cls] = 0  # For 'accuracy', which is a float\n\n# Aggregate the metrics across all folds\nfor report in classification_reports:\n    for cls, metrics in report.items():\n        if isinstance(metrics, dict):\n            for metric, value in metrics.items():\n                avg_classification_report[cls][metric] += value\n        else:\n            avg_classification_report[cls] += metrics  # For 'accuracy'\n\n# Calculate the average for each metric\nnum_folds = len(classification_reports)\nfor cls, metrics in avg_classification_report.items():\n    if isinstance(metrics, dict):\n        for metric in metrics:\n            avg_classification_report[cls][metric] /= num_folds\n    else:\n        avg_classification_report[cls] /= num_folds  # For 'accuracy'\n\n# Convert accuracy (float) into a dictionary for consistent formatting\nif 'accuracy' in avg_classification_report:\n    acc_value = avg_classification_report['accuracy']\n    avg_classification_report['accuracy'] = {\n        'precision': acc_value,\n        'recall': acc_value,\n        'f1-score': acc_value,\n        'support': acc_value  # For display purposes\n    }\n\n# Convert the averaged classification report to a pandas DataFrame\nreport_df = pd.DataFrame.from_dict(avg_classification_report, orient='index')\n\n# Rearrange columns to match the required output format\nreport_df = report_df[['precision', 'recall', 'f1-score', 'support']]\n\n# Display the formatted classification report\nprint(\"\\nAveraged Classification Report across all folds:\\n\")\nprint(report_df.to_string(float_format='%.6f'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:33:01.254992Z","iopub.execute_input":"2025-02-10T12:33:01.255282Z","iopub.status.idle":"2025-02-10T12:33:01.270208Z","shell.execute_reply.started":"2025-02-10T12:33:01.255261Z","shell.execute_reply":"2025-02-10T12:33:01.269453Z"}},"outputs":[{"name":"stdout","text":"\nAveraged Classification Report across all folds:\n\n              precision   recall  f1-score    support\n0              0.987662 0.978856  0.983222 409.666667\n1              0.979490 0.988017  0.983719 417.333333\naccuracy       0.983474 0.983474  0.983474   0.983474\nmacro avg      0.983576 0.983437  0.983471 827.000000\nweighted avg   0.983543 0.983474  0.983473 827.000000\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Save the entire model\nmodel.save('complete_model.keras')  # Use .keras format (recommended) or .h5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:33:10.210471Z","iopub.execute_input":"2025-02-10T12:33:10.210803Z","iopub.status.idle":"2025-02-10T12:33:11.000800Z","shell.execute_reply.started":"2025-02-10T12:33:10.210777Z","shell.execute_reply":"2025-02-10T12:33:10.999853Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}